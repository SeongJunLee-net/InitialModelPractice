
    for epoch in range(num_epochs):
        running_loss = 0.0
        for dt in train_loader:

            out = model(dt)
           
            print(target)
            #print(out)
           
            loss = criterion(out,target)# 손실함수 계산

            optimizer.zero_grad() # optimizer 초기화

            loss.backward()

            optimizer.step() # optimizer 최적화

            running_loss = running_loss + loss.item()
   
        print('mse',running_loss/n)
    out_list.append(out)
    loss_graph.append(running_loss/n)
141/81:
out_list = []
# Y = data[['volumn']].values[-2*split:-split]

# for num in range(split)
#     new_tr = torch.FloatTensor(new_train).unsqueeze(0).to(device)
#     out = model(new_tr)
#     target = torch.FloatTensor(Y[num]).to(device)
for num in range(split):
    if num==0:
        new_train=X[-60:].tolist()
        target = torch.FloatTensor(X[-60])
    else:
        new_train = new_train[1:]+out.cpu().tolist()
        target = torch.FloatTensor([[out.cpu()[0].item()+out.cpu()[0].item()-new_train[-1][0]]]).unsqueeze(0).to(device)
    #print(type(new_train))
    new_tr = torch.FloatTensor(new_train).unsqueeze(0).to(device)
##############################################################################################
# target을 Y[num] 말고 다른걸로도 시도해볼만 한가

    
    
##############################################################################################

    train_loader = DataLoader(dataset = new_tr, batch_size = 64,shuffle = False)
    #print(next(iter(train_loader)))
    
    for epoch in range(num_epochs):
        running_loss = 0.0
        for dt in train_loader:

            out = model(dt)
           
            print(target)
            #print(out)
           
            loss = criterion(out,target)# 손실함수 계산

            optimizer.zero_grad() # optimizer 초기화

            loss.backward()

            optimizer.step() # optimizer 최적화

            running_loss = running_loss + loss.item()
   
        print('mse',running_loss/n)
    out_list.append(out)
    loss_graph.append(running_loss/n)
141/82:
out_list = []
# Y = data[['volumn']].values[-2*split:-split]

# for num in range(split)
#     new_tr = torch.FloatTensor(new_train).unsqueeze(0).to(device)
#     out = model(new_tr)
#     target = torch.FloatTensor(Y[num]).to(device)
for num in range(split):
    if num==0:
        new_train=X[-60:].tolist()
        target = torch.FloatTensor(X[-60]).unsqueeze(0).to(device)
    else:
        new_train = new_train[1:]+out.cpu().tolist()
        target = torch.FloatTensor([[out.cpu()[0].item()+out.cpu()[0].item()-new_train[-1][0]]]).unsqueeze(0).to(device)
    #print(type(new_train))
    new_tr = torch.FloatTensor(new_train).unsqueeze(0).to(device)
##############################################################################################
# target을 Y[num] 말고 다른걸로도 시도해볼만 한가

    
    
##############################################################################################

    train_loader = DataLoader(dataset = new_tr, batch_size = 64,shuffle = False)
    #print(next(iter(train_loader)))
    
    for epoch in range(num_epochs):
        running_loss = 0.0
        for dt in train_loader:

            out = model(dt)
           
            print(target)
            #print(out)
           
            loss = criterion(out,target)# 손실함수 계산

            optimizer.zero_grad() # optimizer 초기화

            loss.backward()

            optimizer.step() # optimizer 최적화

            running_loss = running_loss + loss.item()
   
        print('mse',running_loss/n)
    out_list.append(out)
    loss_graph.append(running_loss/n)
141/83:
out_list = []
# Y = data[['volumn']].values[-2*split:-split]

# for num in range(split)
#     new_tr = torch.FloatTensor(new_train).unsqueeze(0).to(device)
#     out = model(new_tr)
#     target = torch.FloatTensor(Y[num]).to(device)
for num in range(split):
    if num==0:
        new_train=X[-60:].tolist()
        target = torch.FloatTensor(X[-60]).unsqueeze(0).to(device)
    else:
        target = torch.FloatTensor([[out.cpu()[0].item()+out.cpu()[0].item()-new_train[-1][0]]]).unsqueeze(0).to(device)
        new_train = new_train[1:]+out.cpu().tolist()        
    #print(type(new_train))
    new_tr = torch.FloatTensor(new_train).unsqueeze(0).to(device)
##############################################################################################
# target을 Y[num] 말고 다른걸로도 시도해볼만 한가

    
    
##############################################################################################

    train_loader = DataLoader(dataset = new_tr, batch_size = 64,shuffle = False)
    #print(next(iter(train_loader)))
    
    for epoch in range(num_epochs):
        running_loss = 0.0
        for dt in train_loader:

            out = model(dt)
           
            print(target)
            #print(out)
           
            loss = criterion(out,target)# 손실함수 계산

            optimizer.zero_grad() # optimizer 초기화

            loss.backward()

            optimizer.step() # optimizer 최적화

            running_loss = running_loss + loss.item()
   
        print('mse',running_loss/n)
    out_list.append(out)
    loss_graph.append(running_loss/n)
141/84:
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import torch
import torch.nn as nn
import torch.optim as optim
import torch.functional as F
from torch.utils.data import TensorDataset,DataLoader
import matplotlib.pyplot as plt
141/85:
data = pd.read_csv('../traffic/data/5.csv')
data
141/86:
data['datetime'] = pd.to_datetime(data['datetime'])
data['datetime'].dtypes
141/87:
scaler = MinMaxScaler()
data[['volumn']] = scaler.fit_transform(data[['volumn']])
data
141/88: data.drop(columns=['unknown','service_name','packets'],inplace = True)
141/89: device = torch.device("cuda:0")
141/90:
def seq_data(x,sequence_length):
    x_seq = []
    target = []
    for i in range(len(x)-sequence_length):
        x_seq.append(x[i:i+sequence_length])
        target.append(x[i+sequence_length])

    
    return torch.FloatTensor(x_seq).to(device), torch.FloatTensor(target).to(device).view(-1,1)
141/91:
class VanillaRNN(nn.Module):
    
    def __init__(self,input_size,hidden_size,num_layers,device,sequence_length):
        super(VanillaRNN,self).__init__()
        self.device = device
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.rnn = nn.RNN(input_size,hidden_size,num_layers,batch_first = True)
        # input_size는 feature 개수를 의미함
        # hidden_state는 hidden_state의 feature 개수를 의미함
        self.fc = nn.Sequential(nn.Linear(hidden_size*sequence_length,1))
   
    def forward(self,x):
        h0 = torch.zeros(self.num_layers,x.size()[0],self.hidden_size).to(self.device)
        out,h_n = self.rnn(x,h0) # 64x60x10
        out = out.reshape(out.shape[0],-1)
        out = self.fc(out) # 64x1
        return out
141/92:
split = 1440
sequence_length = 60
X = data[['volumn']].values[:-split]
x_seq, target = seq_data(X,sequence_length)
141/93:
batch_size = 128
train = TensorDataset(x_seq,target)
train_loader = DataLoader(dataset = train,batch_size = batch_size)
# shuffle 제거
141/94:
input_size = x_seq.size(2)
num_layers = 2
hidden_size = 10
141/95:
model = VanillaRNN(input_size = input_size, hidden_size = hidden_size,sequence_length=sequence_length,
                  num_layers = num_layers, device=device).to(device) # GPU 연산을 위한 to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
141/96:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,target = dt
        out = model(seq)
        #print(seq)
        #print('out1',out) #size는 64x1
  
        loss = criterion(out,target)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    #print('out2',out)
    print('mse',running_loss/n)

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았음
141/97: out
141/98:
Y = data[['volumn']].values[-2*split:-split]
len(Y)
141/99: [[1,2]]+[[3,4]]
141/100:
out_list = []
# Y = data[['volumn']].values[-2*split:-split]

# for num in range(split)
#     new_tr = torch.FloatTensor(new_train).unsqueeze(0).to(device)
#     out = model(new_tr)
#     target = torch.FloatTensor(Y[num]).to(device)
for num in range(split):
    if num==0:
        new_train=X[-60:].tolist()
        target = torch.FloatTensor(X[-60]).unsqueeze(0).to(device)
    else:
        target = torch.FloatTensor([[out.cpu()[0].item()+out.cpu()[0].item()-new_train[-1][0]]]).unsqueeze(0).to(device)
        new_train = new_train[1:]+out.cpu().tolist()        
    #print(type(new_train))
    new_tr = torch.FloatTensor(new_train).unsqueeze(0).to(device)
##############################################################################################
# target을 Y[num] 말고 다른걸로도 시도해볼만 한가

    
    
##############################################################################################

    train_loader = DataLoader(dataset = new_tr, batch_size = 64,shuffle = False)
    #print(next(iter(train_loader)))
    
    for epoch in range(num_epochs):
        running_loss = 0.0
        for dt in train_loader:

            out = model(dt)
           
            print(target)
            #print(out)
           
            loss = criterion(out,target)# 손실함수 계산

            optimizer.zero_grad() # optimizer 초기화

            loss.backward()

            optimizer.step() # optimizer 최적화

            running_loss = running_loss + loss.item()
   
        print('mse',running_loss/n)
    out_list.append(out)
    loss_graph.append(running_loss/n)
141/101:
predictions = [prediction.cpu().item() for prediction in out_list]
predictions = scaler.fit_transform(np.array(predictions).reshape(-1,1))
predictions
141/102:
fig,axes = plt.subplots(1,1,figsize=(14,8))
axes.plot(np.arange(1440),data[['volumn']].values[-split:])
axes.plot(len(predictions),predictions)
141/103:
fig,axes = plt.subplots(1,1,figsize=(14,8))
axes.plot(np.arange(1440),data[['volumn']].values[-split:])
axes.plot(predictions.shape(2),predictions)
141/104:
fig,axes = plt.subplots(1,1,figsize=(14,8))
axes.plot(np.arange(1440),data[['volumn']].values[-split:])
axes.plot(predictions.shape[2],predictions)
141/105:
fig,axes = plt.subplots(1,1,figsize=(14,8))
axes.plot(np.arange(1440),data[['volumn']].values[-split:])
axes.plot(predictions.shape[1],predictions)
141/106:
fig,axes = plt.subplots(1,1,figsize=(14,8))
axes.plot(np.arange(1440),data[['volumn']].values[-split:])
axes.plot(np.arange(predictions.shape[1]),predictions)
141/107:
fig,axes = plt.subplots(1,1,figsize=(14,8))
axes.plot(np.arange(1440),data[['volumn']].values[-split:])
axes.plot((np.arange(predictions.shape[1]),predictions)
141/108:
fig,axes = plt.subplots(1,1,figsize=(14,8))
axes.plot(np.arange(1440),data[['volumn']].values[-split:])
axes.plot(np.arange(predictions.shape[1]),predictions)
141/109: np.arange(predictions.shape[1])
141/110: np.arange(predictions.shape[0])
141/111:
fig,axes = plt.subplots(1,1,figsize=(14,8))
axes.plot(np.arange(1440),data[['volumn']].values[-split:])
axes.plot(np.arange(predictions.shape[01]),predictions)
141/112:
fig,axes = plt.subplots(1,1,figsize=(14,8))
axes.plot(np.arange(1440),data[['volumn']].values[-split:])
axes.plot(np.arange(predictions.shape[0]),predictions)
141/113:
out_list = []
# Y = data[['volumn']].values[-2*split:-split]

# for num in range(split)
#     new_tr = torch.FloatTensor(new_train).unsqueeze(0).to(device)
#     out = model(new_tr)
#     target = torch.FloatTensor(Y[num]).to(device)
for num in range(split):
    if num==0:
        new_train=X[-60:].tolist()
        target = torch.FloatTensor(X[-60]).unsqueeze(0).to(device)
    else:
        target = torch.FloatTensor([[out.cpu()[0].item()+out.cpu()[0].item()-new_train[-1][0]]]).unsqueeze(0).to(device)
        new_train = new_train[1:]+out.cpu().tolist()        
    #print(type(new_train))
    new_tr = torch.FloatTensor(new_train).unsqueeze(0).to(device)
##############################################################################################
# target을 Y[num] 말고 다른걸로도 시도해볼만 한가

    
    
##############################################################################################

    train_loader = DataLoader(dataset = new_tr, batch_size = 64,shuffle = False)
    #print(next(iter(train_loader)))
    
    for epoch in range(num_epochs):
        running_loss = 0.0
        for dt in train_loader:

            out = model(dt)
           
            print(target)
            #print(out)
           
            loss = criterion(out,target)# 손실함수 계산

            optimizer.zero_grad() # optimizer 초기화

            loss.backward()

            optimizer.step() # optimizer 최적화

            running_loss = running_loss + loss.item()
   
        print('mse',running_loss/n)
    out_list.append(out)
    loss_graph.append(running_loss/n)
141/114:
out_list = []
Y = data[['volumn']].values[-split:]

# for num in range(split)
#     new_tr = torch.FloatTensor(new_train).unsqueeze(0).to(device)
#     out = model(new_tr)
#     target = torch.FloatTensor(Y[num]).to(device)
for num in range(split):
    if num==0:
        new_train=X[-60:].tolist()
        target = torch.FloatTensor(X[-60]).unsqueeze(0).to(device)
    else:
        target = torch.FloatTensor([[Y[num-1]+out.cpu()[0].item()-new_train[-1][0]]]).unsqueeze(0).to(device)
        # 실시간으로 들어오는 이전정보 반영
        new_train = new_train[1:]+out.cpu().tolist()        
    #print(type(new_train))
    new_tr = torch.FloatTensor(new_train).unsqueeze(0).to(device)
##############################################################################################
# target을 Y[num] 말고 다른걸로도 시도해볼만 한가

    
    
##############################################################################################

    train_loader = DataLoader(dataset = new_tr, batch_size = 64,shuffle = False)
    #print(next(iter(train_loader)))
    
    for epoch in range(num_epochs):
        running_loss = 0.0
        for dt in train_loader:

            out = model(dt)
           
            print(target)
            #print(out)
           
            loss = criterion(out,target)# 손실함수 계산

            optimizer.zero_grad() # optimizer 초기화

            loss.backward()

            optimizer.step() # optimizer 최적화

            running_loss = running_loss + loss.item()
   
        print('mse',running_loss/n)
    out_list.append(out)
    loss_graph.append(running_loss/n)
141/115:
predictions = [prediction.cpu().item() for prediction in out_list]
predictions = scaler.fit_transform(np.array(predictions).reshape(-1,1))
predictions
141/116: np.arange(predictions.shape[0])
141/117:
fig,axes = plt.subplots(1,1,figsize=(14,8))
axes.plot(np.arange(1440),data[['volumn']].values[-split:])
axes.plot(np.arange(predictions.shape[0]),predictions)
141/118:
out_list = []
Y = data[['volumn']].values[-split:]

# for num in range(split)
#     new_tr = torch.FloatTensor(new_train).unsqueeze(0).to(device)
#     out = model(new_tr)
#     target = torch.FloatTensor(Y[num]).to(device)
for num in range(split):
    if num==0:
        new_train=X[-60:].tolist()
        target = torch.FloatTensor(X[-60]).unsqueeze(0).to(device)
    else:
        target = torch.FloatTensor(target = torch.FloatTensor([[out.cpu()[0].item()+out.cpu()[0].item()-new_train[-1][0]]]).unsqueeze(0).to(device)).unsqueeze(0).to(device)
        # 실시간으로 들어오는 이전정보 반영
        new_train = new_train[1:]+out.cpu().tolist()        
    #print(type(new_train))
    new_tr = torch.FloatTensor(new_train).unsqueeze(0).to(device)
##############################################################################################
# target을 Y[num] 말고 다른걸로도 시도해볼만 한가

    
    
##############################################################################################

    train_loader = DataLoader(dataset = new_tr, batch_size = 64,shuffle = False)
    #print(next(iter(train_loader)))
    
    for epoch in range(num_epochs):
        running_loss = 0.0
        for dt in train_loader:

            out = model(dt)
           
            print(target)
            #print(out)
           
            loss = criterion(out,target)# 손실함수 계산

            optimizer.zero_grad() # optimizer 초기화

            loss.backward()

            optimizer.step() # optimizer 최적화

            running_loss = running_loss + loss.item()
   
        print('mse',running_loss/n)
    out_list.append(out)
    loss_graph.append(running_loss/n)
141/119:
class VanillaRNN(nn.Module):
    
    def __init__(self,input_size,hidden_size,num_layers,device,sequence_length):
        super(VanillaRNN,self).__init__()
        self.device = device
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.rnn = nn.RNN(input_size,hidden_size,num_layers,batch_first = True)
        # input_size는 feature 개수를 의미함
        # hidden_state는 hidden_state의 feature 개수를 의미함
        self.fc = nn.Sequential(nn.Linear(hidden_size*sequence_length,1))
   
    def forward(self,x):
        h0 = torch.zeros(self.num_layers,x.size()[0],self.hidden_size).to(self.device)
        out,h_n = self.rnn(x,h0) # 64x60x10
        out = out.reshape(out.shape[0],-1)
        out = self.fc(out) # 64x1
        return out
141/120:
split = 1440
sequence_length = 60
X = data[['volumn']].values[:-split]
x_seq, target = seq_data(X,sequence_length)
141/121:
batch_size = 128
train = TensorDataset(x_seq,target)
train_loader = DataLoader(dataset = train,batch_size = batch_size)
# shuffle 제거
141/122:
input_size = x_seq.size(2)
num_layers = 2
hidden_size = 10
141/123:
model = VanillaRNN(input_size = input_size, hidden_size = hidden_size,sequence_length=sequence_length,
                  num_layers = num_layers, device=device).to(device) # GPU 연산을 위한 to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
141/124:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,target = dt
        out = model(seq)
        #print(seq)
        #print('out1',out) #size는 64x1
  
        loss = criterion(out,target)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    #print('out2',out)
    print('mse',running_loss/n)

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았음
141/125: out
141/126:
Y = data[['volumn']].values[-2*split:-split]
len(Y)
141/127: [[1,2]]+[[3,4]]
141/128:
out_list = []
Y = data[['volumn']].values[-split:]

# for num in range(split)
#     new_tr = torch.FloatTensor(new_train).unsqueeze(0).to(device)
#     out = model(new_tr)
#     target = torch.FloatTensor(Y[num]).to(device)
for num in range(split):
    if num==0:
        new_train=X[-60:].tolist()
        target = torch.FloatTensor(X[-60]).unsqueeze(0).to(device)
    else:
        target = torch.FloatTensor(target = torch.FloatTensor([[out.cpu()[0].item()+out.cpu()[0].item()-new_train[-1][0]]]).unsqueeze(0).to(device))
        # 실시간으로 들어오는 이전정보 반영
        new_train = new_train[1:]+out.cpu().tolist()        
    #print(type(new_train))
    new_tr = torch.FloatTensor(new_train).unsqueeze(0).to(device)
##############################################################################################
# target을 Y[num] 말고 다른걸로도 시도해볼만 한가

    
    
##############################################################################################

    train_loader = DataLoader(dataset = new_tr, batch_size = 64,shuffle = False)
    #print(next(iter(train_loader)))
    
    for epoch in range(num_epochs):
        running_loss = 0.0
        for dt in train_loader:

            out = model(dt)
           
            print(target)
            #print(out)
           
            loss = criterion(out,target)# 손실함수 계산

            optimizer.zero_grad() # optimizer 초기화

            loss.backward()

            optimizer.step() # optimizer 최적화

            running_loss = running_loss + loss.item()
   
        print('mse',running_loss/n)
    out_list.append(out)
    loss_graph.append(running_loss/n)
141/129:
out_list = []
Y = data[['volumn']].values[-split:]

# for num in range(split)
#     new_tr = torch.FloatTensor(new_train).unsqueeze(0).to(device)
#     out = model(new_tr)
#     target = torch.FloatTensor(Y[num]).to(device)
for num in range(split):
    if num==0:
        new_train=X[-60:].tolist()
        target = torch.FloatTensor(X[-60]).unsqueeze(0).to(device)
    else:
        target = torch.FloatTensor(target = torch.FloatTensor([[out.cpu()[0].item()+out.cpu()[0].item()-new_train[-1][0]]]).unsqueeze(0).to(device))
        # 실시간으로 들어오는 이전정보 반영
        new_train = new_train[1:]+out.cpu().tolist()        
    #print(type(new_train))
    new_tr = torch.FloatTensor(new_train).unsqueeze(0).to(device)
##############################################################################################
# target을 Y[num] 말고 다른걸로도 시도해볼만 한가

    
    
##############################################################################################

    train_loader = DataLoader(dataset = new_tr, batch_size = 64,shuffle = False)
    #print(next(iter(train_loader)))
    
    for epoch in range(num_epochs):
        running_loss = 0.0
        for dt in train_loader:

            out = model(dt)
           
            print(target)
            #print(out)
           
            loss = criterion(out,target)# 손실함수 계산

            optimizer.zero_grad() # optimizer 초기화

            loss.backward()

            optimizer.step() # optimizer 최적화

            running_loss = running_loss + loss.item()
   
        print('mse',running_loss/n)
    out_list.append(out)
    loss_graph.append(running_loss/n)
141/130:
out_list = []
Y = data[['volumn']].values[-split:]

# for num in range(split)
#     new_tr = torch.FloatTensor(new_train).unsqueeze(0).to(device)
#     out = model(new_tr)
#     target = torch.FloatTensor(Y[num]).to(device)
for num in range(split):
    if num==0:
        new_train=X[-60:].tolist()
        target = torch.FloatTensor(X[-60]).unsqueeze(0).to(device)
    else:
        target = torch.FloatTensor([[out.cpu()[0].item() + out.cpu()[0].item()-new_train[-1][0]]]).unsqueeze(0).to(device)
        # 실시간으로 들어오는 이전정보 반영
        new_train = new_train[1:]+out.cpu().tolist()        
    #print(type(new_train))
    new_tr = torch.FloatTensor(new_train).unsqueeze(0).to(device)
##############################################################################################
# target을 Y[num] 말고 다른걸로도 시도해볼만 한가

    
    
##############################################################################################

    train_loader = DataLoader(dataset = new_tr, batch_size = 64,shuffle = False)
    #print(next(iter(train_loader)))
    
    for epoch in range(num_epochs):
        running_loss = 0.0
        for dt in train_loader:

            out = model(dt)
           
            #print(target)
            print(out)
           
            loss = criterion(out,target)# 손실함수 계산

            optimizer.zero_grad() # optimizer 초기화

            loss.backward()

            optimizer.step() # optimizer 최적화

            running_loss = running_loss + loss.item()
   
        print('mse',running_loss/n)
    out_list.append(out)
    loss_graph.append(running_loss/n)
141/131:
def seq_data(x,sequence_length):
    x_seq = []
    target = []
    for i in range(len(x)-sequence_length):
        x_seq.append(x[i:i+sequence_length])
        target.append(x[i+sequence_length])

    
    return torch.FloatTensor(x_seq).to(device), torch.FloatTensor(target).to(device).view(-1,1)
141/132:
class VanillaRNN(nn.Module):
    
    def __init__(self,input_size,hidden_size,num_layers,device,sequence_length):
        super(VanillaRNN,self).__init__()
        self.device = device
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.rnn = nn.RNN(input_size,hidden_size,num_layers,batch_first = True)
        # input_size는 feature 개수를 의미함
        # hidden_state는 hidden_state의 feature 개수를 의미함
        self.fc = nn.Sequential(nn.Linear(hidden_size*sequence_length,1))
   
    def forward(self,x):
        h0 = torch.zeros(self.num_layers,x.size()[0],self.hidden_size).to(self.device)
        out,h_n = self.rnn(x,h0) # 64x60x10
        out = out.reshape(out.shape[0],-1)
        out = self.fc(out) # 64x1
        return out
141/133:
split = 1440
sequence_length = 60
X = data[['volumn']].values[:-split]
x_seq, target = seq_data(X,sequence_length)
141/134:
batch_size = 128
train = TensorDataset(x_seq,target)
train_loader = DataLoader(dataset = train,batch_size = batch_size)
# shuffle 제거
141/135:
input_size = x_seq.size(2)
num_layers = 2
hidden_size = 10
141/136:
model = VanillaRNN(input_size = input_size, hidden_size = hidden_size,sequence_length=sequence_length,
                  num_layers = num_layers, device=device).to(device) # GPU 연산을 위한 to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
141/137:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,target = dt
        out = model(seq)
        #print(seq)
        #print('out1',out) #size는 64x1
  
        loss = criterion(out,target)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    #print('out2',out)
    print('mse',running_loss/n)

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았음
141/138:
out_list = []
Y = data[['volumn']].values[-split:]

# for num in range(split)
#     new_tr = torch.FloatTensor(new_train).unsqueeze(0).to(device)
#     out = model(new_tr)
#     target = torch.FloatTensor(Y[num]).to(device)
for num in range(split):
    if num==0:
        new_train=X[-60:].tolist()
        target = torch.FloatTensor(X[-60]).unsqueeze(0).to(device)
    else:
        target = torch.FloatTensor([[out.cpu()[0].item() + out.cpu()[0].item()-new_train[-1][0]]]).unsqueeze(0).to(device)
        # 실시간으로 들어오는 이전정보 반영
        new_train = new_train[1:]+out.cpu().tolist()        
    #print(type(new_train))
    new_tr = torch.FloatTensor(new_train).unsqueeze(0).to(device)
##############################################################################################
# target을 Y[num] 말고 다른걸로도 시도해볼만 한가

    
    
##############################################################################################

    train_loader = DataLoader(dataset = new_tr, batch_size = 64,shuffle = False)
    #print(next(iter(train_loader)))
    
    for epoch in range(num_epochs):
        running_loss = 0.0
        for dt in train_loader:

            out = model(dt)
           
            #print(target)
            print(out)
           
            loss = criterion(out,target)# 손실함수 계산

            optimizer.zero_grad() # optimizer 초기화

            loss.backward()

            optimizer.step() # optimizer 최적화

            running_loss = running_loss + loss.item()
   
        print('mse',running_loss/n)
    out_list.append(out)
    loss_graph.append(running_loss/n)
141/139:
predictions = [prediction.cpu().item() for prediction in out_list]
predictions = scaler.fit_transform(np.array(predictions).reshape(-1,1))
predictions
141/140: np.arange(predictions.shape[0])
141/141:
fig,axes = plt.subplots(1,1,figsize=(14,8))
axes.plot(np.arange(1440),data[['volumn']].values[-split:])
axes.plot(np.arange(predictions.shape[0]),predictions)
139/1:
# Pytorch로 설게하는 신경망은 반드시 따라야 하는 구조가 있다

'''
1. torch.nn.Module을 상속해야 한다
2. __init()__과 forward()를 override 해야한다.
    (1) override: torch.nn.Module(부모클래스)에서 정의한 메소드를 override 해야한다
    (2) __init()__ 에서는 모델에서 사용될 module(nn.Linear, nn.Conv2d), activation function
    (nn.functional.relu, nn.functional.sigmoid)등을 정의한다
    (3) forward()에서는 모델에서 실행되어야하는 계산을 정의한다. backward 계산은 backward()를
    이용시에 Pytorch가 알아서 해주니깐 forward()만을 정의해주면 된다
    input을 넣어서 어떤 계산을 진행해서 output이 나올지를 정의해준다고 이해하면 된다
'''
class moving_avg(nn.Module):
    "M.A.는 시계열의 trend에 집중하는 것을 막는다"
    def __init__(self,kernel_size,stride):
        super(moving_avg,self).__init__()
        # super(Model_Name,self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size = kernel_size, stride = stride, padding =0)
        # pooling 계산을 하는 1차원 평균으로 pooling계산
    
    def forward(self,x):
        " time series의 양 쪽 끝을 padding 한다"
        front = x[:,0:1,:].repeat(1,(self.kernel_size-1)//2,1)
        # Repeats this tensor along the specified dimensions.
        print("front",front)
        
        end = x[:,-1:,:].repeat(1,(self.kernel_size-1)//2,1)
        print("end",end)
        
# front와 end를 추가해서 moving average를 계산해서 공백이 없도록
        
        x = torch.cat([front, x, end], dim=1)
        print("cat",x,x.size())
        
        print('before avg',x.permute(0,2,1))
        x = self.avg(x.permute(0,2,1)) #pooling 계산
        print(x)
        
        x = x.permute(0,2,1) # 원상복귀
        print(x)
        return x
139/2:
import torch
import torch.nn as nn
import torch.nn.functional as F
# 이 모듈에는 torch.nn 라이브러리의 모든 함수가 포함되어 있다 다양한 손실 및
# 활성화 함수 뿐만 아니라, 풀링(pooling) 함수와 같이 신경망을 만드는데 편리한 몇 가지
# 함수도 여기에서 찾을 수 있다.
import numpy as np
139/3:
# Pytorch로 설게하는 신경망은 반드시 따라야 하는 구조가 있다

'''
1. torch.nn.Module을 상속해야 한다
2. __init()__과 forward()를 override 해야한다.
    (1) override: torch.nn.Module(부모클래스)에서 정의한 메소드를 override 해야한다
    (2) __init()__ 에서는 모델에서 사용될 module(nn.Linear, nn.Conv2d), activation function
    (nn.functional.relu, nn.functional.sigmoid)등을 정의한다
    (3) forward()에서는 모델에서 실행되어야하는 계산을 정의한다. backward 계산은 backward()를
    이용시에 Pytorch가 알아서 해주니깐 forward()만을 정의해주면 된다
    input을 넣어서 어떤 계산을 진행해서 output이 나올지를 정의해준다고 이해하면 된다
'''
class moving_avg(nn.Module):
    "M.A.는 시계열의 trend에 집중하는 것을 막는다"
    def __init__(self,kernel_size,stride):
        super(moving_avg,self).__init__()
        # super(Model_Name,self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size = kernel_size, stride = stride, padding =0)
        # pooling 계산을 하는 1차원 평균으로 pooling계산
    
    def forward(self,x):
        " time series의 양 쪽 끝을 padding 한다"
        front = x[:,0:1,:].repeat(1,(self.kernel_size-1)//2,1)
        # Repeats this tensor along the specified dimensions.
        print("front",front)
        
        end = x[:,-1:,:].repeat(1,(self.kernel_size-1)//2,1)
        print("end",end)
        
# front와 end를 추가해서 moving average를 계산해서 공백이 없도록
        
        x = torch.cat([front, x, end], dim=1)
        print("cat",x,x.size())
        
        print('before avg',x.permute(0,2,1))
        x = self.avg(x.permute(0,2,1)) #pooling 계산
        print(x)
        
        x = x.permute(0,2,1) # 원상복귀
        print(x)
        return x
139/4:
class Model(nn.Moodule):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = configs.seq_len # configure의 sequence length
        self.pred_len = configs.pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs.individual #??
        self.channels = configs.enc_in # encoder의 넣는 갯수
        
        if self.individual:
            self.Linear_Seasonal = nn.ModuleList()
139/5: x = FloatTensor([[[1,2,3],[2,3,4]]],[[[1,2,3],[2,3,4]]])
139/6: x = torch.FloatTensor([[[1,2,3],[2,3,4]]],[[[1,2,3],[2,3,4]]])
139/7: x = torch.FloatTensor([[[1,2,3],[2,3,4]]])
139/8: torch.zeros(1,2,3)
139/9: torch.zeros(2,1,3) # batch는 1 length는 2 input size=3
139/10: torch.zeros(2,1,3,dtype=float64) # batch는 2 length는 1 input size=3
139/11: torch.zeros(2,1,3,dtype=float) # batch는 2 length는 1 input size=3
139/12: torch.ones(2,1,3,dtype=float) # batch는 2 length는 1 input size=3
139/13:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = configs.seq_len # configure의 sequence length
        self.pred_len = configs.pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs.individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = configs.enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend. = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        
        def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len])
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])    
                trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
139/14:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = configs.seq_len # configure의 sequence length
        self.pred_len = configs.pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs.individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = configs.enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        
        def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len])
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])    
                trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
139/15:
import torch
import torch.nn as nn
import torch.nn.functional as F
# 이 모듈에는 torch.nn 라이브러리의 모든 함수가 포함되어 있다 다양한 손실 및
# 활성화 함수 뿐만 아니라, 풀링(pooling) 함수와 같이 신경망을 만드는데 편리한 몇 가지
# 함수도 여기에서 찾을 수 있다.
import numpy as np
139/16:
data = pd.read_csv('../traffic/data/5.csv')
data
139/17:
import torch
import torch.nn as nn
import torch.nn.functional as F
# 이 모듈에는 torch.nn 라이브러리의 모든 함수가 포함되어 있다 다양한 손실 및
# 활성화 함수 뿐만 아니라, 풀링(pooling) 함수와 같이 신경망을 만드는데 편리한 몇 가지
# 함수도 여기에서 찾을 수 있다.
import numpy as np
import pandas as pd
139/18:
data = pd.read_csv('../traffic/data/5.csv')
data
139/19: data.drop(columns=['service_name','packets','unknown'])
139/20:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if type(x)==list:
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    else if type(x) == pd.Series:
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length])
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
139/21:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if type(x)==list:
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    else if: type(x) == pd.Series:
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length])
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
139/22:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if type(x)==list:
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    elif type(x) == pd.Series:
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length])
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
139/23:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if type(x)==list|np.array:
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    elif type(x) == pd.Series:
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length])
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
139/24:
split = 720
sequence_length = 60
X = data[['volumn']].values[:-split]
x_seq, target = seq_data(X,sequence_length)
139/25:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if type(x)==list||np.array:
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    elif type(x) == pd.Series:
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length])
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
139/26:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if type(x)==list or np.array:
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    elif type(x) == pd.Series:
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length])
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
139/27:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if type(x)==list or np.array:
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    elif type(x) == pd.Series:
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length])
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
139/28:
split = 720
sequence_length = 60
X = data[['volumn']].values[:-split]
x_seq, target = seq_data(X,sequence_length)
139/29: data['volumn'].iloc[:-split]
139/30: data['volumn'].iloc[:-split].values
139/31: type(data['volumn'].iloc[:-split].values)
139/32: type(data['volumn'].iloc[:-split])
139/33:
split = 720
sequence_length = 60
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
139/34:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if type(x)==list or np.array:
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    elif type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length].values)
    else:
        print('error')
    return seq_list,target_list
139/35:
split = 720
sequence_length = 60
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
139/36:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if type(x)==list or np.array:
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    elif type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length].values)
    else:
        print('error')
    return seq_list,target_list
139/37:
split = 720
sequence_length = 60
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
139/38: seq_data(X,sequence_length)
139/39:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if type(x)==list or np.array:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    elif type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length].values)
    else:
        print('error')
    return seq_list,target_list
139/40:
split = 720
sequence_length = 60
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
139/41:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if type(x)==list or np.array:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    elif type(x) == pd.Series:
        #print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length].values)
    else:
        print('error')
    return seq_list,target_list
139/42:
split = 720
sequence_length = 60
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
139/43: type(data['volumn'].iloc[:-split])
139/44:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
#     if type(x)==list or np.array:
#         print(1)
#         for i in range(len(x)-sequence_length):
#             seq_list.append(x[i:i+sequence_length])
#             target_list.append(x[i+sequence_length])
    
    elif type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length].values)
    else:
        print('error')
    return seq_list,target_list
139/45:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list | np.array):
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    elif type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length].values)
    else:
        print('error')
    return seq_list,target_list
139/46:
split = 720
sequence_length = 60
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
139/47:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list | np.array()):
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    elif type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length].values)
    else:
        print('error')
    return seq_list,target_list
139/48:
split = 720
sequence_length = 60
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
139/49:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list | np.array):
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    elif type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length].values)
    else:
        print('error')
    return seq_list,target_list
139/50:
split = 720
sequence_length = 60
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
139/51:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if type(x)==list or np.array:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    elif type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length].values)
    else:
        print('error')
    return seq_list,target_list
139/52:
split = 720
sequence_length = 60
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
139/53: data['volumn'].iloc[:-split]
139/54:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
#     if type(x)==list or np.array:
#         print(1)
#         for i in range(len(x)-sequence_length):
#             seq_list.append(x[i:i+sequence_length])
#             target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length].values)
    else:
        print('error')
    return seq_list,target_list
139/55:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
#     if type(x)==list or np.array:
#         print(1)
#         for i in range(len(x)-sequence_length):
#             seq_list.append(x[i:i+sequence_length])
#             target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length].values)
    else:
        print('error')
    return seq_list,target_list
139/56:
split = 720
sequence_length = 60
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
139/57: X.iloc[i+sequence_length]
139/58: X.iloc[0+sequence_length]
139/59:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        #rint(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
    return seq_list,target_list
139/60:
split = 720
sequence_length = 60
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
139/61:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
    return seq_list,target_list
139/62:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
    return seq_list,target_list
139/63:
split = 720
sequence_length = 60
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
139/64: x_seq
139/65:
import torch
import torch.nn as nn
import torch.nn.functional as F
# 이 모듈에는 torch.nn 라이브러리의 모든 함수가 포함되어 있다 다양한 손실 및
# 활성화 함수 뿐만 아니라, 풀링(pooling) 함수와 같이 신경망을 만드는데 편리한 몇 가지
# 함수도 여기에서 찾을 수 있다.
import numpy as np
import torch.optim as optim
from torch.nn.utils import TensorDataset,DataLoader
139/66:
import torch
import torch.nn as nn
import torch.nn.functional as F
# 이 모듈에는 torch.nn 라이브러리의 모든 함수가 포함되어 있다 다양한 손실 및
# 활성화 함수 뿐만 아니라, 풀링(pooling) 함수와 같이 신경망을 만드는데 편리한 몇 가지
# 함수도 여기에서 찾을 수 있다.
import numpy as np
import torch.optim as optim
from torch.utils.data import TensorDataset,DataLoader
139/67:
split = 720
sequence_length = 60
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
139/68:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
    return np.array(seq_list),np.array(target_list)
139/69:
split = 720
sequence_length = 60
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
139/70:
import torch
import torch.nn as nn
import torch.nn.functional as F
# 이 모듈에는 torch.nn 라이브러리의 모든 함수가 포함되어 있다 다양한 손실 및
# 활성화 함수 뿐만 아니라, 풀링(pooling) 함수와 같이 신경망을 만드는데 편리한 몇 가지
# 함수도 여기에서 찾을 수 있다.
import numpy as np
import torch.optim as optim
from torch.utils.data import TensorDataset,DataLoader
from torch import FloatTensor
139/71:
device = torch.device("cuda:0")
torch_data = TensorDataset(FloatTensor(x_seq).unsqueeze(0).to(device),
                           FloatTensor(target).unsqueeze(0).to(device))
139/72: train_loader = DataLoader(torch_data,batch_size = 64, shuffle =True)
139/73:
A = {'a':1,'b':2}
A.'a'
139/74:
A = {'a':1,'b':2}
A.()'a')
139/75:
A = {'a':1,'b':2}
A.('a')
139/76:
A = {'a':1,'b':2}
A.'a'
139/77:
A = {'a':1,'b':2}
A.a
139/78:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        
        def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len])
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])    
                trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
139/79: torch.ones(2,1,3,dtype=float) # batch는 2 length는 1 input size=3
139/80:
import torch
import torch.nn as nn
import torch.nn.functional as F
# 이 모듈에는 torch.nn 라이브러리의 모든 함수가 포함되어 있다 다양한 손실 및
# 활성화 함수 뿐만 아니라, 풀링(pooling) 함수와 같이 신경망을 만드는데 편리한 몇 가지
# 함수도 여기에서 찾을 수 있다.
import numpy as np
import pandas as pd
139/81:
data = pd.read_csv('../traffic/data/5.csv')
data
139/82: data.drop(columns=['service_name','packets','unknown'])
139/83:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
    return np.array(seq_list),np.array(target_list)
139/84:
split = 720
sequence_length = 360
pred_len = 1
individual = False
enc_in = 1
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
139/85:
device = torch.device("cuda:0")
torch_data = TensorDataset(FloatTensor(x_seq).unsqueeze(0).to(device),
                           FloatTensor(target).unsqueeze(0).to(device))
139/86: train_loader = DataLoader(torch_data,batch_size = 64, shuffle =True)
139/87:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
139/88:
class series_decomp(nn.Module):
    "statsmodels.tsa의 seasonal_decompose와 역할이 똑같다"
    def __init__(self,kernel_size):
        super(series_decomp,self).__init__()
        self.moving_avg = moving_avg(kernel_size,stride = 1)
    
    def forward(self,x):
        moving_mean = self.moving_avg(x) #output은 (batch,sequence_length,input_size)
        res = x - moving_mean 
        # Classical ma중에서 additive model
        return res,moving_mean
139/89:
x = torch.FloatTensor([1,2,3,4]).view(1,-1,1)
decomp = series_decomp(3) 
# kernel_size를 짝수로하면 오류 발생 (size가 맞지 않은 오류)
# 이경우는 짝수 ma를 두번진행하면 된다(아래 예시)
a,b=decomp.forward(x)
139/90:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        
        def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len])
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])    
                trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
139/91: torch.ones(2,1,3,dtype=float) # batch는 2 length는 1 input size=3
139/92:
import torch
import torch.nn as nn
import torch.nn.functional as F
# 이 모듈에는 torch.nn 라이브러리의 모든 함수가 포함되어 있다 다양한 손실 및
# 활성화 함수 뿐만 아니라, 풀링(pooling) 함수와 같이 신경망을 만드는데 편리한 몇 가지
# 함수도 여기에서 찾을 수 있다.
import numpy as np
import pandas as pd
139/93:
data = pd.read_csv('../traffic/data/5.csv')
data
139/94: data.drop(columns=['service_name','packets','unknown'])
139/95:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
    return np.array(seq_list),np.array(target_list)
139/96:
split = 720
sequence_length = 360
pred_len = 1
individual = False
enc_in = 1
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
139/97:
device = torch.device("cuda:0")
torch_data = TensorDataset(FloatTensor(x_seq).unsqueeze(0).to(device),
                           FloatTensor(target).unsqueeze(0).to(device))
139/98: train_loader = DataLoader(torch_data,batch_size = 64, shuffle =True)
139/99:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
139/100:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,target = dt
        out = model(seq)
  
        loss = criterion(out,target)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
139/101:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        
        def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len])
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])    
                trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
139/102:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        
        def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len])
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])    
                trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
139/103: torch.ones(2,1,3,dtype=float) # batch는 2 length는 1 input size=3
139/104:
import torch
import torch.nn as nn
import torch.nn.functional as F
# 이 모듈에는 torch.nn 라이브러리의 모든 함수가 포함되어 있다 다양한 손실 및
# 활성화 함수 뿐만 아니라, 풀링(pooling) 함수와 같이 신경망을 만드는데 편리한 몇 가지
# 함수도 여기에서 찾을 수 있다.
import numpy as np
import pandas as pd
139/105:
data = pd.read_csv('../traffic/data/5.csv')
data
139/106: data.drop(columns=['service_name','packets','unknown'])
139/107:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
    return np.array(seq_list),np.array(target_list)
139/108:
split = 720
sequence_length = 360
pred_len = 1
individual = False
enc_in = 1
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
139/109:
device = torch.device("cuda:0")
torch_data = TensorDataset(FloatTensor(x_seq).unsqueeze(0).to(device),
                           FloatTensor(target).unsqueeze(0).to(device))
139/110: train_loader = DataLoader(torch_data,batch_size = 64, shuffle =True)
139/111:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
139/112:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,target = dt
        out = model(seq)
  
        loss = criterion(out,target)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
139/113:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len])
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])    
                trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
139/114:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len])
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])    
                trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
139/115: torch.ones(2,1,3,dtype=float) # batch는 2 length는 1 input size=3
139/116:
import torch
import torch.nn as nn
import torch.nn.functional as F
# 이 모듈에는 torch.nn 라이브러리의 모든 함수가 포함되어 있다 다양한 손실 및
# 활성화 함수 뿐만 아니라, 풀링(pooling) 함수와 같이 신경망을 만드는데 편리한 몇 가지
# 함수도 여기에서 찾을 수 있다.
import numpy as np
import pandas as pd
139/117:
data = pd.read_csv('../traffic/data/5.csv')
data
139/118: data.drop(columns=['service_name','packets','unknown'])
139/119:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
    return np.array(seq_list),np.array(target_list)
139/120:
split = 720
sequence_length = 360
pred_len = 1
individual = False
enc_in = 1
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
139/121:
split = 720
sequence_length = 360
pred_len = 1
individual = False
enc_in = 1
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
139/122: train_loader = DataLoader(torch_data,batch_size = 64, shuffle =True)
139/123:
device = torch.device("cuda:0")
torch_data = TensorDataset(FloatTensor(x_seq).unsqueeze(0).to(device),
                           FloatTensor(target).unsqueeze(0).to(device))
139/124: train_loader = DataLoader(torch_data,batch_size = 64, shuffle =True)
139/125:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
139/126:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,target = dt
        out = model(seq)
  
        loss = criterion(out,target)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
139/127:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len])
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
139/128:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,target = dt
        out = model(seq)
  
        loss = criterion(out,target)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
139/129:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len])
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
139/130: torch.ones(2,1,3,dtype=float) # batch는 2 length는 1 input size=3
139/131:
import torch
import torch.nn as nn
import torch.nn.functional as F
# 이 모듈에는 torch.nn 라이브러리의 모든 함수가 포함되어 있다 다양한 손실 및
# 활성화 함수 뿐만 아니라, 풀링(pooling) 함수와 같이 신경망을 만드는데 편리한 몇 가지
# 함수도 여기에서 찾을 수 있다.
import numpy as np
import pandas as pd
139/132:
data = pd.read_csv('../traffic/data/5.csv')
data
139/133: data.drop(columns=['service_name','packets','unknown'])
139/134:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
    return np.array(seq_list),np.array(target_list)
139/135:
split = 720
sequence_length = 360
pred_len = 1
individual = False
enc_in = 1
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
139/136:
device = torch.device("cuda:0")
torch_data = TensorDataset(FloatTensor(x_seq).unsqueeze(0).to(device),
                           FloatTensor(target).unsqueeze(0).to(device))
139/137: train_loader = DataLoader(torch_data,batch_size = 64, shuffle =True)
139/138:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
139/139:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,target = dt
        out = model(seq)
  
        loss = criterion(out,target)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
139/140:
# Pytorch로 설게하는 신경망은 반드시 따라야 하는 구조가 있다

'''
1. torch.nn.Module을 상속해야 한다
2. __init()__과 forward()를 override 해야한다.
    (1) override: torch.nn.Module(부모클래스)에서 정의한 메소드를 override 해야한다
    (2) __init()__ 에서는 모델에서 사용될 module(nn.Linear, nn.Conv2d), activation function
    (nn.functional.relu, nn.functional.sigmoid)등을 정의한다
    (3) forward()에서는 모델에서 실행되어야하는 계산을 정의한다. backward 계산은 backward()를
    이용시에 Pytorch가 알아서 해주니깐 forward()만을 정의해주면 된다
    input을 넣어서 어떤 계산을 진행해서 output이 나올지를 정의해준다고 이해하면 된다
'''
class moving_avg(nn.Module):
    "M.A.는 시계열의 trend에 집중하는 것을 막는다"
    def __init__(self,kernel_size,stride):
        super(moving_avg,self).__init__()
        # super(Model_Name,self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size = kernel_size, stride = stride, padding =0)
        # pooling 계산을 하는 1차원 평균으로 pooling계산
    
    def forward(self,x):
        " time series의 양 쪽 끝을 padding 한다"
        front = x[:,0:1,:].repeat(1,(self.kernel_size-1)//2,1)
        # Repeats this tensor along the specified dimensions.
        #print("front",front)
        
        end = x[:,-1:,:].repeat(1,(self.kernel_size-1)//2,1)
        #print("end",end)
        
# front와 end를 추가해서 moving average를 계산해서 공백이 없도록
        
        x = torch.cat([front, x, end], dim=1)
        #print("cat",x,x.size())
        
        #print('before avg',x.permute(0,2,1))
        x = self.avg(x.permute(0,2,1)) #pooling 계산
        #print(x)
        
        x = x.permute(0,2,1) # 원상복귀
        #print(x)
        return x
139/141:
x=torch.FloatTensor([2,1,3,4]).view(1,-1,1) #4개의 데이터를 가진 것을 표현
print(x)
print('-----------------')
ma = moving_avg(3,1) 
x = ma.forward(x)
print(x)
139/142:
class series_decomp(nn.Module):
    "statsmodels.tsa의 seasonal_decompose와 역할이 똑같다"
    def __init__(self,kernel_size):
        super(series_decomp,self).__init__()
        self.moving_avg = moving_avg(kernel_size,stride = 1)
    
    def forward(self,x):
        moving_mean = self.moving_avg(x) #output은 (batch,sequence_length,input_size)
        res = x - moving_mean 
        # Classical ma중에서 additive model
        return res,moving_mean
139/143:
x = torch.FloatTensor([1,2,3,4]).view(1,-1,1)
decomp = series_decomp(3) 
# kernel_size를 짝수로하면 오류 발생 (size가 맞지 않은 오류)
# 이경우는 짝수 ma를 두번진행하면 된다(아래 예시)
a,b=decomp.forward(x)
139/144:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = configs.seq_len # configure의 sequence length
        self.pred_len = configs.pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs.individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = configs.enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len])
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
139/145:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len])
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
139/146: torch.ones(2,1,3,dtype=float) # batch는 2 length는 1 input size=3
139/147:
import torch
import torch.nn as nn
import torch.nn.functional as F
# 이 모듈에는 torch.nn 라이브러리의 모든 함수가 포함되어 있다 다양한 손실 및
# 활성화 함수 뿐만 아니라, 풀링(pooling) 함수와 같이 신경망을 만드는데 편리한 몇 가지
# 함수도 여기에서 찾을 수 있다.
import numpy as np
import pandas as pd
139/148:
data = pd.read_csv('../traffic/data/5.csv')
data
139/149: data.drop(columns=['service_name','packets','unknown'])
139/150:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
    return np.array(seq_list),np.array(target_list)
139/151:
split = 720
sequence_length = 360
pred_len = 1
individual = False
enc_in = 1
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
139/152:
device = torch.device("cuda:0")
torch_data = TensorDataset(FloatTensor(x_seq).unsqueeze(0).to(device),
                           FloatTensor(target).unsqueeze(0).to(device))
139/153: train_loader = DataLoader(torch_data,batch_size = 64)
139/154:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
139/155:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
139/156:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        print(seq)
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
139/157:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        print(seq.size())
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
139/158: FloatTensor(x_seq).
139/159: FloatTensor(x_seq)
139/160: next(iter(train_loader))
139/161: next(iter(train_loader)).cpu()
139/162: next(iter(train_loader))[0].cpu().size()
139/163: train_loader = DataLoader(torch_data,batch_size = 64)
139/164: next(iter(train_loader))[0].cpu().size()
139/165:
device = torch.device("cuda:0")
torch_data = TensorDataset(x_seq,target)
139/166:
split = 720
sequence_length = 360
pred_len = 1
individual = False
enc_in = 1
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
139/167: FloatTensor(x_seq)
139/168:
device = torch.device("cuda:0")
torch_data = TensorDataset(x_seq,target)
139/169: x_seq
139/170:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
    return FloatTensor(seq_list).to(device),FloatTensor(target_list).to(device)
139/171:
split = 720
sequence_length = 360
pred_len = 1
individual = False
enc_in = 1
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
139/172: FloatTensor(x_seq)
139/173:
device = torch.device("cuda:0")
torch_data = TensorDataset(x_seq,target)
139/174: x_seq
139/175: x_seq.size()
139/176: train_loader = DataLoader(torch_data,batch_size = 64)
139/177: next(iter(train_loader))[0].cpu().size()
139/178: next(iter(train_loader))
139/179: x_seq
139/180: target
139/181:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
    return FloatTensor(seq_list).to(device),FloatTensor(target_list).to(device).view(-1,1)
139/182:
split = 720
sequence_length = 360
pred_len = 1
individual = False
enc_in = 1
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
139/183: x_seq
139/184: target
139/185:
device = torch.device("cuda:0")
torch_data = TensorDataset(x_seq,target)
139/186: next(iter(train_loader))
139/187: train_loader = DataLoader(torch_data,batch_size = 64)
139/188:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
139/189:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        print(seq.size())
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
139/190:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        print(seq)
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
139/191:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        print(seq.view(-1,360))
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
139/192:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        print(seq.squeeze(0))
        out = model(seq.squeeze(0))
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
139/193:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        print(seq.squeeze(1))
        out = model(seq.squeeze(0))
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
139/194:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq.unsqueeze(0))
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
139/195:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
139/196: train_loader = DataLoader(torch_data,batch_size = 1)
139/197:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
139/198:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
139/199: train_loader = DataLoader(torch_data,batch_size = 64)
139/200:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        print(seq)
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
139/201: seq.cpu().unsqueeze(0)
139/202: seq.cpu().unsqueeze(1)
139/203:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        seq.cpu().unsqueeze(0)
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
139/204:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        seq = seq.cpu().unsqueeze(0).to(device)
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
139/205:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values.shape(-1,1))
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
    return FloatTensor(seq_list).to(device),FloatTensor(target_list).to(device).view(-1,1)
139/206:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values.shape(-1,1))
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
    return FloatTensor(seq_list).to(device),FloatTensor(target_list).to(device).view(-1,1)
139/207:
split = 720
sequence_length = 360
pred_len = 1
individual = False
enc_in = 1
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
139/208:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
    return FloatTensor(seq_list).to(device).view(-1,1),FloatTensor(target_list).to(device).view(-1,1)
139/209:
split = 720
sequence_length = 360
pred_len = 1
individual = False
enc_in = 1
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
139/210:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
    return FloatTensor(seq_list).to(device).view(1,-1,1),FloatTensor(target_list).to(device).view(-1,1)
139/211:
split = 720
sequence_length = 360
pred_len = 1
individual = False
enc_in = 1
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
139/212: x_seq
139/213:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
    return FloatTensor(seq_list).to(device).view(-1,360,1),FloatTensor(target_list).to(device).view(-1,1)
139/214:
split = 720
sequence_length = 360
pred_len = 1
individual = False
enc_in = 1
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
139/215: x_seq
139/216: target
139/217:
device = torch.device("cuda:0")
torch_data = TensorDataset(x_seq,target)
139/218: next(iter(train_loader))
139/219: train_loader = DataLoader(torch_data,batch_size = 64)
139/220:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
139/221:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
139/222:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
    return FloatTensor(seq_list).to(device).view(360,-1),FloatTensor(target_list).to(device).view(-1,1)
139/223:
split = 720
sequence_length = 360
pred_len = 1
individual = False
enc_in = 1
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
139/224:
split = 720
sequence_length = 360
pred_len = 1
individual = False
enc_in = 1
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
139/225:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
    return FloatTensor(seq_list).to(device).view(360,-1),FloatTensor(target_list).to(device).view(-1,1)
139/226:
split = 720
sequence_length = 360
pred_len = 1
individual = False
enc_in = 1
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
139/227:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
    return FloatTensor(seq_list).to(device),FloatTensor(target_list).to(device).view(-1,1)
139/228:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
    return FloatTensor(seq_list).to(device),FloatTensor(target_list).to(device).view(-1,1)
139/229:
split = 720
sequence_length = 360
pred_len = 1
individual = False
enc_in = 1
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
143/1:
import torch
import torch.nn as nn
import torch.nn.functional as F
# 이 모듈에는 torch.nn 라이브러리의 모든 함수가 포함되어 있다 다양한 손실 및
# 활성화 함수 뿐만 아니라, 풀링(pooling) 함수와 같이 신경망을 만드는데 편리한 몇 가지
# 함수도 여기에서 찾을 수 있다.
import numpy as np
import torch.optim as optim
from torch.utils.data import TensorDataset,DataLoader
from torch import FloatTensor
143/2:
# Pytorch로 설게하는 신경망은 반드시 따라야 하는 구조가 있다

'''
1. torch.nn.Module을 상속해야 한다
2. __init()__과 forward()를 override 해야한다.
    (1) override: torch.nn.Module(부모클래스)에서 정의한 메소드를 override 해야한다
    (2) __init()__ 에서는 모델에서 사용될 module(nn.Linear, nn.Conv2d), activation function
    (nn.functional.relu, nn.functional.sigmoid)등을 정의한다
    (3) forward()에서는 모델에서 실행되어야하는 계산을 정의한다. backward 계산은 backward()를
    이용시에 Pytorch가 알아서 해주니깐 forward()만을 정의해주면 된다
    input을 넣어서 어떤 계산을 진행해서 output이 나올지를 정의해준다고 이해하면 된다
'''
class moving_avg(nn.Module):
    "M.A.는 시계열의 trend에 집중하는 것을 막는다"
    def __init__(self,kernel_size,stride):
        super(moving_avg,self).__init__()
        # super(Model_Name,self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size = kernel_size, stride = stride, padding =0)
        # pooling 계산을 하는 1차원 평균으로 pooling계산
    
    def forward(self,x):
        " time series의 양 쪽 끝을 padding 한다"
        front = x[:,0:1,:].repeat(1,(self.kernel_size-1)//2,1)
        # Repeats this tensor along the specified dimensions.
        #print("front",front)
        
        end = x[:,-1:,:].repeat(1,(self.kernel_size-1)//2,1)
        #print("end",end)
        
# front와 end를 추가해서 moving average를 계산해서 공백이 없도록
        
        x = torch.cat([front, x, end], dim=1)
        #print("cat",x,x.size())
        
        #print('before avg',x.permute(0,2,1))
        x = self.avg(x.permute(0,2,1)) #pooling 계산
        #print(x)
        
        x = x.permute(0,2,1) # 원상복귀
        #print(x)
        return x
143/3:
x=torch.FloatTensor([2,1,3,4]).view(1,-1,1) #4개의 데이터를 가진 것을 표현
print(x)
print('-----------------')
ma = moving_avg(3,1) 
x = ma.forward(x)
print(x)
143/4:
class series_decomp(nn.Module):
    "statsmodels.tsa의 seasonal_decompose와 역할이 똑같다"
    def __init__(self,kernel_size):
        super(series_decomp,self).__init__()
        self.moving_avg = moving_avg(kernel_size,stride = 1)
    
    def forward(self,x):
        moving_mean = self.moving_avg(x) #output은 (batch,sequence_length,input_size)
        res = x - moving_mean 
        # Classical ma중에서 additive model
        return res,moving_mean
143/5:
x = torch.FloatTensor([1,2,3,4]).view(1,-1,1)
decomp = series_decomp(3) 
# kernel_size를 짝수로하면 오류 발생 (size가 맞지 않은 오류)
# 이경우는 짝수 ma를 두번진행하면 된다(아래 예시)
a,b=decomp.forward(x)
143/6:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = configs.seq_len # configure의 sequence length
        self.pred_len = configs.pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs.individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = configs.enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len])
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
143/7:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len])
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
143/8: torch.ones(2,1,3,dtype=float) # batch는 2 length는 1 input size=3
143/9:
import torch
import torch.nn as nn
import torch.nn.functional as F
# 이 모듈에는 torch.nn 라이브러리의 모든 함수가 포함되어 있다 다양한 손실 및
# 활성화 함수 뿐만 아니라, 풀링(pooling) 함수와 같이 신경망을 만드는데 편리한 몇 가지
# 함수도 여기에서 찾을 수 있다.
import numpy as np
import pandas as pd
143/10:
data = pd.read_csv('../traffic/data/5.csv')
data
143/11: data.drop(columns=['service_name','packets','unknown'])
143/12:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
    return FloatTensor(seq_list).to(device),FloatTensor(target_list).to(device).view(-1,1)
143/13:
split = 720
sequence_length = 360
pred_len = 1
individual = False
enc_in = 1
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
143/14:
split = 720
sequence_length = 360
pred_len = 1
individual = False
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
143/15: x_seq
143/16:
# Pytorch로 설게하는 신경망은 반드시 따라야 하는 구조가 있다

'''
1. torch.nn.Module을 상속해야 한다
2. __init()__과 forward()를 override 해야한다.
    (1) override: torch.nn.Module(부모클래스)에서 정의한 메소드를 override 해야한다
    (2) __init()__ 에서는 모델에서 사용될 module(nn.Linear, nn.Conv2d), activation function
    (nn.functional.relu, nn.functional.sigmoid)등을 정의한다
    (3) forward()에서는 모델에서 실행되어야하는 계산을 정의한다. backward 계산은 backward()를
    이용시에 Pytorch가 알아서 해주니깐 forward()만을 정의해주면 된다
    input을 넣어서 어떤 계산을 진행해서 output이 나올지를 정의해준다고 이해하면 된다
'''
class moving_avg(nn.Module):
    "M.A.는 시계열의 trend에 집중하는 것을 막는다"
    def __init__(self,kernel_size,stride):
        super(moving_avg,self).__init__()
        # super(Model_Name,self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size = kernel_size, stride = stride, padding =0)
        # pooling 계산을 하는 1차원 평균으로 pooling계산
    
    def forward(self,x):
        " time series의 양 쪽 끝을 padding 한다"
        print(x[:,0:1,:])
        front = x[:,0:1,:].repeat(1,(self.kernel_size-1)//2,1)
        # Repeats this tensor along the specified dimensions.
        #print("front",front)
        
        end = x[:,-1:,:].repeat(1,(self.kernel_size-1)//2,1)
        #print("end",end)
        
# front와 end를 추가해서 moving average를 계산해서 공백이 없도록
        
        x = torch.cat([front, x, end], dim=1)
        #print("cat",x,x.size())
        
        #print('before avg',x.permute(0,2,1))
        x = self.avg(x.permute(0,2,1)) #pooling 계산
        #print(x)
        
        x = x.permute(0,2,1) # 원상복귀
        #print(x)
        return x
143/17: target
143/18:
device = torch.device("cuda:0")
torch_data = TensorDataset(x_seq,target)
143/19: next(iter(train_loader))
143/20: train_loader = DataLoader(torch_data,batch_size = 64)
143/21:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/22:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/23:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        seq = seq.cpu().view(batch_size,pred_len,enc_in)
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/24:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        seq = seq.cpu().view(batch_size,pred_len,enc_in).to(device)
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/25:
batch_size = 64
train_loader = DataLoader(torch_data,batch_size = batch_size)
143/26:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/27:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        seq = seq.cpu().view(batch_size,pred_len,enc_in).to(device)
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/28:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        seq = seq.cpu().view(batch_size,sequence_length,enc_in).to(device)
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/29:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        seq = seq.cpu().unsqueeze(-1).to(device)
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/30:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        seq = seq.cpu().unsqueeze(0).to(device)
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/31:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/32:
# Pytorch로 설게하는 신경망은 반드시 따라야 하는 구조가 있다

'''
1. torch.nn.Module을 상속해야 한다
2. __init()__과 forward()를 override 해야한다.
    (1) override: torch.nn.Module(부모클래스)에서 정의한 메소드를 override 해야한다
    (2) __init()__ 에서는 모델에서 사용될 module(nn.Linear, nn.Conv2d), activation function
    (nn.functional.relu, nn.functional.sigmoid)등을 정의한다
    (3) forward()에서는 모델에서 실행되어야하는 계산을 정의한다. backward 계산은 backward()를
    이용시에 Pytorch가 알아서 해주니깐 forward()만을 정의해주면 된다
    input을 넣어서 어떤 계산을 진행해서 output이 나올지를 정의해준다고 이해하면 된다
'''
class moving_avg(nn.Module):
    "M.A.는 시계열의 trend에 집중하는 것을 막는다"
    def __init__(self,kernel_size,stride):
        super(moving_avg,self).__init__()
        # super(Model_Name,self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size = kernel_size, stride = stride, padding =0)
        # pooling 계산을 하는 1차원 평균으로 pooling계산
    
    def forward(self,x):
        " time series의 양 쪽 끝을 padding 한다"
        print(x[:,0:1])
        front = x[:,0:1].repeat(1,(self.kernel_size-1)//2,1)
        # Repeats this tensor along the specified dimensions.
        #print("front",front)
        
        end = x[:,-1].repeat(1,(self.kernel_size-1)//2,1)
        #print("end",end)
        
# front와 end를 추가해서 moving average를 계산해서 공백이 없도록
        
        x = torch.cat([front, x, end], dim=1)
        #print("cat",x,x.size())
        
        #print('before avg',x.permute(0,2,1))
        x = self.avg(x.permute(0,2,1)) #pooling 계산
        #print(x)
        
        x = x.permute(0,2,1) # 원상복귀
        #print(x)
        return x
143/33:
# Pytorch로 설게하는 신경망은 반드시 따라야 하는 구조가 있다

'''
1. torch.nn.Module을 상속해야 한다
2. __init()__과 forward()를 override 해야한다.
    (1) override: torch.nn.Module(부모클래스)에서 정의한 메소드를 override 해야한다
    (2) __init()__ 에서는 모델에서 사용될 module(nn.Linear, nn.Conv2d), activation function
    (nn.functional.relu, nn.functional.sigmoid)등을 정의한다
    (3) forward()에서는 모델에서 실행되어야하는 계산을 정의한다. backward 계산은 backward()를
    이용시에 Pytorch가 알아서 해주니깐 forward()만을 정의해주면 된다
    input을 넣어서 어떤 계산을 진행해서 output이 나올지를 정의해준다고 이해하면 된다
'''
class moving_avg(nn.Module):
    "M.A.는 시계열의 trend에 집중하는 것을 막는다"
    def __init__(self,kernel_size,stride):
        super(moving_avg,self).__init__()
        # super(Model_Name,self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size = kernel_size, stride = stride, padding =0)
        # pooling 계산을 하는 1차원 평균으로 pooling계산
    
    def forward(self,x):
        " time series의 양 쪽 끝을 padding 한다"
        print(x[:,0:1])
        front = x[:,0:1].repeat(1,(self.kernel_size-1)//2,1)
        # Repeats this tensor along the specified dimensions.
        #print("front",front)
        
        end = x[:,-1].repeat(1,(self.kernel_size-1)//2,1)
        #print("end",end)
        
# front와 end를 추가해서 moving average를 계산해서 공백이 없도록
        
        x = torch.cat([front, x, end], dim=1)
        #print("cat",x,x.size())
        
        #print('before avg',x.permute(0,2,1))
        x = self.avg(x.permute(0,2,1)) #pooling 계산
        #print(x)
        
        x = x.permute(0,2,1) # 원상복귀
        #print(x)
        return x
143/34:
x=torch.FloatTensor([2,1,3,4]).view(1,-1,1) #4개의 데이터를 가진 것을 표현
print(x)
print('-----------------')
ma = moving_avg(3,1) 
x = ma.forward(x)
print(x)
143/35:
class series_decomp(nn.Module):
    "statsmodels.tsa의 seasonal_decompose와 역할이 똑같다"
    def __init__(self,kernel_size):
        super(series_decomp,self).__init__()
        self.moving_avg = moving_avg(kernel_size,stride = 1)
    
    def forward(self,x):
        moving_mean = self.moving_avg(x) #output은 (batch,sequence_length,input_size)
        res = x - moving_mean 
        # Classical ma중에서 additive model
        return res,moving_mean
143/36:
x = torch.FloatTensor([1,2,3,4]).view(1,-1,1)
decomp = series_decomp(3) 
# kernel_size를 짝수로하면 오류 발생 (size가 맞지 않은 오류)
# 이경우는 짝수 ma를 두번진행하면 된다(아래 예시)
a,b=decomp.forward(x)
143/37:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = configs.seq_len # configure의 sequence length
        self.pred_len = configs.pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs.individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = configs.enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len])
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
143/38:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len])
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
143/39: torch.ones(2,1,3,dtype=float) # batch는 2 length는 1 input size=3
143/40:
import torch
import torch.nn as nn
import torch.nn.functional as F
# 이 모듈에는 torch.nn 라이브러리의 모든 함수가 포함되어 있다 다양한 손실 및
# 활성화 함수 뿐만 아니라, 풀링(pooling) 함수와 같이 신경망을 만드는데 편리한 몇 가지
# 함수도 여기에서 찾을 수 있다.
import numpy as np
import pandas as pd
143/41:
data = pd.read_csv('../traffic/data/5.csv')
data
143/42: data.drop(columns=['service_name','packets','unknown'])
143/43:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
    return FloatTensor(seq_list).to(device),FloatTensor(target_list).to(device).view(-1,1)
143/44:
split = 720
sequence_length = 360
pred_len = 1
individual = False
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
143/45: x_seq
143/46: target
143/47:
device = torch.device("cuda:0")
torch_data = TensorDataset(x_seq,target)
143/48:
batch_size = 64
train_loader = DataLoader(torch_data,batch_size = batch_size)
143/49:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/50:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/51:
# Pytorch로 설게하는 신경망은 반드시 따라야 하는 구조가 있다

'''
1. torch.nn.Module을 상속해야 한다
2. __init()__과 forward()를 override 해야한다.
    (1) override: torch.nn.Module(부모클래스)에서 정의한 메소드를 override 해야한다
    (2) __init()__ 에서는 모델에서 사용될 module(nn.Linear, nn.Conv2d), activation function
    (nn.functional.relu, nn.functional.sigmoid)등을 정의한다
    (3) forward()에서는 모델에서 실행되어야하는 계산을 정의한다. backward 계산은 backward()를
    이용시에 Pytorch가 알아서 해주니깐 forward()만을 정의해주면 된다
    input을 넣어서 어떤 계산을 진행해서 output이 나올지를 정의해준다고 이해하면 된다
'''
class moving_avg(nn.Module):
    "M.A.는 시계열의 trend에 집중하는 것을 막는다"
    def __init__(self,kernel_size,stride):
        super(moving_avg,self).__init__()
        # super(Model_Name,self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size = kernel_size, stride = stride, padding =0)
        # pooling 계산을 하는 1차원 평균으로 pooling계산
    
    def forward(self,x):
        " time series의 양 쪽 끝을 padding 한다"
        #print(x[:,0:1])
        front = x[:,0:1].repeat(1,(self.kernel_size-1)//2,1)
        print(front)
        # Repeats this tensor along the specified dimensions.
        #print("front",front)
        
        end = x[:,-1].repeat(1,(self.kernel_size-1)//2,1)
        #print("end",end)
        
# front와 end를 추가해서 moving average를 계산해서 공백이 없도록
        
        x = torch.cat([front, x, end], dim=1)
        #print("cat",x,x.size())
        
        #print('before avg',x.permute(0,2,1))
        x = self.avg(x.permute(0,2,1)) #pooling 계산
        #print(x)
        
        x = x.permute(0,2,1) # 원상복귀
        #print(x)
        return x
143/52:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/53:
# Pytorch로 설게하는 신경망은 반드시 따라야 하는 구조가 있다

'''
1. torch.nn.Module을 상속해야 한다
2. __init()__과 forward()를 override 해야한다.
    (1) override: torch.nn.Module(부모클래스)에서 정의한 메소드를 override 해야한다
    (2) __init()__ 에서는 모델에서 사용될 module(nn.Linear, nn.Conv2d), activation function
    (nn.functional.relu, nn.functional.sigmoid)등을 정의한다
    (3) forward()에서는 모델에서 실행되어야하는 계산을 정의한다. backward 계산은 backward()를
    이용시에 Pytorch가 알아서 해주니깐 forward()만을 정의해주면 된다
    input을 넣어서 어떤 계산을 진행해서 output이 나올지를 정의해준다고 이해하면 된다
'''
class moving_avg(nn.Module):
    "M.A.는 시계열의 trend에 집중하는 것을 막는다"
    def __init__(self,kernel_size,stride):
        super(moving_avg,self).__init__()
        # super(Model_Name,self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size = kernel_size, stride = stride, padding =0)
        # pooling 계산을 하는 1차원 평균으로 pooling계산
    
    def forward(self,x):
        " time series의 양 쪽 끝을 padding 한다"
        #print(x[:,0:1])
        front = x[:,0:1].repeat(1,(self.kernel_size-1)//2)
        
        # Repeats this tensor along the specified dimensions.
        #print("front",front)
        
        end = x[:,-1].repeat(1,(self.kernel_size-1)//2)
        #print("end",end)
        
# front와 end를 추가해서 moving average를 계산해서 공백이 없도록
        
        x = torch.cat([front, x, end], dim=1)
        #print("cat",x,x.size())
        
        #print('before avg',x.permute(0,2,1))
        x = self.avg(x.permute(0,2,1)) #pooling 계산
        #print(x)
        
        x = x.permute(0,2,1) # 원상복귀
        #print(x)
        return x
143/54:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/55:
# Pytorch로 설게하는 신경망은 반드시 따라야 하는 구조가 있다

'''
1. torch.nn.Module을 상속해야 한다
2. __init()__과 forward()를 override 해야한다.
    (1) override: torch.nn.Module(부모클래스)에서 정의한 메소드를 override 해야한다
    (2) __init()__ 에서는 모델에서 사용될 module(nn.Linear, nn.Conv2d), activation function
    (nn.functional.relu, nn.functional.sigmoid)등을 정의한다
    (3) forward()에서는 모델에서 실행되어야하는 계산을 정의한다. backward 계산은 backward()를
    이용시에 Pytorch가 알아서 해주니깐 forward()만을 정의해주면 된다
    input을 넣어서 어떤 계산을 진행해서 output이 나올지를 정의해준다고 이해하면 된다
'''
class moving_avg(nn.Module):
    "M.A.는 시계열의 trend에 집중하는 것을 막는다"
    def __init__(self,kernel_size,stride):
        super(moving_avg,self).__init__()
        # super(Model_Name,self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size = kernel_size, stride = stride, padding =0)
        # pooling 계산을 하는 1차원 평균으로 pooling계산
    
    def forward(self,x):
        " time series의 양 쪽 끝을 padding 한다"
        #print(x[:,0:1])
        front = x[:,0:1].repeat(1,(self.kernel_size-1)//2)
        print(front)
        # Repeats this tensor along the specified dimensions.
        #print("front",front)
        
        end = x[:,-1].repeat(1,(self.kernel_size-1)//2)
        #print("end",end)
        
# front와 end를 추가해서 moving average를 계산해서 공백이 없도록
        
        x = torch.cat([front, x, end], dim=1)
        #print("cat",x,x.size())
        
        #print('before avg',x.permute(0,2,1))
        x = self.avg(x.permute(0,2,1)) #pooling 계산
        #print(x)
        
        x = x.permute(0,2,1) # 원상복귀
        #print(x)
        return x
143/56:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/57:
# Pytorch로 설게하는 신경망은 반드시 따라야 하는 구조가 있다

'''
1. torch.nn.Module을 상속해야 한다
2. __init()__과 forward()를 override 해야한다.
    (1) override: torch.nn.Module(부모클래스)에서 정의한 메소드를 override 해야한다
    (2) __init()__ 에서는 모델에서 사용될 module(nn.Linear, nn.Conv2d), activation function
    (nn.functional.relu, nn.functional.sigmoid)등을 정의한다
    (3) forward()에서는 모델에서 실행되어야하는 계산을 정의한다. backward 계산은 backward()를
    이용시에 Pytorch가 알아서 해주니깐 forward()만을 정의해주면 된다
    input을 넣어서 어떤 계산을 진행해서 output이 나올지를 정의해준다고 이해하면 된다
'''
class moving_avg(nn.Module):
    "M.A.는 시계열의 trend에 집중하는 것을 막는다"
    def __init__(self,kernel_size,stride):
        super(moving_avg,self).__init__()
        # super(Model_Name,self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size = kernel_size, stride = stride, padding =0)
        # pooling 계산을 하는 1차원 평균으로 pooling계산
    
    def forward(self,x):
        " time series의 양 쪽 끝을 padding 한다"
        #print(x[:,0:1])
        front = x[:,:,0:1].repeat(1,(self.kernel_size-1)//2,1)
        print(front)
        # Repeats this tensor along the specified dimensions.
        #print("front",front)
        
        end = x[:,:,-1].repeat(1,(self.kernel_size-1)//2,1)
        #print("end",end)
        
# front와 end를 추가해서 moving average를 계산해서 공백이 없도록
        
        x = torch.cat([front, x, end], dim=1)
        #print("cat",x,x.size())
        
        #print('before avg',x.permute(0,2,1))
        x = self.avg(x.permute(0,2,1)) #pooling 계산
        #print(x)
        
        x = x.permute(0,2,1) # 원상복귀
        #print(x)
        return x
143/58:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/59:
# Pytorch로 설게하는 신경망은 반드시 따라야 하는 구조가 있다

'''
1. torch.nn.Module을 상속해야 한다
2. __init()__과 forward()를 override 해야한다.
    (1) override: torch.nn.Module(부모클래스)에서 정의한 메소드를 override 해야한다
    (2) __init()__ 에서는 모델에서 사용될 module(nn.Linear, nn.Conv2d), activation function
    (nn.functional.relu, nn.functional.sigmoid)등을 정의한다
    (3) forward()에서는 모델에서 실행되어야하는 계산을 정의한다. backward 계산은 backward()를
    이용시에 Pytorch가 알아서 해주니깐 forward()만을 정의해주면 된다
    input을 넣어서 어떤 계산을 진행해서 output이 나올지를 정의해준다고 이해하면 된다
'''
class moving_avg(nn.Module):
    "M.A.는 시계열의 trend에 집중하는 것을 막는다"
    def __init__(self,kernel_size,stride):
        super(moving_avg,self).__init__()
        # super(Model_Name,self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size = kernel_size, stride = stride, padding =0)
        # pooling 계산을 하는 1차원 평균으로 pooling계산
    
    def forward(self,x):
        " time series의 양 쪽 끝을 padding 한다"
        #print(x[:,0:1])
        front = x[:,:,0:1].repeat(1,(self.kernel_size-1)//2,1)
        print(front)
        # Repeats this tensor along the specified dimensions.
        #print("front",front)
        
        end = x[:,:,-1].repeat(1,(self.kernel_size-1)//2,1)
        #print("end",end)
        
# front와 end를 추가해서 moving average를 계산해서 공백이 없도록
        
        x = torch.cat([front, x, end], dim=2)
        #print("cat",x,x.size())
        
        #print('before avg',x.permute(0,2,1))
        x = self.avg(x.permute(0,2,1)) #pooling 계산
        #print(x)
        
        x = x.permute(0,2,1) # 원상복귀
        #print(x)
        return x
143/60:
x=torch.FloatTensor([2,1,3,4]).view(1,-1,1) #4개의 데이터를 가진 것을 표현
print(x)
print('-----------------')
ma = moving_avg(3,1) 
x = ma.forward(x)
print(x)
143/61:
class series_decomp(nn.Module):
    "statsmodels.tsa의 seasonal_decompose와 역할이 똑같다"
    def __init__(self,kernel_size):
        super(series_decomp,self).__init__()
        self.moving_avg = moving_avg(kernel_size,stride = 1)
    
    def forward(self,x):
        moving_mean = self.moving_avg(x) #output은 (batch,sequence_length,input_size)
        res = x - moving_mean 
        # Classical ma중에서 additive model
        return res,moving_mean
143/62:
x = torch.FloatTensor([1,2,3,4]).view(1,-1,1)
decomp = series_decomp(3) 
# kernel_size를 짝수로하면 오류 발생 (size가 맞지 않은 오류)
# 이경우는 짝수 ma를 두번진행하면 된다(아래 예시)
a,b=decomp.forward(x)
143/63:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = configs.seq_len # configure의 sequence length
        self.pred_len = configs.pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs.individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = configs.enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len])
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
143/64:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len])
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
143/65: torch.ones(2,1,3,dtype=float) # batch는 2 length는 1 input size=3
143/66:
import torch
import torch.nn as nn
import torch.nn.functional as F
# 이 모듈에는 torch.nn 라이브러리의 모든 함수가 포함되어 있다 다양한 손실 및
# 활성화 함수 뿐만 아니라, 풀링(pooling) 함수와 같이 신경망을 만드는데 편리한 몇 가지
# 함수도 여기에서 찾을 수 있다.
import numpy as np
import pandas as pd
143/67:
data = pd.read_csv('../traffic/data/5.csv')
data
143/68: data.drop(columns=['service_name','packets','unknown'])
143/69:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
    return FloatTensor(seq_list).to(device),FloatTensor(target_list).to(device).view(-1,1)
143/70:
split = 720
sequence_length = 360
pred_len = 1
individual = False
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
143/71: x_seq
143/72: target
143/73:
device = torch.device("cuda:0")
torch_data = TensorDataset(x_seq,target)
143/74:
batch_size = 64
train_loader = DataLoader(torch_data,batch_size = batch_size)
143/75:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/76:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/77:
# Pytorch로 설게하는 신경망은 반드시 따라야 하는 구조가 있다

'''
1. torch.nn.Module을 상속해야 한다
2. __init()__과 forward()를 override 해야한다.
    (1) override: torch.nn.Module(부모클래스)에서 정의한 메소드를 override 해야한다
    (2) __init()__ 에서는 모델에서 사용될 module(nn.Linear, nn.Conv2d), activation function
    (nn.functional.relu, nn.functional.sigmoid)등을 정의한다
    (3) forward()에서는 모델에서 실행되어야하는 계산을 정의한다. backward 계산은 backward()를
    이용시에 Pytorch가 알아서 해주니깐 forward()만을 정의해주면 된다
    input을 넣어서 어떤 계산을 진행해서 output이 나올지를 정의해준다고 이해하면 된다
'''
class moving_avg(nn.Module):
    "M.A.는 시계열의 trend에 집중하는 것을 막는다"
    def __init__(self,kernel_size,stride):
        super(moving_avg,self).__init__()
        # super(Model_Name,self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size = kernel_size, stride = stride, padding =0)
        # pooling 계산을 하는 1차원 평균으로 pooling계산
    
    def forward(self,x):
        " time series의 양 쪽 끝을 padding 한다"
        #print(x[:,0:1])
        front = x[:,:,0:1].repeat(1,(self.kernel_size-1)//2,1)
        print(front)
        # Repeats this tensor along the specified dimensions.
        #print("front",front)
        
        end = x[:,:,-1].repeat(1,(self.kernel_size-1)//2,1)
        #print("end",end)
        
# front와 end를 추가해서 moving average를 계산해서 공백이 없도록
        
        x = torch.cat([front, x, end], dim=1)
        #print("cat",x,x.size())
        
        #print('before avg',x.permute(0,2,1))
        x = self.avg(x.permute(0,2,1)) #pooling 계산
        #print(x)
        
        x = x.permute(0,2,1) # 원상복귀
        #print(x)
        return x
143/78:
x=torch.FloatTensor([2,1,3,4]).view(1,-1,1) #4개의 데이터를 가진 것을 표현
print(x)
print('-----------------')
ma = moving_avg(3,1) 
x = ma.forward(x)
print(x)
143/79:
class series_decomp(nn.Module):
    "statsmodels.tsa의 seasonal_decompose와 역할이 똑같다"
    def __init__(self,kernel_size):
        super(series_decomp,self).__init__()
        self.moving_avg = moving_avg(kernel_size,stride = 1)
    
    def forward(self,x):
        moving_mean = self.moving_avg(x) #output은 (batch,sequence_length,input_size)
        res = x - moving_mean 
        # Classical ma중에서 additive model
        return res,moving_mean
143/80:
x = torch.FloatTensor([1,2,3,4]).view(1,-1,1)
decomp = series_decomp(3) 
# kernel_size를 짝수로하면 오류 발생 (size가 맞지 않은 오류)
# 이경우는 짝수 ma를 두번진행하면 된다(아래 예시)
a,b=decomp.forward(x)
143/81:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = configs.seq_len # configure의 sequence length
        self.pred_len = configs.pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs.individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = configs.enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len])
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
143/82:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len])
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
143/83:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/84:
# Pytorch로 설게하는 신경망은 반드시 따라야 하는 구조가 있다

'''
1. torch.nn.Module을 상속해야 한다
2. __init()__과 forward()를 override 해야한다.
    (1) override: torch.nn.Module(부모클래스)에서 정의한 메소드를 override 해야한다
    (2) __init()__ 에서는 모델에서 사용될 module(nn.Linear, nn.Conv2d), activation function
    (nn.functional.relu, nn.functional.sigmoid)등을 정의한다
    (3) forward()에서는 모델에서 실행되어야하는 계산을 정의한다. backward 계산은 backward()를
    이용시에 Pytorch가 알아서 해주니깐 forward()만을 정의해주면 된다
    input을 넣어서 어떤 계산을 진행해서 output이 나올지를 정의해준다고 이해하면 된다
'''
class moving_avg(nn.Module):
    "M.A.는 시계열의 trend에 집중하는 것을 막는다"
    def __init__(self,kernel_size,stride):
        super(moving_avg,self).__init__()
        # super(Model_Name,self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size = kernel_size, stride = stride, padding =0)
        # pooling 계산을 하는 1차원 평균으로 pooling계산
    
    def forward(self,x):
        " time series의 양 쪽 끝을 padding 한다"
        print(x[:,0:1])
        front = x[:,0:1,:].repeat(1,(self.kernel_size-1)//2,1)
        # Repeats this tensor along the specified dimensions.
        #print("front",front)
        
        end = x[:,-1,:].repeat(1,(self.kernel_size-1)//2,1)
        #print("end",end)
        
# front와 end를 추가해서 moving average를 계산해서 공백이 없도록
        
        x = torch.cat([front, x, end], dim=1)
        #print("cat",x,x.size())
        
        #print('before avg',x.permute(0,2,1))
        x = self.avg(x.permute(0,2,1)) #pooling 계산
        #print(x)
        
        x = x.permute(0,2,1) # 원상복귀
        #print(x)
        return x
143/85:
x=torch.FloatTensor([2,1,3,4]).view(1,-1,1) #4개의 데이터를 가진 것을 표현
print(x)
print('-----------------')
ma = moving_avg(3,1) 
x = ma.forward(x)
print(x)
143/86:
class series_decomp(nn.Module):
    "statsmodels.tsa의 seasonal_decompose와 역할이 똑같다"
    def __init__(self,kernel_size):
        super(series_decomp,self).__init__()
        self.moving_avg = moving_avg(kernel_size,stride = 1)
    
    def forward(self,x):
        moving_mean = self.moving_avg(x) #output은 (batch,sequence_length,input_size)
        res = x - moving_mean 
        # Classical ma중에서 additive model
        return res,moving_mean
143/87:
x = torch.FloatTensor([1,2,3,4]).view(1,-1,1)
decomp = series_decomp(3) 
# kernel_size를 짝수로하면 오류 발생 (size가 맞지 않은 오류)
# 이경우는 짝수 ma를 두번진행하면 된다(아래 예시)
a,b=decomp.forward(x)
143/88:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = configs.seq_len # configure의 sequence length
        self.pred_len = configs.pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs.individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = configs.enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len])
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
143/89:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len])
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
143/90: torch.ones(2,1,3,dtype=float) # batch는 2 length는 1 input size=3
143/91:
import torch
import torch.nn as nn
import torch.nn.functional as F
# 이 모듈에는 torch.nn 라이브러리의 모든 함수가 포함되어 있다 다양한 손실 및
# 활성화 함수 뿐만 아니라, 풀링(pooling) 함수와 같이 신경망을 만드는데 편리한 몇 가지
# 함수도 여기에서 찾을 수 있다.
import numpy as np
import pandas as pd
143/92:
data = pd.read_csv('../traffic/data/5.csv')
data
143/93: data.drop(columns=['service_name','packets','unknown'])
143/94:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
    return FloatTensor(seq_list).to(device),FloatTensor(target_list).to(device).view(-1,1)
143/95:
split = 720
sequence_length = 360
pred_len = 1
individual = False
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
143/96: x_seq
143/97: target
143/98: x_seq.size()
143/99: next(iter(train_loader)).size()
143/100: next(iter(train_loader))
143/101: next(iter(train_loader))
143/102:
split = 720
sequence_length = 360
pred_len = 1
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
143/103: x_seq.size()
143/104: target
143/105:
device = torch.device("cuda:0")
torch_data = TensorDataset(x_seq,target)
143/106:
batch_size = 64
train_loader = DataLoader(torch_data,batch_size = batch_size)
143/107: next(iter(train_loader))
143/108:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/109:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/110: seq
143/111:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        seq = seq.cpu().unsqueeze(0).to(device)
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/112:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len])
                print(trend_output)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
143/113:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        seq = seq.cpu().unsqueeze(0).to(device)
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/114:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        print(1)
        self.decomposition = series_decomp(kernel_size)
        print(2)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len])
                print(trend_output)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
143/115:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        seq = seq.cpu().unsqueeze(0).to(device)
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/116:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/117: seq
143/118:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        seq = seq.cpu().unsqueeze(0).to(device)
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/119:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len])
                print(trend_output.size())
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
143/120:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/121: seq
143/122:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        seq = seq.cpu().unsqueeze(0).to(device)
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/123:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len])
                print(trend_output.size())
                print(1)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    print(seasonal_init[:,i,:])
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    print(2)
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
143/124:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/125: seq
143/126:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        seq = seq.cpu().unsqueeze(0).to(device)
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/127:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len])
                print(trend_output.size())
                print(1)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    print(seasonal_init[:,i,:].size())
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    print(2)
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    print(trend_init[:i,:].size())
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
                    print(3)
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
143/128:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        seq = seq.cpu().unsqueeze(0).to(device)
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/129:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/130: seq
143/131:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        seq = seq.cpu().unsqueeze(0).to(device)
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/132:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len])
                print(trend_output.size())
                print(1)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    print(seasonal_init[:,i,:].size())
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    print(2)
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    print(trend_init[:,i,:].size())
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
                    print(3)
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
143/133:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/134: seq
143/135:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        seq = seq.cpu().unsqueeze(0).to(device)
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/136:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/137: seq
143/138:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        seq = seq.cpu().unsqueeze(0).to(device)
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/139:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = configs.seq_len # configure의 sequence length
        self.pred_len = configs.pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs.individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = configs.enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len])
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
143/140:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/141: seq
143/142:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len])
                print(trend_output.size())
                print(1)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    print(seasonal_init[:,i,:].size())
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    print(2)
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    print(trend_init[:,i,:].size())
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
                    print(3)
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
143/143:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/144: seq
143/145:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        seq = seq.cpu().unsqueeze(0).to(device)
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/146:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len])
                print(trend_output.size())
                print(1)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    print(seasonal_init[:,i,:].size())
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    print(2)
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    print(trend_init[:,i,:].size())
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
                    print(3)
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
143/147:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/148: seq
143/149:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        seq = seq.cpu().unsqueeze(0).to(device)
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/150:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len])
                print(trend_output.size())
                print(1)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    print(seasonal_init[:,i,:].size())
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    print(2)
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    print(trend_init[:,i,:].size())
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:].unsqueeze(2))
                    print(3)
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
143/151:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/152: seq
143/153:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        seq = seq.cpu().unsqueeze(0).to(device)
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/154:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len])
                print(trend_output.size())
                print(1)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    print(seasonal_init[:,i,:].size())
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    print(2)
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    print(trend_init[:,i,:].size())
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:].unsqueeze(0))
                    print(3)
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
143/155:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/156: seq
143/157:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        seq = seq.cpu().unsqueeze(0).to(device)
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/158:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len])
                print(trend_output.size())
                print(1)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    print(seasonal_init[:,i,:].size())
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    print(2)
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    print(trend_init[:,i,:].size())
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:].permute(0,2,1))
                    print(3)
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
143/159:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/160: seq
143/161:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        seq = seq.cpu().unsqueeze(0).to(device)
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/162:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len])
                print(trend_output.size())
                print(1)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    print(seasonal_init[:,i,:].size())
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    print(2)
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    print(trend_init[:,i,:].size())
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
                    print(3)
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
143/163:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        seq = seq.cpu().unsqueeze(0).to(device)
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/164:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len])
                print(trend_output.size())
                print(1)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    print(seasonal_init[:,i,:].size())
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    print(seasonal_init)
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    print(trend_init[:,i,:].size())
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
                    print(3)
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
143/165:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len])
                print(trend_output.size())
                print(1)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    print(seasonal_init[:,i,:].size())
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    print(seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    print(trend_init[:,i,:].size())
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
                    print(3)
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
143/166:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/167: seq
143/168:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        seq = seq.cpu().unsqueeze(0).to(device)
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/169: seq
143/170: seq.size()
143/171: seq.size()
143/172: seq.permute(1,2,0)
143/173:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        seq = seq.cpu().unsqueeze(0).permute(1,2,0).to(device)
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/174:
class series_decomp(nn.Module):
    "statsmodels.tsa의 seasonal_decompose와 역할이 똑같다"
    def __init__(self,kernel_size):
        super(series_decomp,self).__init__()
        self.moving_avg = moving_avg(kernel_size,stride = 1)
    
    def forward(self,x):
        moving_mean = self.moving_avg(x) #output은 (batch,sequence_length,input_size)
        res = x - moving_mean 
        # Classical ma중에서 additive model
        print(res)
        print('----')
        print(moving_mean)
        return res,moving_mean
143/175:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len])
                print(trend_output.size())
                print(1)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    print(seasonal_init[:,i,:].size())
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    print(seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    print(trend_init[:,i,:].size())
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
                    print(3)
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
143/176:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/177: seq
143/178:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        seq = seq.cpu().unsqueeze(0).to(device)
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/179:
class series_decomp(nn.Module):
    "statsmodels.tsa의 seasonal_decompose와 역할이 똑같다"
    def __init__(self,kernel_size):
        super(series_decomp,self).__init__()
        self.moving_avg = moving_avg(kernel_size,stride = 1)
    
    def forward(self,x):
        moving_mean = self.moving_avg(x) #output은 (batch,sequence_length,input_size)
        res = x - moving_mean 
        # Classical ma중에서 additive model
        print(res.size())
        print('----')
        print(moving_mean.size()
        return res,moving_mean
143/180:
class series_decomp(nn.Module):
    "statsmodels.tsa의 seasonal_decompose와 역할이 똑같다"
    def __init__(self,kernel_size):
        super(series_decomp,self).__init__()
        self.moving_avg = moving_avg(kernel_size,stride = 1)
    
    def forward(self,x):
        moving_mean = self.moving_avg(x) #output은 (batch,sequence_length,input_size)
        res = x - moving_mean 
        # Classical ma중에서 additive model
        print(res.size())
        print('----')
        print(moving_mean.size())
        return res,moving_mean
143/181:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/182:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        seq = seq.cpu().unsqueeze(0).to(device)
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/183:
class series_decomp(nn.Module):
    "statsmodels.tsa의 seasonal_decompose와 역할이 똑같다"
    def __init__(self,kernel_size):
        super(series_decomp,self).__init__()
        self.moving_avg = moving_avg(kernel_size,stride = 1)
    
    def forward(self,x):
        moving_mean = self.moving_avg(x) #output은 (batch,sequence_length,input_size)
        res = x - moving_mean 
        # Classical ma중에서 additive model
        print(res.size())
        print('----')
        print(moving_mean.size())
        return res.permute(1,2,0),moving_mean.permute(1,2,0)
143/184:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len])
                print(trend_output.size())
                print(1)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    print(seasonal_init[:,i,:].size())
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    print(seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    print(trend_init[:,i,:].size())
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
                    print(3)
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
143/185:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/186:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        seq = seq.cpu().unsqueeze(0).to(device)
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/187:
class series_decomp(nn.Module):
    "statsmodels.tsa의 seasonal_decompose와 역할이 똑같다"
    def __init__(self,kernel_size):
        super(series_decomp,self).__init__()
        self.moving_avg = moving_avg(kernel_size,stride = 1)
    
    def forward(self,x):
        moving_mean = self.moving_avg(x) #output은 (batch,sequence_length,input_size)
        res = x - moving_mean 
        # Classical ma중에서 additive model
        print(res.permute(1,2,0).size())
        print('----')
        print(moving_mean.permute(1,2,0).size())
        return res.permute(1,2,0),moving_mean.permute(1,2,0)
143/188:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len])
                print(trend_output.size())
                print(1)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    print(seasonal_init[:,i,:].size())
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    print(seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    print(trend_init[:,i,:].size())
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
                    print(3)
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
143/189:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
    return FloatTensor(seq_list).to(device),FloatTensor(target_list).to(device).view(-1,1)
143/190:
split = 720
sequence_length = 360
pred_len = 1
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
143/191:
device = torch.device("cuda:0")
torch_data = TensorDataset(x_seq,target)
143/192:
batch_size = 64
train_loader = DataLoader(torch_data,batch_size = batch_size)
143/193: next(iter(train_loader))
143/194:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/195:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        seq = seq.cpu().unsqueeze(0).to(device)
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/196:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len])
                print(trend_output.size())
                print(1)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    print(seasonal_init[:,i,:])
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    print(seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    print(trend_init[:,i,:].size())
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
                    print(3)
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
143/197:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/198:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        seq = seq.cpu().unsqueeze(0).to(device)
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/199:
torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
143/200:
torch.zeros([64,1,1],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
143/201: torch.zeros([64,1,1])
143/202:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            print(seasonal_init.size(),trend_init.size())
            print(0)
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len])
                print(trend_output.size())
                print(1)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    print(seasonal_init[:,i,:])
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    print(seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    print(trend_init[:,i,:].size())
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
                    print(3)
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
143/203:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/204:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        seq = seq.cpu().unsqueeze(0).to(device)
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/205:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            print(seasonal_init.size(),trend_init.size())
            print(0)
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len])
                print(trend_output.size())
                print(1)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    print(seasonal_init[:,i,:])
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    print(seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    print(trend_init[:,i,:].size())
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
                    print(3)
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
143/206:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/207:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        seq = seq.cpu().unsqueeze(0).to(device)
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/208:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            print(seasonal_init.size(),trend_init.size())
            print(0)
   #         seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len])
                print(trend_output.size())
                print(1)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    print(seasonal_init[:,i,:])
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    print(seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    print(trend_init[:,i,:].size())
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
                    print(3)
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
143/209:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/210:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        seq = seq.cpu().unsqueeze(0).to(device)
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/211: torch.zeros([64,360,1])
143/212:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            print(seasonal_init.size(),trend_init.size())
            print(0)
   #         seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                 trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                print(trend_output.size())
                print(1)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    print(seasonal_init[:,i,:])
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    print(seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    print(trend_init[:,i,:].size())
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
                    print(3)
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
143/213:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            print(seasonal_init.size(),trend_init.size())
            print(0)
   #         seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                print(trend_output.size())
                print(1)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    print(seasonal_init[:,i,:])
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    print(seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    print(trend_init[:,i,:].size())
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
                    print(3)
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
143/214:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            print(seasonal_init.size(),trend_init.size())
            print(0)
   #         seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                print(trend_output.size())
                print(1)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    print(seasonal_init[:,i,:])
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    print(seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    print(trend_init[:,i,:].size())
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
                    print(3)
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
143/215:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            print(seasonal_init.size(),trend_init.size())
            print(0)
   #         seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                 trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                print(trend_output.size())
                print(1)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    print(seasonal_init[:,i,:])
                    print(seasonal_init[:,i,:].size())
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    print(seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    print(trend_init[:,i,:].size())
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
                    print(3)
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
143/216:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            print(seasonal_init.size(),trend_init.size())
            print(0)
   #         seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                print(trend_output.size())
                print(1)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    print(seasonal_init[:,i,:])
                    print(seasonal_init[:,i,:].size())
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    print(seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    print(trend_init[:,i,:].size())
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
                    print(3)
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
143/217:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            print(seasonal_init.size(),trend_init.size())
            print(0)
   #         seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                print(trend_output.size())
                print(1)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    print('------------seasonal--------')
                    print(seasonal_init[:,i,:])
                    print(seasonal_init[:,i,:].size())
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    print(seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    print(trend_init[:,i,:].size())
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
                    print(3)
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
143/218:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/219:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        seq = seq.cpu().unsqueeze(0).to(device)
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/220: print(seasonal_init[i,:,:])
143/221:
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    """
    Decomposition-Linear
    """
    def __init__(self, configs):
        super(Model, self).__init__()
        self.seq_len = configs.seq_len
        self.pred_len = configs.pred_len

        # Decompsition Kernel Size
        kernel_size = 25
        self.decompsition = series_decomp(kernel_size)
        self.individual = configs.individual
        self.channels = configs.enc_in

        if self.individual:
            self.Linear_Seasonal = nn.ModuleList()
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))

                # Use this two lines if you want to visualize the weights
                # self.Linear_Seasonal[i].weight = nn.Parameter((1/self.seq_len)*torch.ones([self.pred_len,self.seq_len]))
                # self.Linear_Trend[i].weight = nn.Parameter((1/self.seq_len)*torch.ones([self.pred_len,self.seq_len]))
        else:
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
            # Use this two lines if you want to visualize the weights
            # self.Linear_Seasonal.weight = nn.Parameter((1/self.seq_len)*torch.ones([self.pred_len,self.seq_len]))
            # self.Linear_Trend.weight = nn.Parameter((1/self.seq_len)*torch.ones([self.pred_len,self.seq_len]))

    def forward(self, x):
        # x: [Batch, Input length, Channel]
        seasonal_init, trend_init = self.decompsition(x)
        seasonal_init, trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1)
        if self.individual:
            seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],dtype=seasonal_init.dtype).to(seasonal_init.device)
            trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len],dtype=trend_init.dtype).to(trend_init.device)
            for i in range(self.channels):
                seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
        else:
            seasonal_output = self.Linear_Seasonal(seasonal_init)
            trend_output = self.Linear_Trend(trend_init)

        x = seasonal_output + trend_output
        return x.permute(0,2,1) # to [Batch, Output length, Channel]
143/222:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/223:
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    """
    Decomposition-Linear
    """
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        

    def forward(self, x):
        # x: [Batch, Input length, Channel]
        seasonal_init, trend_init = self.decompsition(x)
        seasonal_init, trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1)
        if self.individual:
            seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],dtype=seasonal_init.dtype).to(seasonal_init.device)
            trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len],dtype=trend_init.dtype).to(trend_init.device)
            for i in range(self.channels):
                seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
        else:
            seasonal_output = self.Linear_Seasonal(seasonal_init)
            trend_output = self.Linear_Trend(trend_init)

        x = seasonal_output + trend_output
        return x.permute(0,2,1) # to [Batch, Output length, Channel]
143/224:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/225:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/226:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        seq = seq.cpu().unsqueeze(0).to(device)
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/227:
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    """
    Decomposition-Linear
    """
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        

    def forward(self, x):
        # x: [Batch, Input length, Channel]
        seasonal_init, trend_init = self.decomposition(x)
        seasonal_init, trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1)
        if self.individual:
            seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],dtype=seasonal_init.dtype).to(seasonal_init.device)
            trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len],dtype=trend_init.dtype).to(trend_init.device)
            for i in range(self.channels):
                seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
        else:
            seasonal_output = self.Linear_Seasonal(seasonal_init)
            trend_output = self.Linear_Trend(trend_init)

        x = seasonal_output + trend_output
        return x.permute(0,2,1) # to [Batch, Output length, Channel]
143/228:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/229:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        seq = seq.cpu().unsqueeze(0).to(device)
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/230:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
    return FloatTensor(seq_list).to(device),FloatTensor(target_list).to(device).view(-1,1)
143/231:
split = 720
sequence_length = 360
pred_len = 1
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
143/232: x_seq
143/233:
for i,j in trainloader:
    i
143/234:
for i,j in train_loader:
    i
143/235:
for i,j in train_loader:
    print(i)
143/236:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
    return FloatTensor(seq_list).unsqueeze(0).to(device),FloatTensor(target_list).unsqueeze(0).view(1,-1,1).to(device)
143/237:
split = 720
sequence_length = 360
pred_len = 1
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
143/238:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/239:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        seq = seq.cpu().unsqueeze(0).to(device)
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/240:
device = torch.device("cuda:0")
torch_data = TensorDataset(x_seq,target)
143/241:
batch_size = 64
train_loader = DataLoader(torch_data,batch_size = batch_size)
143/242:
for i,j in train_loader:
    print(i)
143/243:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/244:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        seq = seq.cpu().unsqueeze(0).to(device)
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/245:
split = 720
sequence_length = 360
pred_len = 1
individual = True
enc_in = 60
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
143/246: x_seq
143/247: target
143/248:
device = torch.device("cuda:0")
torch_data = TensorDataset(x_seq,target)
143/249:
batch_size = 64
train_loader = DataLoader(torch_data,batch_size = batch_size)
143/250:
for i,j in train_loader:
    print(i)
143/251:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/252:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        seq = seq.cpu().unsqueeze(0).to(device)
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/253:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/254:
for i,j in train_loader:
    print(i.cpu().size())
    print(j.cpu().size())
143/255:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
    return FloatTensor(seq_list).unsqueeze(2).to(device),FloatTensor(target_list).unsqueeze(2).view(1,-1,1).to(device)
143/256:
split = 720
sequence_length = 360
pred_len = 1
individual = True
enc_in = 60
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
143/257:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(1,-1,1).to(device)
143/258:
split = 720
sequence_length = 360
pred_len = 1
individual = True
enc_in = 60
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
143/259:
split = 720
sequence_length = 360
pred_len = 1
individual = True
enc_in = 60
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
print(x_seq,target)
143/260:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,1).to(device)
143/261:
split = 720
sequence_length = 360
pred_len = 1
individual = True
enc_in = 60
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
print(x_seq,target)
143/262: x_seq
143/263:
device = torch.device("cuda:0")
torch_data = TensorDataset(x_seq,target)
143/264:
batch_size = 64
train_loader = DataLoader(torch_data,batch_size = batch_size)
143/265:
for i,j in train_loader:
    print(i.cpu().size())
    print(j.cpu().size())
143/266:
count =1
for i,j in train_loader:
    print(i.cpu().size())
    print(j.cpu().size())
    count+=1
print(count)
143/267:
split = 720
sequence_length = 360
pred_len = 1
individual = True
enc_in = 60
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
print(x_seq,target)
143/268: x_seq
143/269: target
143/270:
device = torch.device("cuda:0")
torch_data = TensorDataset(x_seq,target)
143/271:
batch_size = 64
train_loader = DataLoader(torch_data,batch_size = batch_size)
143/272:
count =1
for i,j in train_loader:
    print(i.cpu().size())
    print(j.cpu().size())
    count+=1
print(count)
143/273:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/274:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/275:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
    return FloatTensor(seq_list).unsqueeze(1).permute(0,2,1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,1).to(device)
143/276:
split = 720
sequence_length = 360
pred_len = 1
individual = True
enc_in = 60
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
print(x_seq,target)
143/277: x_seq
143/278: target
143/279:
device = torch.device("cuda:0")
torch_data = TensorDataset(x_seq,target)
143/280:
batch_size = 64
train_loader = DataLoader(torch_data,batch_size = batch_size)
143/281:
count =1
for i,j in train_loader:
    print(i.cpu().size())
    print(j.cpu().size())
    count+=1
print(count)
143/282:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/283:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/284:
split = 720
sequence_length = 360
pred_len = 1
individual = True
enc_in = 360
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
print(x_seq,target)
143/285: target
143/286:
device = torch.device("cuda:0")
torch_data = TensorDataset(x_seq,target)
143/287: x_seq
143/288: target
143/289:
device = torch.device("cuda:0")
torch_data = TensorDataset(x_seq,target)
143/290:
batch_size = 64
train_loader = DataLoader(torch_data,batch_size = batch_size)
143/291:
count =1
for i,j in train_loader:
    print(i.cpu().size())
    print(j.cpu().size())
    count+=1
print(count)
143/292:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/293:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/294:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,1).to(device)
143/295:
split = 720
sequence_length = 360
pred_len = 1
individual = True
enc_in = 360
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
print(x_seq,target)
143/296:
device = torch.device("cuda:0")
torch_data = TensorDataset(x_seq,target)
143/297:
batch_size = 64
train_loader = DataLoader(torch_data,batch_size = batch_size)
143/298:
count =1
for i,j in train_loader:
    print(i.cpu().size())
    print(j.cpu().size())
    count+=1
print(count)
143/299:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/300:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/301:
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    """
    Decomposition-Linear
    """
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        

    def forward(self, x):
        # x: [Batch, Input length, Channel]
        seasonal_init, trend_init = self.decomposition(x)
        #seasonal_init, trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1)
        if self.individual:
            seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],dtype=seasonal_init.dtype).to(seasonal_init.device)
            trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len],dtype=trend_init.dtype).to(trend_init.device)
            for i in range(self.channels):
                seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
        else:
            seasonal_output = self.Linear_Seasonal(seasonal_init)
            trend_output = self.Linear_Trend(trend_init)

        x = seasonal_output + trend_output
        return x.permute(0,2,1) # to [Batch, Output length, Channel]
143/302:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,1).to(device)
143/303:
split = 720
sequence_length = 360
pred_len = 1
individual = True
enc_in = 360
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
print(x_seq,target)
143/304: target
143/305:
device = torch.device("cuda:0")
torch_data = TensorDataset(x_seq,target)
143/306: x_seq
143/307: target
143/308:
device = torch.device("cuda:0")
torch_data = TensorDataset(x_seq,target)
143/309:
batch_size = 64
train_loader = DataLoader(torch_data,batch_size = batch_size)
143/310:
count =1
for i,j in train_loader:
    print(i.cpu().size())
    print(j.cpu().size())
    count+=1
print(count)
143/311:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/312:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/313:
split = 720
sequence_length = 360
pred_len = 1
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
print(x_seq,target)
143/314: x_seq
143/315: target
143/316:
device = torch.device("cuda:0")
torch_data = TensorDataset(x_seq,target)
143/317:
batch_size = 64
train_loader = DataLoader(torch_data,batch_size = batch_size)
143/318:
count =1
for i,j in train_loader:
    print(i.cpu().size())
    print(j.cpu().size())
    count+=1
print(count)
143/319:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/320:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/321:
split = 720
sequence_length = 360
pred_len = 1
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
print(x_seq,target)
143/322: x_seq
143/323: target
143/324:
device = torch.device("cuda:0")
torch_data = TensorDataset(x_seq,target)
143/325:
batch_size = 64
train_loader = DataLoader(torch_data,batch_size = batch_size)
143/326:
count =1
for i,j in train_loader:
    print(i.cpu().size())
    print(j.cpu().size())
    count+=1
print(count)
143/327:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/328:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    out_list.append(out)
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/329: out_list
143/330: pd.Series(out_list[0].cpu().tolist()).plot()
143/331: pd.Series(out_list[0].cpu().items()).plot()
143/332: pd.Series(out_list[0].cpu().item().plot()
143/333: pd.Series(out_list[0].cpu().item()).plot()
143/334: out_list[0].cpu().item()
143/335: out_list[0].cpu()
143/336: out_list[0].cpu()[0]
143/337: out_list[0].cpu().tolist()
143/338: out_list[0].cpu().squeeze(0)
143/339:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        out_list.append(out)
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/340: out_list[0].cpu().
143/341: out_list[0].cpu()
143/342: out_list.cpu()
143/343: out_list
143/344:
split = 720
sequence_length = 1440
pred_len = 720
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
print(x_seq,target)
143/345:
split = 360
sequence_length = 720
pred_len = 360
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
print(x_seq,target)
143/346: x_seq
143/347: target
143/348:
device = torch.device("cuda:0")
torch_data = TensorDataset(x_seq,target)
143/349:
batch_size = 64
train_loader = DataLoader(torch_data,batch_size = batch_size)
143/350:
count =1
for i,j in train_loader:
    print(i.cpu().size())
    print(j.cpu().size())
    count+=1
print(count)
143/351:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/352:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/353:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/354:
batch_size = 32
train_loader = DataLoader(torch_data,batch_size = batch_size)
143/355:
count =1
for i,j in train_loader:
    print(i.cpu().size())
    print(j.cpu().size())
    count+=1
print(count)
143/356:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/357:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
143/358:
batch_size = 32
train_loader = DataLoader(torch_data,batch_size = 8)
143/359:
count =1
for i,j in train_loader:
    print(i.cpu().size())
    print(j.cpu().size())
    count+=1
print(count)
143/360:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/361:
batch_size = 32
train_loader = DataLoader(torch_data,batch_size = 4
143/362:
count =1
for i,j in train_loader:
    print(i.cpu().size())
    print(j.cpu().size())
    count+=1
print(count)
143/363:
batch_size = 32
train_loader = DataLoader(torch_data,batch_size = 4)
143/364:
count =1
for i,j in train_loader:
    print(i.cpu().size())
    print(j.cpu().size())
    count+=1
print(count)
143/365:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
143/366:
split = 180
sequence_length = 360
pred_len = 180
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
print(x_seq,target)
145/1:
import torch
import torch.nn as nn
import torch.nn.functional as F
# 이 모듈에는 torch.nn 라이브러리의 모든 함수가 포함되어 있다 다양한 손실 및
# 활성화 함수 뿐만 아니라, 풀링(pooling) 함수와 같이 신경망을 만드는데 편리한 몇 가지
# 함수도 여기에서 찾을 수 있다.
import numpy as np
import torch.optim as optim
from torch.utils.data import TensorDataset,DataLoader
from torch import FloatTensor
145/2:
# Pytorch로 설게하는 신경망은 반드시 따라야 하는 구조가 있다

'''
1. torch.nn.Module을 상속해야 한다
2. __init()__과 forward()를 override 해야한다.
    (1) override: torch.nn.Module(부모클래스)에서 정의한 메소드를 override 해야한다
    (2) __init()__ 에서는 모델에서 사용될 module(nn.Linear, nn.Conv2d), activation function
    (nn.functional.relu, nn.functional.sigmoid)등을 정의한다
    (3) forward()에서는 모델에서 실행되어야하는 계산을 정의한다. backward 계산은 backward()를
    이용시에 Pytorch가 알아서 해주니깐 forward()만을 정의해주면 된다
    input을 넣어서 어떤 계산을 진행해서 output이 나올지를 정의해준다고 이해하면 된다
'''
class moving_avg(nn.Module):
    "M.A.는 시계열의 trend에 집중하는 것을 막는다"
    def __init__(self,kernel_size,stride):
        super(moving_avg,self).__init__()
        # super(Model_Name,self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size = kernel_size, stride = stride, padding =0)
        # pooling 계산을 하는 1차원 평균으로 pooling계산
    
    def forward(self,x):
        " time series의 양 쪽 끝을 padding 한다"
        print(x[:,0:1])
        front = x[:,0:1,:].repeat(1,(self.kernel_size-1)//2,1)
        # Repeats this tensor along the specified dimensions.
        #print("front",front)
        
        end = x[:,-1,:].repeat(1,(self.kernel_size-1)//2,1)
        #print("end",end)
        
# front와 end를 추가해서 moving average를 계산해서 공백이 없도록
        
        x = torch.cat([front, x, end], dim=1)
        #print("cat",x,x.size())
        
        #print('before avg',x.permute(0,2,1))
        x = self.avg(x.permute(0,2,1)) #pooling 계산
        #print(x)
        
        x = x.permute(0,2,1) # 원상복귀
        #print(x)
        return x
145/3:
x=torch.FloatTensor([2,1,3,4]).view(1,-1,1) #4개의 데이터를 가진 것을 표현
print(x)
print('-----------------')
ma = moving_avg(3,1) 
x = ma.forward(x)
print(x)
145/4:
class series_decomp(nn.Module):
    "statsmodels.tsa의 seasonal_decompose와 역할이 똑같다"
    def __init__(self,kernel_size):
        super(series_decomp,self).__init__()
        self.moving_avg = moving_avg(kernel_size,stride = 1)
    
    def forward(self,x):
        moving_mean = self.moving_avg(x) #output은 (batch,sequence_length,input_size)
        res = x - moving_mean 
        # Classical ma중에서 additive model
        print(res.permute(1,2,0).size())
        print('----')
        print(moving_mean.permute(1,2,0).size())
        return res.permute(1,2,0),moving_mean.permute(1,2,0)
145/5:
x = torch.FloatTensor([1,2,3,4]).view(1,-1,1)
decomp = series_decomp(3) 
# kernel_size를 짝수로하면 오류 발생 (size가 맞지 않은 오류)
# 이경우는 짝수 ma를 두번진행하면 된다(아래 예시)
a,b=decomp.forward(x)
145/6:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = configs.seq_len # configure의 sequence length
        self.pred_len = configs.pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs.individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = configs.enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
145/7:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            print(seasonal_init.size(),trend_init.size())
            print(0)
   #         seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                print(trend_output.size())
                print(1)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    print('------------seasonal--------')
                    print(seasonal_init[i,:,:])
                    print(seasonal_init[i,:,:].size())
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[i,:,:])
                   
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    print(trend_init[:,i,:].size())
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
                    print(3)
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
145/8:
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    """
    Decomposition-Linear
    """
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        

    def forward(self, x):
        # x: [Batch, Input length, Channel]
        seasonal_init, trend_init = self.decomposition(x)
        seasonal_init, trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1)
        if self.individual:
            seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],dtype=seasonal_init.dtype).to(seasonal_init.device)
            trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len],dtype=trend_init.dtype).to(trend_init.device)
            for i in range(self.channels):
                seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
        else:
            seasonal_output = self.Linear_Seasonal(seasonal_init)
            trend_output = self.Linear_Trend(trend_init)

        x = seasonal_output + trend_output
        return x.permute(0,2,1) # to [Batch, Output length, Channel]
145/9: torch.ones(2,1,3,dtype=float) # batch는 2 length는 1 input size=3
145/10:
import torch
import torch.nn as nn
import torch.nn.functional as F
# 이 모듈에는 torch.nn 라이브러리의 모든 함수가 포함되어 있다 다양한 손실 및
# 활성화 함수 뿐만 아니라, 풀링(pooling) 함수와 같이 신경망을 만드는데 편리한 몇 가지
# 함수도 여기에서 찾을 수 있다.
import numpy as np
import pandas as pd
145/11:
data = pd.read_csv('../traffic/data/5.csv')
data
145/12: data.drop(columns=['service_name','packets','unknown'])
145/13:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,1).to(device)
145/14:
split = 180
sequence_length = 360
pred_len = 180
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
print(x_seq,target)
145/15: x_seq
145/16: target
145/17:
device = torch.device("cuda:0")
torch_data = TensorDataset(x_seq,target)
145/18:
batch_size = 32
train_loader = DataLoader(torch_data,batch_size = 4)
145/19:
count =1
for i,j in train_loader:
    print(i.cpu().size())
    print(j.cpu().size())
    count+=1
print(count)
145/20:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
145/21:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
145/22:
batch_size = 32
train_loader = DataLoader(torch_data,batch_size = batch_size)
145/23:
count =1
for i,j in train_loader:
    print(i.cpu().size())
    print(j.cpu().size())
    count+=1
print(count)
145/24:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
145/25:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
145/26:
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    """
    Decomposition-Linear
    """
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        

    def forward(self, x):
        # x: [Batch, Input length, Channel]
        seasonal_init, trend_init = self.decomposition(x)
        seasonal_init, trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1)
        if self.individual:
            seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],dtype=seasonal_init.dtype).to(seasonal_init.device)
            trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len],dtype=trend_init.dtype).to(trend_init.device)
            for i in range(self.channels):
                seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
        else:
            seasonal_output = self.Linear_Seasonal(seasonal_init)
            trend_output = self.Linear_Trend(trend_init)

        x = seasonal_output + trend_output
        return x.permute(0,2,1) # to [Batch, Output length, Channel]
145/27: torch.ones(2,1,3,dtype=float) # batch는 2 length는 1 input size=3
145/28:
import torch
import torch.nn as nn
import torch.nn.functional as F
# 이 모듈에는 torch.nn 라이브러리의 모든 함수가 포함되어 있다 다양한 손실 및
# 활성화 함수 뿐만 아니라, 풀링(pooling) 함수와 같이 신경망을 만드는데 편리한 몇 가지
# 함수도 여기에서 찾을 수 있다.
import numpy as np
import pandas as pd
145/29:
data = pd.read_csv('../traffic/data/5.csv')
data
145/30: data.drop(columns=['service_name','packets','unknown'])
145/31:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,1).to(device)
145/32:
split = 180
sequence_length = 360
pred_len = 180
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
print(x_seq,target)
145/33: x_seq
145/34: target
145/35:
device = torch.device("cuda:0")
torch_data = TensorDataset(x_seq,target)
145/36:
batch_size = 32
train_loader = DataLoader(torch_data,batch_size = batch_size)
145/37:
count =1
for i,j in train_loader:
    print(i.cpu().size())
    print(j.cpu().size())
    count+=1
print(count)
145/38:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
145/39:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
145/40:
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    """
    Decomposition-Linear
    """
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        

    def forward(self, x):
        # x: [Batch, Input length, Channel]
        seasonal_init, trend_init = self.decomposition(x)
        seasonal_init, trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1)
        if self.individual:
            seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],dtype=seasonal_init.dtype).to(seasonal_init.device)
            trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len],dtype=trend_init.dtype).to(trend_init.device)
            for i in range(self.channels):
                seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
        else:
            seasonal_output = self.Linear_Seasonal(seasonal_init)
            trend_output = self.Linear_Trend(trend_init)

        x = seasonal_output + trend_output
        return x.permute(0,2,1) # to [Batch, Output length, Channel]
145/41:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,1).to(device)
145/42:
split = 180
sequence_length = 360
pred_len = 180
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
print(x_seq,target)
145/43: x_seq
145/44: target
145/45:
device = torch.device("cuda:0")
torch_data = TensorDataset(x_seq,target)
145/46:
batch_size = 32
train_loader = DataLoader(torch_data,batch_size = batch_size)
145/47:
count =1
for i,j in train_loader:
    print(i.cpu().size())
    print(j.cpu().size())
    count+=1
print(count)
145/48:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
145/49:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
145/50:
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    """
    Decomposition-Linear
    """
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        

    def forward(self, x):
        # x: [Batch, Input length, Channel]
        seasonal_init, trend_init = self.decomposition(x)
        print(seasonal_init,trend_init)
        seasonal_init, trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1)
        if self.individual:
            seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],dtype=seasonal_init.dtype).to(seasonal_init.device)
            trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len],dtype=trend_init.dtype).to(trend_init.device)
            for i in range(self.channels):
                seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
        else:
            seasonal_output = self.Linear_Seasonal(seasonal_init)
            trend_output = self.Linear_Trend(trend_init)

        x = seasonal_output + trend_output
        return x.permute(0,2,1) # to [Batch, Output length, Channel]
145/51: torch.ones(2,1,3,dtype=float) # batch는 2 length는 1 input size=3
145/52:
import torch
import torch.nn as nn
import torch.nn.functional as F
# 이 모듈에는 torch.nn 라이브러리의 모든 함수가 포함되어 있다 다양한 손실 및
# 활성화 함수 뿐만 아니라, 풀링(pooling) 함수와 같이 신경망을 만드는데 편리한 몇 가지
# 함수도 여기에서 찾을 수 있다.
import numpy as np
import pandas as pd
145/53:
data = pd.read_csv('../traffic/data/5.csv')
data
145/54: data.drop(columns=['service_name','packets','unknown'])
145/55:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,1).to(device)
145/56:
split = 180
sequence_length = 360
pred_len = 180
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
print(x_seq,target)
145/57: x_seq
145/58: target
145/59:
device = torch.device("cuda:0")
torch_data = TensorDataset(x_seq,target)
145/60:
batch_size = 32
train_loader = DataLoader(torch_data,batch_size = batch_size)
145/61:
count =1
for i,j in train_loader:
    print(i.cpu().size())
    print(j.cpu().size())
    count+=1
print(count)
145/62:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
145/63:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
145/64:
split = 180
sequence_length = 360
pred_len = 180
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
print(x_seq.shape,target.shape)
print(x_seq.permute(0,2,1),target)
145/65:
split = 180
sequence_length = 360
pred_len = 180
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
print(x_seq.permute(0,2,1),target)
145/66:
split = 180
sequence_length = 360
pred_len = 180
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
145/67:
batch_size = 32
train_loader = DataLoader(torch_data,batch_size = batch_size)
145/68:
device = torch.device("cuda:0")
torch_data = TensorDataset(x_seq,target)
145/69:
batch_size = 32
train_loader = DataLoader(torch_data,batch_size = batch_size)
145/70:
count =1
for i,j in train_loader:
    print(i.cpu().size())
    print(j.cpu().size())
    count+=1
print(count)
145/71:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
145/72:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
145/73:
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    """
    Decomposition-Linear
    """
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        

    def forward(self, x):
        # x: [Batch, Input length, Channel]
        seasonal_init, trend_init = self.decomposition(x)
        seasonal_init, trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1)
        if self.individual:
            seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],dtype=seasonal_init.dtype).to(seasonal_init.device)
            trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len],dtype=trend_init.dtype).to(trend_init.device)
            for i in range(self.channels):
                seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
        else:
            seasonal_output = self.Linear_Seasonal(seasonal_init)
            trend_output = self.Linear_Trend(trend_init)

        x = seasonal_output + trend_output
        return x.permute(0,2,1) # to [Batch, Output length, Channel]
145/74: torch.ones(2,1,3,dtype=float) # batch는 2 length는 1 input size=3
145/75:
import torch
import torch.nn as nn
import torch.nn.functional as F
# 이 모듈에는 torch.nn 라이브러리의 모든 함수가 포함되어 있다 다양한 손실 및
# 활성화 함수 뿐만 아니라, 풀링(pooling) 함수와 같이 신경망을 만드는데 편리한 몇 가지
# 함수도 여기에서 찾을 수 있다.
import numpy as np
import pandas as pd
145/76:
data = pd.read_csv('../traffic/data/5.csv')
data
145/77: data.drop(columns=['service_name','packets','unknown'])
145/78:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,1).to(device)
145/79:
split = 180
sequence_length = 360
pred_len = 180
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
145/80:
device = torch.device("cuda:0")
torch_data = TensorDataset(x_seq,target)
145/81:
batch_size = 32
train_loader = DataLoader(torch_data,batch_size = batch_size)
145/82:
count =1
for i,j in train_loader:
    print(i.cpu().size())
    print(j.cpu().size())
    count+=1
print(count)
145/83:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
145/84:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
145/85: out_list
145/86: X[-sequence_length:].tolist()
145/87: len(X[-sequence_length:].tolist())
145/88:
out_list = []
Y = data[['volumn']].values[-2*split:-split]



with torch.no_grad():
    model.eval()
    #if num==0:
    new_train=X[-sequence_length:].tolist()
#     else:
#         new_train = new_train[1:]+out.cpu().tolist()
        #print(type(new_train))
    new_tr = torch.FloatTensor(new_train).unsqueeze(0).to(device)
    out = model(new_tr)
#    out_list.append(out)
#   loss_graph.append(running_loss/n)
print(out)
145/89:
out_list = []
Y = data[['volumn']].values[-2*split:-split]



with torch.no_grad():
    model.eval()
    #if num==0:
    new_train=[X[-sequence_length:].tolist()]
#     else:
#         new_train = new_train[1:]+out.cpu().tolist()
        #print(type(new_train))
    new_tr = torch.FloatTensor(new_train).unsqueeze(0).to(device)
    out = model(new_tr)
#    out_list.append(out)
#   loss_graph.append(running_loss/n)
print(out)
145/90: X[-sequence_length:].tolist()
145/91: [X[-sequence_length:].tolist()]
145/92: torch.FloatTensor([X[-sequence_length:].tolist()]).unsqueeze(1)
145/93:
out_list = []
Y = data[['volumn']].values[-2*split:-split]



with torch.no_grad():
    model.eval()
    #if num==0:
    new_train=[X[-sequence_length:].tolist()]
#     else:
#         new_train = new_train[1:]+out.cpu().tolist()
        #print(type(new_train))
    new_tr = torch.FloatTensor(new_train).unsqueeze(1).to(device)
    out = model(new_tr)
#    out_list.append(out)
#   loss_graph.append(running_loss/n)
print(out)
145/94: torch.FloatTensor([X[-sequence_length:].tolist()]).unsqueeze(1).size()
145/95: torch.FloatTensor([X[-sequence_length:].tolist()]).unsqueeze(0).size()
145/96: torch.FloatTensor([X[-sequence_length:].tolist()]).unsqueeze(2).size()
145/97: torch.FloatTensor([X[-sequence_length:].tolist()]).unsqueeze(2)
145/98:
out_list = []
Y = data[['volumn']].values[-2*split:-split]



with torch.no_grad():
    model.eval()
    #if num==0:
    new_train=[X[-sequence_length:].tolist()]
#     else:
#         new_train = new_train[1:]+out.cpu().tolist()
        #print(type(new_train))
    new_tr = torch.FloatTensor(new_train).unsqueeze(2).to(device)
    out = model(new_tr)
#    out_list.append(out)
#   loss_graph.append(running_loss/n)
print(out)
145/99:
plot_data = out.cpu().tolist()
plot_data
145/100:
plot_data = np.array(out.cpu().tolist()).reshape(1,-1)
plot_data
145/101:
plot_data = np.array(out.cpu().tolist()).reshape(1,-1)[0]
plot_data
145/102:
plot_data = np.array(out.cpu().tolist()).reshape(1,-1)[0]
pd.series(np.arange(plot_data),plot_data)
145/103:
plot_data = np.array(out.cpu().tolist()).reshape(1,-1)[0]
plt.plot(pd.series(np.arange(plot_data))1,plot_data)
145/104:
plot_data = np.array(out.cpu().tolist()).reshape(1,-1)[0]
plt.plot(pd.series(np.arange(plot_data)),plot_data)
145/105:
import torch
import torch.nn as nn
import torch.nn.functional as F
# 이 모듈에는 torch.nn 라이브러리의 모든 함수가 포함되어 있다 다양한 손실 및
# 활성화 함수 뿐만 아니라, 풀링(pooling) 함수와 같이 신경망을 만드는데 편리한 몇 가지
# 함수도 여기에서 찾을 수 있다.
import numpy as np
import torch.optim as optim
import matplotlib.pyplot as plt
from torch.utils.data import TensorDataset,DataLoader
from torch import FloatTensor
145/106:
plot_data = np.array(out.cpu().tolist()).reshape(1,-1)[0]
plt.plot(pd.series(np.arange(plot_data)),plot_data)
145/107:
plot_data = np.array(out.cpu().tolist()).reshape(1,-1)[0]
plt.plot(np.arange(plot_data),plot_data)
145/108:
plot_data = np.array(out.cpu().tolist()).reshape(1,-1)[0]
plt.plot(np.arange(len(plot_data)),plot_data)
145/109:
plot_data = np.array(out.cpu().tolist()).reshape(1,-1)[0]
plt.plot(np.arange(len(plot_data)),plot_data)
plt.plot(np.arange(len(plot_data)),X[-split:])
145/110:
import torch
import torch.nn as nn
import torch.nn.functional as F
# 이 모듈에는 torch.nn 라이브러리의 모든 함수가 포함되어 있다 다양한 손실 및
# 활성화 함수 뿐만 아니라, 풀링(pooling) 함수와 같이 신경망을 만드는데 편리한 몇 가지
# 함수도 여기에서 찾을 수 있다.
import numpy as np
import torch.optim as optim
import matplotlib.pyplot as plt
from torch.utils.data import TensorDataset,DataLoader
from torch import FloatTensor
from sklearn.preprocessing import MinMaxScaler
145/111:
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    """
    Decomposition-Linear
    """
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        

    def forward(self, x):
        # x: [Batch, Input length, Channel]
        seasonal_init, trend_init = self.decomposition(x)# res = seasonality 를 moving_mean은 trend를 의미
        # seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
        seasonal_init, trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1)
        if self.individual:
            seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],dtype=seasonal_init.dtype).to(seasonal_init.device)
             # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
            trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len],dtype=trend_init.dtype).to(trend_init.device)
            for i in range(self.channels):
                seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
                # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                # 이때 RNN과 달리 한번에 학습을 진행하기 때문에 seq의 배열 자체가 feature가 됨
        else:
            seasonal_output = self.Linear_Seasonal(seasonal_init)
            trend_output = self.Linear_Trend(trend_init)

        x = seasonal_output + trend_output
        return x.permute(0,2,1) # [batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)]로 변경해서 return
145/112: torch.ones(2,1,3,dtype=float) # batch는 2 length는 1 input size=3
145/113:
import torch
import torch.nn as nn
import torch.nn.functional as F
# 이 모듈에는 torch.nn 라이브러리의 모든 함수가 포함되어 있다 다양한 손실 및
# 활성화 함수 뿐만 아니라, 풀링(pooling) 함수와 같이 신경망을 만드는데 편리한 몇 가지
# 함수도 여기에서 찾을 수 있다.
import numpy as np
import pandas as pd
145/114:
data = pd.read_csv('../traffic/data/5.csv')
data
145/115: data.drop(columns=['service_name','packets','unknown'])
145/116:
scaler = MinMaxScaler()
data[['volumn']] = scaler.fit_transform(data[['volumn']])
data
145/117:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,1).to(device)
145/118:
split = 180
sequence_length = 360
pred_len = 180
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
145/119:
device = torch.device("cuda:0")
torch_data = TensorDataset(x_seq,target)
145/120:
batch_size = 32
train_loader = DataLoader(torch_data,batch_size = batch_size)
145/121:
split = 60
sequence_length = 1440
pred_len = 60
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
145/122:
device = torch.device("cuda:0")
torch_data = TensorDataset(x_seq,target)
145/123:
batch_size = 32
train_loader = DataLoader(torch_data,batch_size = batch_size)
145/124:
count =1
for i,j in train_loader:
    print(i.cpu().size())
    print(j.cpu().size())
    count+=1
print(count)
145/125:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
145/126:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
145/127:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-2)
145/128:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
145/129:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,1).to(device)
145/130:
split = 60
sequence_length = 1440
pred_len = 60
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
145/131:
device = torch.device("cuda:0")
torch_data = TensorDataset(x_seq,target)
145/132:
batch_size = 32
train_loader = DataLoader(torch_data,batch_size = batch_size)
145/133:
count =1
for i,j in train_loader:
    print(i.cpu().size())
    print(j.cpu().size())
    count+=1
print(count)
145/134:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-4)
145/135:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
145/136:
out_list = []
Y = data[['volumn']].values[-2*split:-split]



with torch.no_grad():
    model.eval()
    #if num==0:
    new_train=[X[-sequence_length:].tolist()]
#     else:
#         new_train = new_train[1:]+out.cpu().tolist()
        #print(type(new_train))
    new_tr = torch.FloatTensor(new_train).unsqueeze(2).to(device)
    out = model(new_tr)
#    out_list.append(out)
#   loss_graph.append(running_loss/n)
print(out)
145/137:
plot_data = np.array(out.cpu().tolist()).reshape(1,-1)[0]
plot_data
145/138:
plot_data = np.array(out.cpu().tolist()).reshape(1,-1)[0]
plt.plot(np.arange(len(plot_data)),plot_data)
plt.plot(np.arange(len(plot_data)),X[-split:])
145/139:
plot_data = np.array(out.cpu().tolist()).reshape(1,-1)[0]
plt.plot(np.arange(len(plot_data)),plot_data,labels = 'predcition')
plt.plot(np.arange(len(plot_data)),X[-split:],labels = 'real')
plt.legend()
145/140:
plot_data = np.array(out.cpu().tolist()).reshape(1,-1)[0]
plt.plot(np.arange(len(plot_data)),plot_data,label = 'predcition')
plt.plot(np.arange(len(plot_data)),X[-split:],label = 'real')
plt.legend()
145/141:
split = 360
sequence_length = 1440
pred_len = 360
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
145/142:
device = torch.device("cuda:0")
torch_data = TensorDataset(x_seq,target)
145/143:
batch_size = 32
train_loader = DataLoader(torch_data,batch_size = batch_size)
145/144:
count =1
for i,j in train_loader:
    print(i.cpu().size())
    print(j.cpu().size())
    count+=1
print(count)
145/145:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-4)
145/146:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
145/147:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
145/148:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
145/149: torch.FloatTensor([X[-sequence_length:].tolist()]).unsqueeze(2)
145/150:
out_list = []
Y = data[['volumn']].values[-2*split:-split]



with torch.no_grad():
    model.eval()
    #if num==0:
    new_train=[X[-sequence_length:].tolist()]
#     else:
#         new_train = new_train[1:]+out.cpu().tolist()
        #print(type(new_train))
    new_tr = torch.FloatTensor(new_train).unsqueeze(2).to(device)
    out = model(new_tr)
#    out_list.append(out)
#   loss_graph.append(running_loss/n)
print(out)
145/151:
plot_data = np.array(out.cpu().tolist()).reshape(1,-1)[0]
plot_data
145/152:
plot_data = np.array(out.cpu().tolist()).reshape(1,-1)[0]
plt.plot(np.arange(len(plot_data)),plot_data,label = 'predcition')
plt.plot(np.arange(len(plot_data)),X[-split:],label = 'real')
plt.legend()
145/153:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-4)
145/154:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
145/155: torch.FloatTensor([X[-sequence_length:].tolist()]).unsqueeze(2)
145/156:
out_list = []
Y = data[['volumn']].values[-2*split:-split]



with torch.no_grad():
    model.eval()
    #if num==0:
    new_train=[X[-sequence_length:].tolist()]
#     else:
#         new_train = new_train[1:]+out.cpu().tolist()
        #print(type(new_train))
    new_tr = torch.FloatTensor(new_train).unsqueeze(2).to(device)
    out = model(new_tr)
#    out_list.append(out)
#   loss_graph.append(running_loss/n)
print(out)
145/157:
plot_data = np.array(out.cpu().tolist()).reshape(1,-1)[0]
plot_data
145/158:
plot_data = np.array(out.cpu().tolist()).reshape(1,-1)[0]
plt.plot(np.arange(len(plot_data)),plot_data,label = 'predcition')
plt.plot(np.arange(len(plot_data)),X[-split:],label = 'real')
plt.legend()
145/159:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
# Q) 왜 learning_rate를 조정하면 MSE값의 차이가 크게나지
145/160:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
145/161: torch.FloatTensor([X[-sequence_length:].tolist()]).unsqueeze(2)
145/162:
out_list = []
Y = data[['volumn']].values[-2*split:-split]



with torch.no_grad():
    model.eval()
    #if num==0:
    new_train=[X[-sequence_length:].tolist()]
#     else:
#         new_train = new_train[1:]+out.cpu().tolist()
        #print(type(new_train))
    new_tr = torch.FloatTensor(new_train).unsqueeze(2).to(device)
    out = model(new_tr)
#    out_list.append(out)
#   loss_graph.append(running_loss/n)
print(out)
145/163:
plot_data = np.array(out.cpu().tolist()).reshape(1,-1)[0]
plot_data
145/164:
plot_data = np.array(out.cpu().tolist()).reshape(1,-1)[0]
plt.plot(np.arange(len(plot_data)),plot_data,label = 'predcition')
plt.plot(np.arange(len(plot_data)),X[-split:],label = 'real')
plt.legend()
145/165:
split = 180
sequence_length = 1440
pred_len = 180
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
145/166:
device = torch.device("cuda:0")
torch_data = TensorDataset(x_seq,target)
145/167:
batch_size = 32
train_loader = DataLoader(torch_data,batch_size = batch_size)
145/168:
count =1
for i,j in train_loader:
    print(i.cpu().size())
    print(j.cpu().size())
    count+=1
print(count)
145/169:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
# Q) 왜 learning_rate를 조정하면 MSE값의 차이가 크게나지
145/170:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
145/171: torch.FloatTensor([X[-sequence_length:].tolist()]).unsqueeze(2)
145/172:
out_list = []
Y = data[['volumn']].values[-2*split:-split]



with torch.no_grad():
    model.eval()
    #if num==0:
    new_train=[X[-sequence_length:].tolist()]
#     else:
#         new_train = new_train[1:]+out.cpu().tolist()
        #print(type(new_train))
    new_tr = torch.FloatTensor(new_train).unsqueeze(2).to(device)
    out = model(new_tr)
#    out_list.append(out)
#   loss_graph.append(running_loss/n)
print(out)
145/173:
plot_data = np.array(out.cpu().tolist()).reshape(1,-1)[0]
plot_data
145/174:
plot_data = np.array(out.cpu().tolist()).reshape(1,-1)[0]
plt.plot(np.arange(len(plot_data)),plot_data,label = 'predcition')
plt.plot(np.arange(len(plot_data)),X[-split:],label = 'real')
plt.legend()
145/175:
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    """
    Decomposition-Linear
    """
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 5 #커널사이즈를 5로 변경
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        

    def forward(self, x):
        # x: [Batch, Input length, Channel]
        seasonal_init, trend_init = self.decomposition(x)# res = seasonality 를 moving_mean은 trend를 의미
        # seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
        seasonal_init, trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1)
        if self.individual:
            seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],dtype=seasonal_init.dtype).to(seasonal_init.device)
             # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
            trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len],dtype=trend_init.dtype).to(trend_init.device)
            for i in range(self.channels):
                seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
                # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                # 이때 RNN과 달리 한번에 학습을 진행하기 때문에 seq의 배열 자체가 feature가 됨
        else:
            seasonal_output = self.Linear_Seasonal(seasonal_init)
            trend_output = self.Linear_Trend(trend_init)

        x = seasonal_output + trend_output
        return x.permute(0,2,1) # [batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)]로 변경해서 return
145/176: torch.ones(2,1,3,dtype=float) # batch는 2 length는 1 input size=3
145/177:
import torch
import torch.nn as nn
import torch.nn.functional as F
# 이 모듈에는 torch.nn 라이브러리의 모든 함수가 포함되어 있다 다양한 손실 및
# 활성화 함수 뿐만 아니라, 풀링(pooling) 함수와 같이 신경망을 만드는데 편리한 몇 가지
# 함수도 여기에서 찾을 수 있다.
import numpy as np
import pandas as pd
145/178:
data = pd.read_csv('../traffic/data/5.csv')
data
145/179: data.drop(columns=['service_name','packets','unknown'])
145/180:
scaler = MinMaxScaler()
data[['volumn']] = scaler.fit_transform(data[['volumn']])
data
145/181:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,1).to(device)
145/182:
split = 180
sequence_length = 1440
pred_len = 180
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
145/183:
split = 120
sequence_length = 1440
pred_len = 120
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
147/1:
import torch
import torch.nn as nn
import torch.nn.functional as F
# 이 모듈에는 torch.nn 라이브러리의 모든 함수가 포함되어 있다 다양한 손실 및
# 활성화 함수 뿐만 아니라, 풀링(pooling) 함수와 같이 신경망을 만드는데 편리한 몇 가지
# 함수도 여기에서 찾을 수 있다.
import numpy as np
import torch.optim as optim
import matplotlib.pyplot as plt
from torch.utils.data import TensorDataset,DataLoader
from torch import FloatTensor
from sklearn.preprocessing import MinMaxScaler
147/2:
# Pytorch로 설게하는 신경망은 반드시 따라야 하는 구조가 있다

'''
1. torch.nn.Module을 상속해야 한다
2. __init()__과 forward()를 override 해야한다.
    (1) override: torch.nn.Module(부모클래스)에서 정의한 메소드를 override 해야한다
    (2) __init()__ 에서는 모델에서 사용될 module(nn.Linear, nn.Conv2d), activation function
    (nn.functional.relu, nn.functional.sigmoid)등을 정의한다
    (3) forward()에서는 모델에서 실행되어야하는 계산을 정의한다. backward 계산은 backward()를
    이용시에 Pytorch가 알아서 해주니깐 forward()만을 정의해주면 된다
    input을 넣어서 어떤 계산을 진행해서 output이 나올지를 정의해준다고 이해하면 된다
'''
class moving_avg(nn.Module):
    "M.A.는 시계열의 trend에 집중하는 것을 막는다"
    def __init__(self,kernel_size,stride):
        super(moving_avg,self).__init__()
        # super(Model_Name,self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size = kernel_size, stride = stride, padding =0)
        # pooling 계산을 하는 1차원 평균으로 pooling계산
    
    def forward(self,x):
        " time series의 양 쪽 끝을 padding 한다"
        print(x[:,0:1])
        front = x[:,0:1,:].repeat(1,(self.kernel_size-1)//2,1)
        # Repeats this tensor along the specified dimensions.
        #print("front",front)
        
        end = x[:,-1,:].repeat(1,(self.kernel_size-1)//2,1)
        #print("end",end)
        
# front와 end를 추가해서 moving average를 계산해서 공백이 없도록
        
        x = torch.cat([front, x, end], dim=1)
        #print("cat",x,x.size())
        
        #print('before avg',x.permute(0,2,1))
        x = self.avg(x.permute(0,2,1)) #pooling 계산
        #print(x)
        
        x = x.permute(0,2,1) # 원상복귀
        #print(x)
        return x
147/3:
x=torch.FloatTensor([2,1,3,4]).view(1,-1,1) #4개의 데이터를 가진 것을 표현
print(x)
print('-----------------')
ma = moving_avg(3,1) 
x = ma.forward(x)
print(x)
147/4:
class series_decomp(nn.Module):
    "statsmodels.tsa의 seasonal_decompose와 역할이 똑같다"
    def __init__(self,kernel_size):
        super(series_decomp,self).__init__()
        self.moving_avg = moving_avg(kernel_size,stride = 1)
    
    def forward(self,x):
        moving_mean = self.moving_avg(x) #output은 (batch,sequence_length,input_size)
        res = x - moving_mean 
        # Classical ma중에서 additive model
        print(res.permute(1,2,0).size())
        print('----')
        print(moving_mean.permute(1,2,0).size())
        return res.permute(1,2,0),moving_mean.permute(1,2,0)
147/5:
x = torch.FloatTensor([1,2,3,4]).view(1,-1,1)
decomp = series_decomp(3) 
# kernel_size를 짝수로하면 오류 발생 (size가 맞지 않은 오류)
# 이경우는 짝수 ma를 두번진행하면 된다(아래 예시)
a,b=decomp.forward(x)
147/6:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = configs.seq_len # configure의 sequence length
        self.pred_len = configs.pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs.individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = configs.enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
147/7:
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    """
    Decomposition-Linear
    """
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 5 #커널사이즈를 5로 변경
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        

    def forward(self, x):
        # x: [Batch, Input length, Channel]
        seasonal_init, trend_init = self.decomposition(x)# res = seasonality 를 moving_mean은 trend를 의미
        # seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
        seasonal_init, trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1)
        if self.individual:
            seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],dtype=seasonal_init.dtype).to(seasonal_init.device)
             # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
            trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len],dtype=trend_init.dtype).to(trend_init.device)
            for i in range(self.channels):
                seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
                # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                # 이때 RNN과 달리 한번에 학습을 진행하기 때문에 seq의 배열 자체가 feature가 됨
        else:
            seasonal_output = self.Linear_Seasonal(seasonal_init)
            trend_output = self.Linear_Trend(trend_init)

        x = seasonal_output + trend_output
        return x.permute(0,2,1) # [batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)]로 변경해서 return
147/8: torch.ones(2,1,3,dtype=float) # batch는 2 length는 1 input size=3
147/9:
import torch
import torch.nn as nn
import torch.nn.functional as F
# 이 모듈에는 torch.nn 라이브러리의 모든 함수가 포함되어 있다 다양한 손실 및
# 활성화 함수 뿐만 아니라, 풀링(pooling) 함수와 같이 신경망을 만드는데 편리한 몇 가지
# 함수도 여기에서 찾을 수 있다.
import numpy as np
import pandas as pd
147/10:
data = pd.read_csv('../traffic/data/5.csv')
data
147/11: data.drop(columns=['service_name','packets','unknown'])
147/12:
scaler = MinMaxScaler()
data[['volumn']] = scaler.fit_transform(data[['volumn']])
data
147/13:
def seq_data(x,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x.iloc[i+sequence_length])
    else:
        print('error')
    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,1).to(device)
147/14:
split = 120
sequence_length = 1440
pred_len = 120
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
147/15:
device = torch.device("cuda:0")
torch_data = TensorDataset(x_seq,target)
147/16:
batch_size = 32
train_loader = DataLoader(torch_data,batch_size = batch_size)
147/17:
count =1
for i,j in train_loader:
    print(i.cpu().size())
    print(j.cpu().size())
    count+=1
print(count)
147/18:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
# Q) 왜 learning_rate를 조정하면 MSE값의 차이가 크게나지
147/19:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
147/20: torch.FloatTensor([X[-sequence_length:].tolist()]).unsqueeze(2)
147/21:
out_list = []
Y = data[['volumn']].values[-2*split:-split]



with torch.no_grad():
    model.eval()
    #if num==0:
    new_train=[X[-sequence_length:].tolist()]
#     else:
#         new_train = new_train[1:]+out.cpu().tolist()
        #print(type(new_train))
    new_tr = torch.FloatTensor(new_train).unsqueeze(2).to(device)
    out = model(new_tr)
#    out_list.append(out)
#   loss_graph.append(running_loss/n)
print(out)
147/22:
plot_data = np.array(out.cpu().tolist()).reshape(1,-1)[0]
plot_data
147/23:
plot_data = np.array(out.cpu().tolist()).reshape(1,-1)[0]
plt.plot(np.arange(len(plot_data)),plot_data,label = 'predcition')
plt.plot(np.arange(len(plot_data)),X[-split:],label = 'real')
plt.legend()
147/24:
plot_data = np.array(out.cpu().tolist()).reshape(1,-1)[0]
plot_data
147/25:
plot_data = np.array(out.cpu().tolist()).reshape(1,-1)[0]
plt.plot(np.arange(len(plot_data)),plot_data,label = 'predcition')
plt.plot(np.arange(len(plot_data)),X[-split:],label = 'real')
plt.legend()
147/26:
batch_size = 32
train_loader = DataLoader(torch_data,batch_size = batch_size,shuffle = True)
147/27:
count =1
for i,j in train_loader:
    print(i.cpu().size())
    print(j.cpu().size())
    count+=1
print(count)
147/28:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
# Q) 왜 learning_rate를 조정하면 MSE값의 차이가 크게나지
147/29:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
147/30: torch.FloatTensor([X[-sequence_length:].tolist()]).unsqueeze(2)
147/31:
out_list = []
Y = data[['volumn']].values[-2*split:-split]



with torch.no_grad():
    model.eval()
    #if num==0:
    new_train=[X[-sequence_length:].tolist()]
#     else:
#         new_train = new_train[1:]+out.cpu().tolist()
        #print(type(new_train))
    new_tr = torch.FloatTensor(new_train).unsqueeze(2).to(device)
    out = model(new_tr)
#    out_list.append(out)
#   loss_graph.append(running_loss/n)
print(out)
147/32:
plot_data = np.array(out.cpu().tolist()).reshape(1,-1)[0]
plot_data
147/33:
plot_data = np.array(out.cpu().tolist()).reshape(1,-1)[0]
plt.plot(np.arange(len(plot_data)),plot_data,label = 'predcition')
plt.plot(np.arange(len(plot_data)),X[-split:],label = 'real')
plt.legend()
147/34:
split = 360
sequence_length = 1440
pred_len = 360
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
147/35:
device = torch.device("cuda:0")
torch_data = TensorDataset(x_seq,target)
147/36:
batch_size = 32
train_loader = DataLoader(torch_data,batch_size = batch_size,shuffle = True)
147/37:
count =1
for i,j in train_loader:
    print(i.cpu().size())
    print(j.cpu().size())
    count+=1
print(count)
147/38:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
# Q) 왜 learning_rate를 조정하면 MSE값의 차이가 크게나지
147/39:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
147/40: torch.FloatTensor([X[-sequence_length:].tolist()]).unsqueeze(2)
147/41:
out_list = []
Y = data[['volumn']].values[-2*split:-split]



with torch.no_grad():
    model.eval()
    #if num==0:
    new_train=[X[-sequence_length:].tolist()]
#     else:
#         new_train = new_train[1:]+out.cpu().tolist()
        #print(type(new_train))
    new_tr = torch.FloatTensor(new_train).unsqueeze(2).to(device)
    out = model(new_tr)
#    out_list.append(out)
#   loss_graph.append(running_loss/n)
print(out)
147/42:
plot_data = np.array(out.cpu().tolist()).reshape(1,-1)[0]
plot_data
147/43:
plot_data = np.array(out.cpu().tolist()).reshape(1,-1)[0]
plt.plot(np.arange(len(plot_data)),plot_data,label = 'predcition')
plt.plot(np.arange(len(plot_data)),X[-split:],label = 'real')
plt.legend()
147/44:
def seq_data(x,sequence_length,pred_len):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len])
    else:
        print('error')
    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,1).to(device)
147/45:
split = 360
sequence_length = 1440
pred_len = 360
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length,pred_len)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
147/46:
def seq_data(x,sequence_length,pred_len):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
    else:
        print('error')
    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,1).to(device)
147/47:
split = 360
sequence_length = 1440
pred_len = 360
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length,pred_len)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
147/48:
def seq_data(x,sequence_length,pred_len):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            print(len(x.iloc[i:i+sequence_length].values))
            seq_list.append(x.iloc[i:i+sequence_length].values)
            print(len(x[i+sequence_length:i+sequence_length+pred_len+1].values))
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len+1].values)
    else:
        print('error')
    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,1).to(device)
147/49:
split = 360
sequence_length = 1440
pred_len = 360
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length,pred_len)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
147/50:
def seq_data(x,sequence_length,pred_len):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
    else:
        print('error')
    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,1).to(device)
147/51:
split = 360
sequence_length = 1440
pred_len = 360
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length,pred_len)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
147/52:
def seq_data(x,sequence_length,pred_len):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
            if i==1:
                print(target_list)
    else:
        print('error')
    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,1).to(device)
147/53:
split = 360
sequence_length = 1440
pred_len = 360
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length,pred_len)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
147/54:
def seq_data(x,sequence_length,pred_len):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
            if i==1:
                print(target_list[0].size())
    else:
        print('error')
    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,1).to(device)
147/55:
split = 360
sequence_length = 1440
pred_len = 360
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length,pred_len)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
147/56:
device = torch.device("cuda:0")
torch_data = TensorDataset(x_seq,target)
147/57:
def seq_data(x,sequence_length,pred_len):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
            if i==1:
                print(target_list.size())
    else:
        print('error')
    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,1).to(device)
147/58:
split = 360
sequence_length = 1440
pred_len = 360
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length,pred_len)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
147/59:
def seq_data(x,sequence_length,pred_len):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
            if i==1:
                print(target_list)
    else:
        print('error')
    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,1).to(device)
147/60:
split = 360
sequence_length = 1440
pred_len = 360
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length,pred_len)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
147/61:
def seq_data(x,sequence_length,pred_len):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
            if i==1:
                print(target_list[0].shape())
    else:
        print('error')
    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,1).to(device)
147/62:
def seq_data(x,sequence_length,pred_len):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
            if i==1:
                print(target_list[0].shape
    else:
        print('error')
    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,1).to(device)
147/63:
def seq_data(x,sequence_length,pred_len):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
            if i==1:
                print(target_list[0].shape)
    else:
        print('error')
    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,1).to(device)
147/64:
split = 360
sequence_length = 1440
pred_len = 360
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length,pred_len)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
147/65: FloatTensor(target_list).unsqueeze(1).view(-1,1,1).to(device)
147/66:
def seq_data(x,sequence_length,pred_len):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
            if i==1:
                print(target_list[0].shape)
    else:
        print('error')
    print(FloatTensor(target_list).unsqueeze(0).view(-1,1,1).to(device))
    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(0).view(-1,1,1).to(device)
147/67:
split = 360
sequence_length = 1440
pred_len = 360
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length,pred_len)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
147/68:
def seq_data(x,sequence_length,pred_len):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
    else:
        print('error')
    print(len(target_list))
    print(len(seq_list))
    
    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,1).to(device)
147/69:
split = 360
sequence_length = 1440
pred_len = 360
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(X,sequence_length,pred_len)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
147/70:
def seq_data(x,sequence_length,pred_len):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
            if i==0 :
                print(x[i+sequence_length:i+sequence_length+pred_len])
    else:
        print('error')
    print(len(target_list))
    print(len(seq_list))
    
    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,1).to(device)
147/71:
split = 360
sequence_length = 1440
pred_len = 360
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(x=X,sequence_length=sequence_length,pred_len=pred_len)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
147/72:
def seq_data(x,sequence_length,pred_len):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
                print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')
    print(len(target_list))
    print(len(seq_list))
    
    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,1).to(device)
147/73:
def seq_data(x,sequence_length,pred_len):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
            print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')
    print(len(target_list))
    print(len(seq_list))
    
    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,1).to(device)
147/74:
split = 360
sequence_length = 1440
pred_len = 360
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(x=X,sequence_length=sequence_length,pred_len=pred_len)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
147/75:
split = 240
sequence_length = 480
pred_len = 240
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(x=X,sequence_length=sequence_length,pred_len=pred_len)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
147/76:
def seq_data(x,sequence_length,pred_len):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)            
    else:
        print('error')

    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,1).to(device)
147/77:
split = 240
sequence_length = 480
pred_len = 240
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(x=X,sequence_length=sequence_length,pred_len=pred_len)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
147/78:
def seq_data(x,sequence_length,pred_len):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
            print(target_list)
    else:
        print('error')

    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,1).to(device)
147/79:
split = 240
sequence_length = 480
pred_len = 240
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(x=X,sequence_length=sequence_length,pred_len=pred_len)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
147/80:
def seq_data(x,sequence_length,pred_len):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
            print(target_list.shape)
    else:
        print('error')

    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,1).to(device)
147/81:
split = 240
sequence_length = 480
pred_len = 240
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(x=X,sequence_length=sequence_length,pred_len=pred_len)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
147/82:
def seq_data(x,sequence_length,pred_len):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
            print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,1).to(device)
147/83:
split = 240
sequence_length = 480
pred_len = 240
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(x=X,sequence_length=sequence_length,pred_len=pred_len)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
147/84:
split = 720
sequence_length = 1440
pred_len = 360
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(x=X,sequence_length=sequence_length,pred_len=pred_len)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
147/85:
def seq_data(x,sequence_length,pred_len):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
            print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,1).to(device)
147/86:
def seq_data(x,sequence_length,pred_len):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,1).to(device)
147/87:
split = 720
sequence_length = 1440
pred_len = 360
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
x_seq, target = seq_data(x=X,sequence_length=sequence_length,pred_len=pred_len)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
147/88:
split = 720
sequence_length = 1440
pred_len = 360
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split+pred_len]
x_seq, target = seq_data(x=X,sequence_length=sequence_length,pred_len=pred_len)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
147/89:
def seq_data(x,y,sequence_length,pred_len):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,1).to(device)
147/90:
split = 360
sequence_length = 1440
pred_len = 360
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
Y = data['volumn']
x_seq, target = seq_data(x=X,y=Y,sequence_length=sequence_length,pred_len=pred_len)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
147/91:
device = torch.device("cuda:0")
torch_data = TensorDataset(x_seq,target)
147/92:
def seq_data(x,y,sequence_length,pred_len):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,360,1).to(device)
147/93:
split = 360
sequence_length = 1440
pred_len = 360
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
Y = data['volumn']
x_seq, target = seq_data(x=X,y=Y,sequence_length=sequence_length,pred_len=pred_len)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
148/1:
import torch
import torch.nn as nn
import torch.nn.functional as F
# 이 모듈에는 torch.nn 라이브러리의 모든 함수가 포함되어 있다 다양한 손실 및
# 활성화 함수 뿐만 아니라, 풀링(pooling) 함수와 같이 신경망을 만드는데 편리한 몇 가지
# 함수도 여기에서 찾을 수 있다.
import numpy as np
import torch.optim as optim
import matplotlib.pyplot as plt
from torch.utils.data import TensorDataset,DataLoader
from torch import FloatTensor
from sklearn.preprocessing import MinMaxScaler
148/2:
# Pytorch로 설게하는 신경망은 반드시 따라야 하는 구조가 있다

'''
1. torch.nn.Module을 상속해야 한다
2. __init()__과 forward()를 override 해야한다.
    (1) override: torch.nn.Module(부모클래스)에서 정의한 메소드를 override 해야한다
    (2) __init()__ 에서는 모델에서 사용될 module(nn.Linear, nn.Conv2d), activation function
    (nn.functional.relu, nn.functional.sigmoid)등을 정의한다
    (3) forward()에서는 모델에서 실행되어야하는 계산을 정의한다. backward 계산은 backward()를
    이용시에 Pytorch가 알아서 해주니깐 forward()만을 정의해주면 된다
    input을 넣어서 어떤 계산을 진행해서 output이 나올지를 정의해준다고 이해하면 된다
'''
class moving_avg(nn.Module):
    "M.A.는 시계열의 trend에 집중하는 것을 막는다"
    def __init__(self,kernel_size,stride):
        super(moving_avg,self).__init__()
        # super(Model_Name,self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size = kernel_size, stride = stride, padding =0)
        # pooling 계산을 하는 1차원 평균으로 pooling계산
    
    def forward(self,x):
        " time series의 양 쪽 끝을 padding 한다"
        print(x[:,0:1])
        front = x[:,0:1,:].repeat(1,(self.kernel_size-1)//2,1)
        # Repeats this tensor along the specified dimensions.
        #print("front",front)
        
        end = x[:,-1,:].repeat(1,(self.kernel_size-1)//2,1)
        #print("end",end)
        
# front와 end를 추가해서 moving average를 계산해서 공백이 없도록
        
        x = torch.cat([front, x, end], dim=1)
        #print("cat",x,x.size())
        
        #print('before avg',x.permute(0,2,1))
        x = self.avg(x.permute(0,2,1)) #pooling 계산
        #print(x)
        
        x = x.permute(0,2,1) # 원상복귀
        #print(x)
        return x
148/3:
x=torch.FloatTensor([2,1,3,4]).view(1,-1,1) #4개의 데이터를 가진 것을 표현
print(x)
print('-----------------')
ma = moving_avg(3,1) 
x = ma.forward(x)
print(x)
148/4:
class series_decomp(nn.Module):
    "statsmodels.tsa의 seasonal_decompose와 역할이 똑같다"
    def __init__(self,kernel_size):
        super(series_decomp,self).__init__()
        self.moving_avg = moving_avg(kernel_size,stride = 1)
    
    def forward(self,x):
        moving_mean = self.moving_avg(x) #output은 (batch,sequence_length,input_size)
        res = x - moving_mean 
        # Classical ma중에서 additive model
        print(res.permute(1,2,0).size())
        print('----')
        print(moving_mean.permute(1,2,0).size())
        return res.permute(1,2,0),moving_mean.permute(1,2,0)
148/5:
x = torch.FloatTensor([1,2,3,4]).view(1,-1,1)
decomp = series_decomp(3) 
# kernel_size를 짝수로하면 오류 발생 (size가 맞지 않은 오류)
# 이경우는 짝수 ma를 두번진행하면 된다(아래 예시)
a,b=decomp.forward(x)
148/6:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = configs.seq_len # configure의 sequence length
        self.pred_len = configs.pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs.individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = configs.enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
148/7:
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    """
    Decomposition-Linear
    """
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 5 #커널사이즈를 5로 변경
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        

    def forward(self, x):
        # x: [Batch, Input length, Channel]
        seasonal_init, trend_init = self.decomposition(x)# res = seasonality 를 moving_mean은 trend를 의미
        # seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
        seasonal_init, trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1)
        if self.individual:
            seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],dtype=seasonal_init.dtype).to(seasonal_init.device)
             # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
            trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len],dtype=trend_init.dtype).to(trend_init.device)
            for i in range(self.channels):
                seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
                # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                # 이때 RNN과 달리 한번에 학습을 진행하기 때문에 seq의 배열 자체가 feature가 됨
        else:
            seasonal_output = self.Linear_Seasonal(seasonal_init)
            trend_output = self.Linear_Trend(trend_init)

        x = seasonal_output + trend_output
        return x.permute(0,2,1) # [batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)]로 변경해서 return
148/8: torch.ones(2,1,3,dtype=float) # batch는 2 length는 1 input size=3
148/9:
import torch
import torch.nn as nn
import torch.nn.functional as F
# 이 모듈에는 torch.nn 라이브러리의 모든 함수가 포함되어 있다 다양한 손실 및
# 활성화 함수 뿐만 아니라, 풀링(pooling) 함수와 같이 신경망을 만드는데 편리한 몇 가지
# 함수도 여기에서 찾을 수 있다.
import numpy as np
import pandas as pd
148/10:
data = pd.read_csv('../traffic/data/5.csv')
data
148/11: data.drop(columns=['service_name','packets','unknown'])
148/12:
scaler = MinMaxScaler()
data[['volumn']] = scaler.fit_transform(data[['volumn']])
data
148/13: FloatTensor(target_list).unsqueeze(1).view(-1,1,1).to(device)
148/14:
def seq_data(x,y,sequence_length,pred_len):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,360,1).to(device)
148/15:
split = 360
sequence_length = 1440
pred_len = 360
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
Y = data['volumn']
x_seq, target = seq_data(x=X,y=Y,sequence_length=sequence_length,pred_len=pred_len)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
148/16:
def seq_data(x,y,sequence_length,pred_len):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,360).to(device)
148/17:
def seq_data(x,y,sequence_length,pred_len):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,360).to(device)
148/18:
split = 360
sequence_length = 1440
pred_len = 360
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
Y = data['volumn']
x_seq, target = seq_data(x=X,y=Y,sequence_length=sequence_length,pred_len=pred_len)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
148/19:
device = torch.device("cuda:0")
torch_data = TensorDataset(x_seq,target)
148/20:
batch_size = 32
train_loader = DataLoader(torch_data,batch_size = batch_size,shuffle = True)
148/21:
count =1
for i,j in train_loader:
    print(i.cpu().size())
    print(j.cpu().size())
    count+=1
print(count)
148/22:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
# Q) 왜 learning_rate를 조정하면 MSE값의 차이가 크게나지
148/23:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
148/24: torch.FloatTensor([X[-sequence_length:].tolist()]).unsqueeze(2)
148/25:
out_list = []
Y = data[['volumn']].values[-2*split:-split]



with torch.no_grad():
    model.eval()
    #if num==0:
    new_train=[X[-sequence_length:].tolist()]
#     else:
#         new_train = new_train[1:]+out.cpu().tolist()
        #print(type(new_train))
    new_tr = torch.FloatTensor(new_train).unsqueeze(2).to(device)
    out = model(new_tr)
#    out_list.append(out)
#   loss_graph.append(running_loss/n)
print(out)
148/26:
plot_data = np.array(out.cpu().tolist()).reshape(1,-1)[0]
plot_data
148/27:
plot_data = np.array(out.cpu().tolist()).reshape(1,-1)[0]
plt.plot(np.arange(len(plot_data)),plot_data,label = 'predcition')
plt.plot(np.arange(len(plot_data)),X[-split:],label = 'real')
plt.legend()
148/28:
device = torch.device("cuda:0")
torch_data = TensorDataset(x_seq,target)
148/29:
batch_size = 32
train_loader = DataLoader(torch_data,batch_size = batch_size)
148/30:
count =1
for i,j in train_loader:
    print(i.cpu().size())
    print(j.cpu().size())
    count+=1
print(count)
148/31:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
# Q) 왜 learning_rate를 조정하면 MSE값의 차이가 크게나지
148/32:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
148/33:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
# Q) 왜 learning_rate를 조정하면 MSE값의 차이가 크게나지
148/34:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
148/35:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
# Q) 왜 learning_rate를 조정하면 MSE값의 차이가 크게나지
148/36:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
148/37:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
# Q) 왜 learning_rate를 조정하면 MSE값의 차이가 크게나지
148/38:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
148/39: torch.FloatTensor([X[-sequence_length:].tolist()]).unsqueeze(2)
148/40:
out_list = []
Y = data[['volumn']].values[-2*split:-split]



with torch.no_grad():
    model.eval()
    #if num==0:
    new_train=[X[-sequence_length:].tolist()]
#     else:
#         new_train = new_train[1:]+out.cpu().tolist()
        #print(type(new_train))
    new_tr = torch.FloatTensor(new_train).unsqueeze(2).to(device)
    out = model(new_tr)
#    out_list.append(out)
#   loss_graph.append(running_loss/n)
print(out)
148/41:
plot_data = np.array(out.cpu().tolist()).reshape(1,-1)[0]
plot_data
148/42:
plot_data = np.array(out.cpu().tolist()).reshape(1,-1)[0]
plt.plot(np.arange(len(plot_data)),plot_data,label = 'predcition')
plt.plot(np.arange(len(plot_data)),X[-split:],label = 'real')
plt.legend()
148/43:
def seq_data(x,y,sequence_length,pred_len):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length:i+sequence_length+pred_len].values)
            if i == 0:
                print(x.iloc[i:i+sequence_length].values)
                print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,360).to(device)
148/44:
split = 360
sequence_length = 1440
pred_len = 360
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
Y = data['volumn']
x_seq, target = seq_data(x=X,y=Y,sequence_length=sequence_length,pred_len=pred_len)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
148/45: print(X[1440])
148/46: print(X[1439])
148/47:
split = 360
sequence_length = 1440
pred_len = 360
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
Y = data['volumn']
x_seq, target = seq_data(x=X,y=Y,sequence_length=sequence_length,pred_len=pred_len)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
149/1:
import torch
import torch.nn as nn
import torch.nn.functional as F
# 이 모듈에는 torch.nn 라이브러리의 모든 함수가 포함되어 있다 다양한 손실 및
# 활성화 함수 뿐만 아니라, 풀링(pooling) 함수와 같이 신경망을 만드는데 편리한 몇 가지
# 함수도 여기에서 찾을 수 있다.
import numpy as np
import torch.optim as optim
import matplotlib.pyplot as plt
from torch.utils.data import TensorDataset,DataLoader
from torch import FloatTensor
from sklearn.preprocessing import MinMaxScaler
149/2:
# Pytorch로 설게하는 신경망은 반드시 따라야 하는 구조가 있다

'''
1. torch.nn.Module을 상속해야 한다
2. __init()__과 forward()를 override 해야한다.
    (1) override: torch.nn.Module(부모클래스)에서 정의한 메소드를 override 해야한다
    (2) __init()__ 에서는 모델에서 사용될 module(nn.Linear, nn.Conv2d), activation function
    (nn.functional.relu, nn.functional.sigmoid)등을 정의한다
    (3) forward()에서는 모델에서 실행되어야하는 계산을 정의한다. backward 계산은 backward()를
    이용시에 Pytorch가 알아서 해주니깐 forward()만을 정의해주면 된다
    input을 넣어서 어떤 계산을 진행해서 output이 나올지를 정의해준다고 이해하면 된다
'''
class moving_avg(nn.Module):
    "M.A.는 시계열의 trend에 집중하는 것을 막는다"
    def __init__(self,kernel_size,stride):
        super(moving_avg,self).__init__()
        # super(Model_Name,self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size = kernel_size, stride = stride, padding =0)
        # pooling 계산을 하는 1차원 평균으로 pooling계산
    
    def forward(self,x):
        " time series의 양 쪽 끝을 padding 한다"
        print(x[:,0:1])
        front = x[:,0:1,:].repeat(1,(self.kernel_size-1)//2,1)
        # Repeats this tensor along the specified dimensions.
        #print("front",front)
        
        end = x[:,-1,:].repeat(1,(self.kernel_size-1)//2,1)
        #print("end",end)
        
# front와 end를 추가해서 moving average를 계산해서 공백이 없도록
        
        x = torch.cat([front, x, end], dim=1)
        #print("cat",x,x.size())
        
        #print('before avg',x.permute(0,2,1))
        x = self.avg(x.permute(0,2,1)) #pooling 계산
        #print(x)
        
        x = x.permute(0,2,1) # 원상복귀
        #print(x)
        return x
149/3:
x=torch.FloatTensor([2,1,3,4]).view(1,-1,1) #4개의 데이터를 가진 것을 표현
print(x)
print('-----------------')
ma = moving_avg(3,1) 
x = ma.forward(x)
print(x)
149/4:
class series_decomp(nn.Module):
    "statsmodels.tsa의 seasonal_decompose와 역할이 똑같다"
    def __init__(self,kernel_size):
        super(series_decomp,self).__init__()
        self.moving_avg = moving_avg(kernel_size,stride = 1)
    
    def forward(self,x):
        moving_mean = self.moving_avg(x) #output은 (batch,sequence_length,input_size)
        res = x - moving_mean 
        # Classical ma중에서 additive model
        print(res.permute(1,2,0).size())
        print('----')
        print(moving_mean.permute(1,2,0).size())
        return res.permute(1,2,0),moving_mean.permute(1,2,0)
149/5:
x = torch.FloatTensor([1,2,3,4]).view(1,-1,1)
decomp = series_decomp(3) 
# kernel_size를 짝수로하면 오류 발생 (size가 맞지 않은 오류)
# 이경우는 짝수 ma를 두번진행하면 된다(아래 예시)
a,b=decomp.forward(x)
149/6:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = configs.seq_len # configure의 sequence length
        self.pred_len = configs.pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs.individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = configs.enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
149/7:
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    """
    Decomposition-Linear
    """
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 5 #커널사이즈를 5로 변경
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        

    def forward(self, x):
        # x: [Batch, Input length, Channel]
        seasonal_init, trend_init = self.decomposition(x)# res = seasonality 를 moving_mean은 trend를 의미
        # seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
        seasonal_init, trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1)
        if self.individual:
            seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],dtype=seasonal_init.dtype).to(seasonal_init.device)
             # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
            trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len],dtype=trend_init.dtype).to(trend_init.device)
            for i in range(self.channels):
                seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
                # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                # 이때 RNN과 달리 한번에 학습을 진행하기 때문에 seq의 배열 자체가 feature가 됨
        else:
            seasonal_output = self.Linear_Seasonal(seasonal_init)
            trend_output = self.Linear_Trend(trend_init)

        x = seasonal_output + trend_output
        return x.permute(0,2,1) # [batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)]로 변경해서 return
149/8: torch.ones(2,1,3,dtype=float) # batch는 2 length는 1 input size=3
149/9:
import torch
import torch.nn as nn
import torch.nn.functional as F
# 이 모듈에는 torch.nn 라이브러리의 모든 함수가 포함되어 있다 다양한 손실 및
# 활성화 함수 뿐만 아니라, 풀링(pooling) 함수와 같이 신경망을 만드는데 편리한 몇 가지
# 함수도 여기에서 찾을 수 있다.
import numpy as np
import pandas as pd
149/10:
data = pd.read_csv('../traffic/data/5.csv')
data
149/11: data.drop(columns=['service_name','packets','unknown'])
149/12:
scaler = MinMaxScaler()
data[['volumn']] = scaler.fit_transform(data[['volumn']])
data
149/13:
def seq_data(x,y,sequence_length,pred_len):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length:i+sequence_length+pred_len].values)
#             if i == 0:
#                 print(x.iloc[i:i+sequence_length].values)
#                 print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,360).to(device)
149/14: print(X[1439])
149/15:
split = 360
sequence_length = 1440
pred_len = 360
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
Y = data['volumn']
x_seq, target = seq_data(x=X,y=Y,sequence_length=sequence_length,pred_len=pred_len)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
149/16:
split = 720 # split까지 학습을 하게 되면 split-pred_len까지의 정답은 알려준셈이 된다
sequence_length = 1440
pred_len = 360
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
Y = data['volumn']
x_seq, target = seq_data(x=X,y=Y,sequence_length=sequence_length,pred_len=pred_len)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
149/17:
device = torch.device("cuda:0")
torch_data = TensorDataset(x_seq,target)
149/18:
batch_size = 32
train_loader = DataLoader(torch_data,batch_size = batch_size)
149/19:
count =1
for i,j in train_loader:
    print(i.cpu().size())
    print(j.cpu().size())
    count+=1
print(count)
149/20:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
# Q) 왜 learning_rate를 조정하면 MSE값의 차이가 크게나지
149/21:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
149/22: torch.FloatTensor([X[-sequence_length:].tolist()]).unsqueeze(2)
149/23:
out_list = []
Y = data[['volumn']].values[-2*split:-split]



with torch.no_grad():
    model.eval()
    #if num==0:
    new_train=[X[-sequence_length:].tolist()]
#     else:
#         new_train = new_train[1:]+out.cpu().tolist()
        #print(type(new_train))
    new_tr = torch.FloatTensor(new_train).unsqueeze(2).to(device)
    out = model(new_tr)
#    out_list.append(out)
#   loss_graph.append(running_loss/n)
print(out)
149/24:
plot_data = np.array(out.cpu().tolist()).reshape(1,-1)[0]
plot_data
149/25:
plot_data = np.array(out.cpu().tolist()).reshape(1,-1)[0]
plt.plot(np.arange(len(plot_data)),plot_data,label = 'predcition')
plt.plot(np.arange(len(plot_data)),X[-split:],label = 'real')
plt.legend()
149/26:
plot_data = np.array(out.cpu().tolist()).reshape(1,-1)[0]
plt.plot(np.arange(len(plot_data)),plot_data,label = 'predcition')
plt.plot(np.arange(len(plot_data)),Y[-360:],label = 'real')
plt.legend()
149/27:
out_list = []
Y = data[['volumn']].values[-2*split:-split]



with torch.no_grad():
    model.eval()
    #if num==0:
    new_train=[Y[-(sequence_length+pred_len):].tolist()]
#     else:
#         new_train = new_train[1:]+out.cpu().tolist()
        #print(type(new_train))
    new_tr = torch.FloatTensor(new_train).unsqueeze(2).to(device)
    out = model(new_tr)
#    out_list.append(out)
#   loss_graph.append(running_loss/n)
print(out)
149/28: [Y[-(sequence_length+pred_len):]
149/29:
out_list = []
Y = data[['volumn']].values[-2*split:-split]



with torch.no_grad():
    model.eval()
    #if num==0:
    new_train=[Y[-(sequence_length+pred_len):-(pred_len)].tolist()]
#     else:
#         new_train = new_train[1:]+out.cpu().tolist()
        #print(type(new_train))
    new_tr = torch.FloatTensor(new_train).unsqueeze(2).to(device)
    out = model(new_tr)
#    out_list.append(out)
#   loss_graph.append(running_loss/n)
print(out)
149/30:
out_list = []
Y = data[['volumn']].values[-2*split:-split]



with torch.no_grad():
    model.eval()
    #if num==0:
    new_train=Y[-(sequence_length+pred_len):-(pred_len)].tolist()
#     else:
#         new_train = new_train[1:]+out.cpu().tolist()
        #print(type(new_train))
    new_tr = torch.FloatTensor(new_train).unsqueeze(2).to(device)
    out = model(new_tr)
#    out_list.append(out)
#   loss_graph.append(running_loss/n)
print(out)
149/31: len(Y[-(sequence_length+pred_len):-(pred_len)].tolist())
149/32:
split = 720 # split까지 학습을 하게 되면 split-pred_len까지의 정답은 알려준셈이 된다
sequence_length = 1440
pred_len = 360
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
Y = data['volumn']
x_seq, target = seq_data(x=X,y=Y,sequence_length=sequence_length,pred_len=pred_len)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
149/33:
device = torch.device("cuda:0")
torch_data = TensorDataset(x_seq,target)
149/34:
batch_size = 32
train_loader = DataLoader(torch_data,batch_size = batch_size)
149/35:
count =1
for i,j in train_loader:
    print(i.cpu().size())
    print(j.cpu().size())
    count+=1
print(count)
149/36:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
# Q) 왜 learning_rate를 조정하면 MSE값의 차이가 크게나지
149/37:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
149/38: len(Y[-(sequence_length+pred_len):-(pred_len)].tolist())
149/39:
out_list = []


with torch.no_grad():
    model.eval()
    #if num==0:
    new_train=[Y[-(sequence_length+pred_len):-(pred_len)].tolist()]
#     else:
#         new_train = new_train[1:]+out.cpu().tolist()
        #print(type(new_train))
    new_tr = torch.FloatTensor(new_train).unsqueeze(2).to(device)
    out = model(new_tr)
#    out_list.append(out)
#   loss_graph.append(running_loss/n)
print(out)
149/40:
plot_data = np.array(out.cpu().tolist()).reshape(1,-1)[0]
plot_data
149/41:
plot_data = np.array(out.cpu().tolist()).reshape(1,-1)[0]
plt.plot(np.arange(len(plot_data)),plot_data,label = 'predcition')
plt.plot(np.arange(len(plot_data)),Y[-360:],label = 'real')
plt.legend()
149/42:
split = 1440 # split까지 학습을 하게 되면 split-pred_len까지의 정답은 알려준셈이 된다
sequence_length = 1800
pred_len = 720
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
Y = data['volumn']
x_seq, target = seq_data(x=X,y=Y,sequence_length=sequence_length,pred_len=pred_len)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
150/1:
import torch
import torch.nn as nn
import torch.nn.functional as F
# 이 모듈에는 torch.nn 라이브러리의 모든 함수가 포함되어 있다 다양한 손실 및
# 활성화 함수 뿐만 아니라, 풀링(pooling) 함수와 같이 신경망을 만드는데 편리한 몇 가지
# 함수도 여기에서 찾을 수 있다.
import numpy as np
import torch.optim as optim
import matplotlib.pyplot as plt
from torch.utils.data import TensorDataset,DataLoader
from torch import FloatTensor
from sklearn.preprocessing import MinMaxScaler
150/2:
# Pytorch로 설게하는 신경망은 반드시 따라야 하는 구조가 있다

'''
1. torch.nn.Module을 상속해야 한다
2. __init()__과 forward()를 override 해야한다.
    (1) override: torch.nn.Module(부모클래스)에서 정의한 메소드를 override 해야한다
    (2) __init()__ 에서는 모델에서 사용될 module(nn.Linear, nn.Conv2d), activation function
    (nn.functional.relu, nn.functional.sigmoid)등을 정의한다
    (3) forward()에서는 모델에서 실행되어야하는 계산을 정의한다. backward 계산은 backward()를
    이용시에 Pytorch가 알아서 해주니깐 forward()만을 정의해주면 된다
    input을 넣어서 어떤 계산을 진행해서 output이 나올지를 정의해준다고 이해하면 된다
'''
class moving_avg(nn.Module):
    "M.A.는 시계열의 trend에 집중하는 것을 막는다"
    def __init__(self,kernel_size,stride):
        super(moving_avg,self).__init__()
        # super(Model_Name,self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size = kernel_size, stride = stride, padding =0)
        # pooling 계산을 하는 1차원 평균으로 pooling계산
    
    def forward(self,x):
        " time series의 양 쪽 끝을 padding 한다"
        print(x[:,0:1])
        front = x[:,0:1,:].repeat(1,(self.kernel_size-1)//2,1)
        # Repeats this tensor along the specified dimensions.
        #print("front",front)
        
        end = x[:,-1,:].repeat(1,(self.kernel_size-1)//2,1)
        #print("end",end)
        
# front와 end를 추가해서 moving average를 계산해서 공백이 없도록
        
        x = torch.cat([front, x, end], dim=1)
        #print("cat",x,x.size())
        
        #print('before avg',x.permute(0,2,1))
        x = self.avg(x.permute(0,2,1)) #pooling 계산
        #print(x)
        
        x = x.permute(0,2,1) # 원상복귀
        #print(x)
        return x
150/3:
x=torch.FloatTensor([2,1,3,4]).view(1,-1,1) #4개의 데이터를 가진 것을 표현
print(x)
print('-----------------')
ma = moving_avg(3,1) 
x = ma.forward(x)
print(x)
150/4:
class series_decomp(nn.Module):
    "statsmodels.tsa의 seasonal_decompose와 역할이 똑같다"
    def __init__(self,kernel_size):
        super(series_decomp,self).__init__()
        self.moving_avg = moving_avg(kernel_size,stride = 1)
    
    def forward(self,x):
        moving_mean = self.moving_avg(x) #output은 (batch,sequence_length,input_size)
        res = x - moving_mean 
        # Classical ma중에서 additive model
        print(res.permute(1,2,0).size())
        print('----')
        print(moving_mean.permute(1,2,0).size())
        return res.permute(1,2,0),moving_mean.permute(1,2,0)
150/5:
x = torch.FloatTensor([1,2,3,4]).view(1,-1,1)
decomp = series_decomp(3) 
# kernel_size를 짝수로하면 오류 발생 (size가 맞지 않은 오류)
# 이경우는 짝수 ma를 두번진행하면 된다(아래 예시)
a,b=decomp.forward(x)
150/6:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = configs.seq_len # configure의 sequence length
        self.pred_len = configs.pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs.individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = configs.enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
150/7:
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    """
    Decomposition-Linear
    """
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 5 #커널사이즈를 5로 변경
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        

    def forward(self, x):
        # x: [Batch, Input length, Channel]
        seasonal_init, trend_init = self.decomposition(x)# res = seasonality 를 moving_mean은 trend를 의미
        # seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
        seasonal_init, trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1)
        if self.individual:
            seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],dtype=seasonal_init.dtype).to(seasonal_init.device)
             # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
            trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len],dtype=trend_init.dtype).to(trend_init.device)
            for i in range(self.channels):
                seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
                # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                # 이때 RNN과 달리 한번에 학습을 진행하기 때문에 seq의 배열 자체가 feature가 됨
        else:
            seasonal_output = self.Linear_Seasonal(seasonal_init)
            trend_output = self.Linear_Trend(trend_init)

        x = seasonal_output + trend_output
        return x.permute(0,2,1) # [batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)]로 변경해서 return
150/8: torch.ones(2,1,3,dtype=float) # batch는 2 length는 1 input size=3
150/9:
import torch
import torch.nn as nn
import torch.nn.functional as F
# 이 모듈에는 torch.nn 라이브러리의 모든 함수가 포함되어 있다 다양한 손실 및
# 활성화 함수 뿐만 아니라, 풀링(pooling) 함수와 같이 신경망을 만드는데 편리한 몇 가지
# 함수도 여기에서 찾을 수 있다.
import numpy as np
import pandas as pd
150/10:
data = pd.read_csv('../traffic/data/5.csv')
data
150/11: data.drop(columns=['service_name','packets','unknown'])
150/12:
scaler = MinMaxScaler()
data[['volumn']] = scaler.fit_transform(data[['volumn']])
data
150/13:
def seq_data(x,y,sequence_length,pred_len):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length:i+sequence_length+pred_len].values)
#             if i == 0:
#                 print(x.iloc[i:i+sequence_length].values)
#                 print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,360).to(device)
150/14:
split = 1440 # split까지 학습을 하게 되면 split-pred_len까지의 정답은 알려준셈이 된다
sequence_length = 1800
pred_len = 720
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
Y = data['volumn']
x_seq, target = seq_data(x=X,y=Y,sequence_length=sequence_length,pred_len=pred_len)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
150/15:
split = 1440 # split까지 학습을 하게 되면 split-pred_len까지의 정답은 알려준셈이 된다
sequence_length = 1800
pred_len = 720
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
Y = data['volumn']
x_seq, target = seq_data(x=X,y=Y,sequence_length=sequence_length,pred_len=pred_len)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
150/16:
def seq_data(x,y,sequence_length,pred_len):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length:i+sequence_length+pred_len].values)
            if i == 0:
                print(x.iloc[i:i+sequence_length].values)
                print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,360).to(device)
150/17:
split = 1440 # split까지 학습을 하게 되면 split-pred_len까지의 정답은 알려준셈이 된다
sequence_length = 1800
pred_len = 720
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
Y = data['volumn']
x_seq, target = seq_data(x=X,y=Y,sequence_length=sequence_length,pred_len=pred_len)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
150/18:
def seq_data(x,y,sequence_length,pred_len):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length:i+sequence_length+pred_len].values)
            if i == 0:
                print(x.iloc[i:i+sequence_length].values)
                print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,720).to(device)
150/19:
def seq_data(x,y,sequence_length,pred_len):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length:i+sequence_length+pred_len].values)
            if i == 0:
                print(x.iloc[i:i+sequence_length].values)
                print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,pred_len).to(device)
150/20:
split = 1440 # split까지 학습을 하게 되면 split-pred_len까지의 정답은 알려준셈이 된다
sequence_length = 1800
pred_len = 720
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
Y = data['volumn']
x_seq, target = seq_data(x=X,y=Y,sequence_length=sequence_length,pred_len=pred_len)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
150/21:
def seq_data(x,y,sequence_length,pred_len):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length:i+sequence_length+pred_len].values)
            if i == 0:
#                 print(x.iloc[i:i+sequence_length].values)
#                 print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,pred_len).to(device)
150/22:
split = 1440 # split까지 학습을 하게 되면 split-pred_len까지의 정답은 알려준셈이 된다
sequence_length = 1800
pred_len = 720
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
Y = data['volumn']
x_seq, target = seq_data(x=X,y=Y,sequence_length=sequence_length,pred_len=pred_len)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
150/23:
def seq_data(x,y,sequence_length,pred_len):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length:i+sequence_length+pred_len].values)
#             if i == 0:
#                  print(x.iloc[i:i+sequence_length].values)
#                  print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,pred_len).to(device)
150/24:
def seq_data(x,y,sequence_length,pred_len):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length:i+sequence_length+pred_len].values)
#             if i == 0:
#                  print(x.iloc[i:i+sequence_length].values)
#                  print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,pred_len).to(device)
150/25:
split = 1440 # split까지 학습을 하게 되면 split-pred_len까지의 정답은 알려준셈이 된다
sequence_length = 1800
pred_len = 720
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
Y = data['volumn']
x_seq, target = seq_data(x=X,y=Y,sequence_length=sequence_length,pred_len=pred_len)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
150/26:
device = torch.device("cuda:0")
torch_data = TensorDataset(x_seq,target)
150/27:
batch_size = 32
train_loader = DataLoader(torch_data,batch_size = batch_size)
150/28:
count =1
for i,j in train_loader:
    print(i.cpu().size())
    print(j.cpu().size())
    count+=1
print(count)
150/29:
model = Model(seq_len=sequence_length,pred_len=pred_len,individual=individual,enc_in=enc_in).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
# Q) 왜 learning_rate를 조정하면 MSE값의 차이가 크게나지
150/30:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
150/31: len(Y[-(sequence_length+pred_len):-(pred_len)].tolist())
150/32:
out_list = []


with torch.no_grad():
    model.eval()
    #if num==0:
    new_train=[Y[-(sequence_length+pred_len):-(pred_len)].tolist()]
#     else:
#         new_train = new_train[1:]+out.cpu().tolist()
        #print(type(new_train))
    new_tr = torch.FloatTensor(new_train).unsqueeze(2).to(device)
    out = model(new_tr)
#    out_list.append(out)
#   loss_graph.append(running_loss/n)
print(out)
150/33:
plot_data = np.array(out.cpu().tolist()).reshape(1,-1)[0]
plot_data
150/34:
plot_data = np.array(out.cpu().tolist()).reshape(1,-1)[0]
plt.plot(np.arange(len(plot_data)),plot_data,label = 'predcition')
plt.plot(np.arange(len(plot_data)),Y[-360:],label = 'real')
plt.legend()
150/35:
plot_data = np.array(out.cpu().tolist()).reshape(1,-1)[0]
plt.plot(np.arange(len(plot_data)),plot_data,label = 'predcition')
plt.plot(np.arange(len(plot_data)),Y[-720:],label = 'real')
plt.legend()
150/36: data['volumn'].plot(figsize=(14,8))
152/1:
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import torch
import torch.nn as nn
import torch.optim as optim
import torch.functional as F
from torch.utils.data import TensorDataset,DataLoader
import matplotlib.pyplot as plt
152/2:
data = pd.read_csv('../traffic/data/5.csv')
data
152/3:
data['datetime'] = pd.to_datetime(data['datetime'])
data['datetime'].dtypes
152/4: data.drop(columns=['unknown','service_name','packets'],inplace = True)
152/5:
scaler = MinMaxScaler()
data[['volumn']] = scaler.fit_transform(data[['volumn']])
data
152/6: device = torch.device("cuda:0")
152/7:
def seq_data(x,sequence_length):
    x_seq = []
    target = []
    for i in range(len(x)-sequence_length):
        x_seq.append(x[i:i+sequence_length])
        target.append(x[i+sequence_length])

    
    return torch.FloatTensor(x_seq).to(device), torch.FloatTensor(target).to(device).view(-1,1)
152/8:
split = 1440
sequence_length = 60
X = data[['volumn']].values[:-split]
x_seq, target = seq_data(X,sequence_length)
152/9: x_seq
152/10: target
152/11: data['volumn'].iloc[60]
152/12: data['volumn'].iloc[59]
152/13:
batch_size = 64
train = TensorDataset(x_seq,target)
train_loader = DataLoader(dataset = train,batch_size = batch_size,shuffle=True)
# shuffle 제거
152/14:
input_size = x_seq.size(2)
num_layers = 2
hidden_size = 8
152/15:
class VanillaRNN(nn.Module):
    
    def __init__(self,input_size,hidden_size,sequence_length,num_layers,device):
        super(VanillaRNN,self).__init__()
        self.device = device
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.rnn = nn.RNN(input_size,hidden_size,num_layers,batch_first = True)
        # input_size는 feature 개수를 의미함
        # hidden_size는 hidden_state의 feature 개수를 의미함 즉 hidden_vector의 크기
        # self.fc = nn.Sequential(nn.Linear(hidden_size*sequence_length,1),nn.Sigmoid())
   
    def forward(self,x):
        h0 = torch.zeros(self.num_layers,x.size()[0],self.hidden_size).to(self.device)
        out,h_n = self.rnn(x,h0)
        out = out.reshape(out.shape[0],-1)
        #out = self.fc(out)
        return out
152/16: from torch.nn import L1Loss
152/17:
model = VanillaRNN(input_size = input_size, hidden_size = hidden_size, sequence_length = sequence_length,
                  num_layers = num_layers, device=device).to(device) # GPU 연산을 위한 to(device)
criterion = L1Loss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3,)
152/18:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,target = dt
        
        out = model(seq)
        print(out)
        break
        loss = criterion(out,target)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
    break
    loss_graph.append(running_loss/n)
    print('mae',running_loss/n)
end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았음
152/19:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,target = dt
        
        out = model(seq)
        print(out.size())
        break
        loss = criterion(out,target)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
    break
    loss_graph.append(running_loss/n)
    print('mae',running_loss/n)
end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았음
153/1:
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import torch
import torch.nn as nn
import torch.optim as optim
import torch.functional as F
from torch.utils.data import TensorDataset,DataLoader
import matplotlib.pyplot as plt
153/2:
data = pd.read_csv('../traffic/data/5.csv')
data
153/3:
data['datetime'] = pd.to_datetime(data['datetime'])
data['datetime'].dtypes
153/4: data.drop(columns=['unknown','service_name','packets'],inplace = True)
153/5:
scaler = MinMaxScaler()
data[['volumn']] = scaler.fit_transform(data[['volumn']])
data
153/6: device = torch.device("cuda:0")
153/7:
def seq_data(x,sequence_length,pred_len):
    x_seq = []
    target = []
    for i in range(len(x)-sequence_length):
        x_seq.append(x[i:i+sequence_length])
        target.append(x[i+sequence_length:i+sequence_length+pred_len])

    
    return torch.FloatTensor(x_seq).to(device), torch.FloatTensor(target).to(device).view(-1,pred_len,1)
153/8:
split = 1440
sequence_length = 60
X = data[['volumn']].values[:-split]
x_seq, target = seq_data(X,sequence_length)
153/9:
split = 1440
sequence_length = 60
hidden_size = pred_len = 8
X = data[['volumn']].values[:-split]
x_seq, target = seq_data(X,sequence_length,pred_len)
153/10:
def seq_data(x,sequence_length,pred_len):
    x_seq = []
    target = []
    for i in range(len(x)-sequence_length):
        x_seq.append(x[i:i+sequence_length])
        target.append(x[i+sequence_length:i+sequence_length+pred_len])
        print(target)

    
    return torch.FloatTensor(x_seq).to(device), torch.FloatTensor(target).to(device).view(-1,pred_len,1)
153/11:
split = 1440
sequence_length = 60
hidden_size = pred_len = 8
X = data[['volumn']].values[:-split]
x_seq, target = seq_data(X,sequence_length,pred_len)
153/12:
def seq_data(x,sequence_length,pred_len):
    x_seq = []
    target = []
    for i in range(len(x)-sequence_length):
        x_seq.append(x[i:i+sequence_length])
        target.append(Y[i+sequence_length:i+sequence_length+pred_len])
        

    
    return torch.FloatTensor(x_seq).to(device), torch.FloatTensor(target).to(device).view(-1,pred_len,1)
153/13:
split = 1440
sequence_length = 60
hidden_size = pred_len = 8
X = data[['volumn']].values[:-split]
x_seq, target = seq_data(X,sequence_length,pred_len)
153/14:
split = 1440
sequence_length = 60
hidden_size = pred_len = 8
X = data[['volumn']].values[:-split]
Y = data['volumn']
x_seq, target = seq_data(X,sequence_length,pred_len)
153/15:
split = 1440
sequence_length = 60
hidden_size = pred_len = 8
X = data[['volumn']].values[:-split]
Y = data['volumn'].values
x_seq, target = seq_data(X,Y,sequence_length,pred_len)
153/16:
def seq_data(x,y,sequence_length,pred_len):
    x_seq = []
    target = []
    for i in range(len(x)-sequence_length):
        x_seq.append(x[i:i+sequence_length])
        target.append(y[i+sequence_length:i+sequence_length+pred_len])
    return torch.FloatTensor(x_seq).to(device), torch.FloatTensor(target).to(device).view(-1,pred_len,1)
153/17:
split = 1440
sequence_length = 60
hidden_size = pred_len = 8
X = data[['volumn']].values[:-split]
Y = data['volumn'].values
x_seq, target = seq_data(x=X,y=Y,sequence_length=sequence_length,pred_len=pred_len)
153/18: x_seq
153/19: target
153/20:
batch_size = 64
train = TensorDataset(x_seq,target)
train_loader = DataLoader(dataset = train,batch_size = batch_size,shuffle=True)
# shuffle 제거
153/21:
input_size = x_seq.size(2)
num_layers = 2
hidden_size = 8 #batch_first = True로 해두면 (batch_size X sequence_length X hidden_size)로 나오게 된다
153/22:
class VanillaRNN(nn.Module):
    
    def __init__(self,input_size,hidden_size,sequence_length,num_layers,device):
        super(VanillaRNN,self).__init__()
        self.device = device
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.rnn = nn.RNN(input_size,hidden_size,num_layers,batch_first = True)
        # input_size는 feature 개수를 의미함
        # hidden_size는 hidden_state의 feature 개수를 의미함 즉 hidden_vector의 크기
        # self.fc = nn.Sequential(nn.Linear(hidden_size*sequence_length,1),nn.Sigmoid())
   
    def forward(self,x):
        h0 = torch.zeros(self.num_layers,x.size()[0],self.hidden_size).to(self.device)
        out,h_n = self.rnn(x,h0) #out 크기 (batchsize,length,feature size)
        out = out.reshape(out.shape[0],-1)
        #out = self.fc(out)
        return out
153/23: from torch.nn import L1Loss
153/24:
model = VanillaRNN(input_size = input_size, hidden_size = hidden_size, sequence_length = sequence_length,
                  num_layers = num_layers, device=device).to(device) # GPU 연산을 위한 to(device)
criterion = L1Loss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3,)
153/25:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,target = dt
        
        out = model(seq)
        
        loss = criterion(out,target)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    print('mae',running_loss/n)
end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았음
153/26:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,target = dt
        
        out = model(seq)
        print(out)
        break
        loss = criterion(out,target)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    print('mae',running_loss/n)
end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았음
153/27:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,target = dt
        
        out = model(seq)
        print('out',out)
        print('target',target)
        break
        loss = criterion(out,target)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    print('mae',running_loss/n)
end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았음
153/28:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,target = dt
        
        out = model(seq)
        print('out',out.size())
        print('target',target.size())
        break
        loss = criterion(out,target)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    print('mae',running_loss/n)
end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았음
153/29:
class VanillaRNN(nn.Module):
    
    def __init__(self,input_size,hidden_size,sequence_length,num_layers,device):
        super(VanillaRNN,self).__init__()
        self.device = device
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.rnn = nn.RNN(input_size,hidden_size,num_layers,batch_first = True)
        # input_size는 feature 개수를 의미함
        # hidden_size는 hidden_state의 feature 개수를 의미함 즉 hidden_vector의 크기
        # self.fc = nn.Sequential(nn.Linear(hidden_size*sequence_length,1),nn.Sigmoid())
   
    def forward(self,x):
        h0 = torch.zeros(self.num_layers,x.size()[0],self.hidden_size).to(self.device)
        out,h_n = self.rnn(x,h0) #out 크기 (batchsize,length,feature size)
        #out = out.reshape(out.shape[0],-1)
        #out = self.fc(out)
        return out
153/30: from torch.nn import L1Loss
153/31:
model = VanillaRNN(input_size = input_size, hidden_size = hidden_size, sequence_length = sequence_length,
                  num_layers = num_layers, device=device).to(device) # GPU 연산을 위한 to(device)
criterion = L1Loss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3,)
153/32:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,target = dt
        
        out = model(seq)
        print('out',out.size())
        print('target',target.size())
        break
        loss = criterion(out,target)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    print('mae',running_loss/n)
end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았음
153/33:
class VanillaRNN(nn.Module):
    
    def __init__(self,input_size,hidden_size,sequence_length,num_layers,device):
        super(VanillaRNN,self).__init__()
        self.device = device
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.rnn = nn.RNN(input_size,hidden_size,num_layers,batch_first = True)
        # input_size는 feature 개수를 의미함
        # hidden_size는 hidden_state의 feature 개수를 의미함 즉 hidden_vector의 크기
        self.fc = nn.Sequential(nn.Linear(hidden_size*sequence_length,1),nn.Sigmoid())
   
    def forward(self,x):
        h0 = torch.zeros(self.num_layers,x.size()[0],self.hidden_size).to(self.device)
        out,h_n = self.rnn(x,h0) #out 크기 (batchsize,length,feature size)
        out = out.reshape(out.shape[0],-1)
        out = self.fc(out)
        return out
153/34: from torch.nn import L1Loss
153/35:
model = VanillaRNN(input_size = input_size, hidden_size = hidden_size, sequence_length = sequence_length,
                  num_layers = num_layers, device=device).to(device) # GPU 연산을 위한 to(device)
criterion = L1Loss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3,)
153/36:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,target = dt
        
        out = model(seq)
        print('out',out.size())
        print('target',target.size())
        break
        loss = criterion(out,target)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    print('mae',running_loss/n)
end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았음
153/37:
class VanillaRNN(nn.Module):
    
    def __init__(self,input_size,hidden_size,sequence_length,num_layers,device):
        super(VanillaRNN,self).__init__()
        self.device = device
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.rnn = nn.RNN(input_size,hidden_size,num_layers,batch_first = True)
        # input_size는 feature 개수를 의미함
        # hidden_size는 hidden_state의 feature 개수를 의미함 즉 hidden_vector의 크기
        #self.fc = nn.Sequential(nn.Linear(hidden_size*sequence_length,1),nn.Sigmoid())
   
    def forward(self,x):
        h0 = torch.zeros(self.num_layers,x.size()[0],self.hidden_size).to(self.device)
        out,h_n = self.rnn(x,h0) #out 크기 (batchsize,length,feature size)
        #out = out.reshape(out.shape[0],-1)
        #out = self.fc(out)
        return out
153/38: from torch.nn import L1Loss
153/39:
model = VanillaRNN(input_size = input_size, hidden_size = hidden_size, sequence_length = sequence_length,
                  num_layers = num_layers, device=device).to(device) # GPU 연산을 위한 to(device)
criterion = L1Loss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3,)
153/40:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,target = dt
        
        out = model(seq)
        print('out',out.size())
        print('target',target.size())
        break
        loss = criterion(out,target)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    print('mae',running_loss/n)
end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았음
153/41:
class VanillaRNN(nn.Module):
    
    def __init__(self,input_size,hidden_size,sequence_length,num_layers,device):
        super(VanillaRNN,self).__init__()
        self.device = device
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.rnn = nn.RNN(input_size,hidden_size,num_layers,batch_first = True)
        # input_size는 feature 개수를 의미함
        # hidden_size는 hidden_state의 feature 개수를 의미함 즉 hidden_vector의 크기
        self.fc = nn.Sequential(nn.Linear(hidden_size,1))
   
    def forward(self,x):
        h0 = torch.zeros(self.num_layers,x.size()[0],self.hidden_size).to(self.device)
        out,h_n = self.rnn(x,h0) #out 크기 (batchsize,length,feature size)
        # out크기를 보니 여기서 rnn은 many to many 즉 전체 입력에 대해 전체 출력이 나온다 
        out = out.reshape(out.shape[0],-1)
        out = self.fc(out)
        return out
153/42: from torch.nn import L1Loss
153/43:
model = VanillaRNN(input_size = input_size, hidden_size = hidden_size, sequence_length = sequence_length,
                  num_layers = num_layers, device=device).to(device) # GPU 연산을 위한 to(device)
criterion = L1Loss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3,)
153/44:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,target = dt
        
        out = model(seq)
        print('out',out.size())
        print('target',target.size())
        break
        loss = criterion(out,target)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    print('mae',running_loss/n)
end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았음
153/45:
class VanillaRNN(nn.Module):
    
    def __init__(self,input_size,hidden_size,sequence_length,num_layers,device):
        super(VanillaRNN,self).__init__()
        self.device = device
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.rnn = nn.RNN(input_size,hidden_size,num_layers,batch_first = True)
        # input_size는 feature 개수를 의미함
        # hidden_size는 hidden_state의 feature 개수를 의미함 즉 hidden_vector의 크기
        self.fc = nn.Sequential(nn.Linear(hidden_size,1))
   
    def forward(self,x):
        h0 = torch.zeros(self.num_layers,x.size()[0],self.hidden_size).to(self.device)
        out,h_n = self.rnn(x,h0) #out 크기 (batchsize,length,feature size)
        # out크기를 보니 여기서 rnn은 many to many 즉 전체 입력에 대해 전체 출력이 나온다 
        # out = out.reshape(out.shape[0],-1)
        out = self.fc(out)
        return out
153/46: from torch.nn import L1Loss
153/47:
model = VanillaRNN(input_size = input_size, hidden_size = hidden_size, sequence_length = sequence_length,
                  num_layers = num_layers, device=device).to(device) # GPU 연산을 위한 to(device)
criterion = L1Loss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3,)
153/48:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,target = dt
        
        out = model(seq)
        print('out',out.size())
        print('target',target.size())
        break
        loss = criterion(out,target)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    print('mae',running_loss/n)
end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았음
153/49:
class VanillaRNN(nn.Module):
    
    def __init__(self,input_size,hidden_size,sequence_length,num_layers,device):
        super(VanillaRNN,self).__init__()
        self.device = device
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.rnn = nn.RNN(input_size,hidden_size,num_layers,batch_first = True)
        # input_size는 feature 개수를 의미함
        # hidden_size는 hidden_state의 feature 개수를 의미함 즉 hidden_vector의 크기
        self.fc = nn.Sequential(nn.Linear(hidden_size,1))
   
    def forward(self,x):
        h0 = torch.zeros(self.num_layers,x.size()[0],self.hidden_size).to(self.device)
        out,h_n = self.rnn(x,h0) #out 크기 (batchsize,length,feature size)
        # out크기를 보니 여기서 rnn은 many to many 즉 전체 입력에 대해 전체 출력이 나온다 
        # out = out.reshape(out.shape[0],-1)
        out = self.fc(out) 
        out = torch.index_select(out,dim=1,index=[-8:]) #파이토치 indexing을 구체적으로 하는방법
        
        return out
153/50:
class VanillaRNN(nn.Module):
    
    def __init__(self,input_size,hidden_size,sequence_length,num_layers,device):
        super(VanillaRNN,self).__init__()
        self.device = device
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.rnn = nn.RNN(input_size,hidden_size,num_layers,batch_first = True)
        # input_size는 feature 개수를 의미함
        # hidden_size는 hidden_state의 feature 개수를 의미함 즉 hidden_vector의 크기
        self.fc = nn.Sequential(nn.Linear(hidden_size,1))
   
    def forward(self,x):
        h0 = torch.zeros(self.num_layers,x.size()[0],self.hidden_size).to(self.device)
        out,h_n = self.rnn(x,h0) #out 크기 (batchsize,length,feature size)
        # out크기를 보니 여기서 rnn은 many to many 즉 전체 입력에 대해 전체 출력이 나온다 
        # out = out.reshape(out.shape[0],-1)
        out = self.fc(out) 
        out = torch.index_select(out,dim=1,index=torch.LongTensor([-8:])) #파이토치 indexing을 구체적으로 하는방법
        
        return out
153/51:
class VanillaRNN(nn.Module):
    
    def __init__(self,input_size,hidden_size,sequence_length,num_layers,device):
        super(VanillaRNN,self).__init__()
        self.device = device
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.rnn = nn.RNN(input_size,hidden_size,num_layers,batch_first = True)
        # input_size는 feature 개수를 의미함
        # hidden_size는 hidden_state의 feature 개수를 의미함 즉 hidden_vector의 크기
        self.fc = nn.Sequential(nn.Linear(hidden_size,1))
   
    def forward(self,x):
        h0 = torch.zeros(self.num_layers,x.size()[0],self.hidden_size).to(self.device)
        out,h_n = self.rnn(x,h0) #out 크기 (batchsize,length,feature size)
        # out크기를 보니 여기서 rnn은 many to many 즉 전체 입력에 대해 전체 출력이 나온다 
        # out = out.reshape(out.shape[0],-1)
        out = self.fc(out) 
        out = torch.index_select(out,dim=1,index=torch.LongTensor(range([-8:0]))) #파이토치 indexing을 구체적으로 하는방법
        
        return out
153/52:
class VanillaRNN(nn.Module):
    
    def __init__(self,input_size,hidden_size,sequence_length,num_layers,device):
        super(VanillaRNN,self).__init__()
        self.device = device
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.rnn = nn.RNN(input_size,hidden_size,num_layers,batch_first = True)
        # input_size는 feature 개수를 의미함
        # hidden_size는 hidden_state의 feature 개수를 의미함 즉 hidden_vector의 크기
        self.fc = nn.Sequential(nn.Linear(hidden_size,1))
   
    def forward(self,x):
        h0 = torch.zeros(self.num_layers,x.size()[0],self.hidden_size).to(self.device)
        out,h_n = self.rnn(x,h0) #out 크기 (batchsize,length,feature size)
        # out크기를 보니 여기서 rnn은 many to many 즉 전체 입력에 대해 전체 출력이 나온다 
        # out = out.reshape(out.shape[0],-1)
        out = self.fc(out) 
        out = torch.index_select(out,dim=1,index=torch.LongTensor(range(-8,0))) #파이토치 indexing을 구체적으로 하는방법
        
        return out
153/53: from torch.nn import L1Loss
153/54: range(-8,0)
153/55: next(iter(range(-8,0)))
153/56:
model = VanillaRNN(input_size = input_size, hidden_size = hidden_size, sequence_length = sequence_length,
                  num_layers = num_layers, device=device).to(device) # GPU 연산을 위한 to(device)
criterion = L1Loss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3,)
153/57:
for i in range(-8,0):
    print(i)
153/58:
class VanillaRNN(nn.Module):
    
    def __init__(self,input_size,hidden_size,sequence_length,num_layers,device):
        super(VanillaRNN,self).__init__()
        self.device = device
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.rnn = nn.RNN(input_size,hidden_size,num_layers,batch_first = True)
        # input_size는 feature 개수를 의미함
        # hidden_size는 hidden_state의 feature 개수를 의미함 즉 hidden_vector의 크기
        self.fc = nn.Sequential(nn.Linear(hidden_size,1))
   
    def forward(self,x):
        h0 = torch.zeros(self.num_layers,x.size()[0],self.hidden_size).to(self.device)
        out,h_n = self.rnn(x,h0) #out 크기 (batchsize,length,feature size)
        # out크기를 보니 여기서 rnn은 many to many 즉 전체 입력에 대해 전체 출력이 나온다 
        # out = out.reshape(out.shape[0],-1)
        out = self.fc(out) 
        out = torch.index_select(out,dim=1,index=torch.LongTensor([i for i in range(-8,0)])) #파이토치 indexing을 구체적으로 하는방법
        
        return out
153/59: from torch.nn import L1Loss
153/60:
model = VanillaRNN(input_size = input_size, hidden_size = hidden_size, sequence_length = sequence_length,
                  num_layers = num_layers, device=device).to(device) # GPU 연산을 위한 to(device)
criterion = L1Loss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3,)
153/61:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,target = dt
        
        out = model(seq)
        print('out',out.size())
        print('target',target.size())
        break
        loss = criterion(out,target)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    print('mae',running_loss/n)
end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았음
153/62:
class VanillaRNN(nn.Module):
    
    def __init__(self,input_size,hidden_size,sequence_length,num_layers,device):
        super(VanillaRNN,self).__init__()
        self.device = device
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.rnn = nn.RNN(input_size,hidden_size,num_layers,batch_first = True)
        # input_size는 feature 개수를 의미함
        # hidden_size는 hidden_state의 feature 개수를 의미함 즉 hidden_vector의 크기
        self.fc = nn.Sequential(nn.Linear(hidden_size,1))
   
    def forward(self,x):
        h0 = torch.zeros(self.num_layers,x.size()[0],self.hidden_size).to(self.device)
        out,h_n = self.rnn(x,h0) #out 크기 (batchsize,length,feature size)
        # out크기를 보니 여기서 rnn은 many to many 즉 전체 입력에 대해 전체 출력이 나온다 
        # out = out.reshape(out.shape[0],-1)
        out = self.fc(out) 
        out = torch.index_select(out.cpu(),dim=1,index=torch.LongTensor([i for i in range(-8,0)])) #파이토치 indexing을 구체적으로 하는방법
        out.to(device)
        return out
153/63: from torch.nn import L1Loss
153/64:
model = VanillaRNN(input_size = input_size, hidden_size = hidden_size, sequence_length = sequence_length,
                  num_layers = num_layers, device=device).to(device) # GPU 연산을 위한 to(device)
criterion = L1Loss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3,)
153/65:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,target = dt
        
        out = model(seq)
        print('out',out.size())
        print('target',target.size())
        break
        loss = criterion(out,target)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    print('mae',running_loss/n)
end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았음
153/66:
class VanillaRNN(nn.Module):
    
    def __init__(self,input_size,hidden_size,sequence_length,num_layers,device):
        super(VanillaRNN,self).__init__()
        self.device = device
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.rnn = nn.RNN(input_size,hidden_size,num_layers,batch_first = True)
        # input_size는 feature 개수를 의미함
        # hidden_size는 hidden_state의 feature 개수를 의미함 즉 hidden_vector의 크기
        self.fc = nn.Sequential(nn.Linear(hidden_size,1))
   
    def forward(self,x):
        h0 = torch.zeros(self.num_layers,x.size()[0],self.hidden_size).to(self.device)
        out,h_n = self.rnn(x,h0) #out 크기 (batchsize,length,feature size)
        # out크기를 보니 여기서 rnn은 many to many 즉 전체 입력에 대해 전체 출력이 나온다 
        # out = out.reshape(out.shape[0],-1)
        out = self.fc(out) 
        out = torch.index_select(out.cpu(),dim=1,index=torch.LongTensor([i for i in range(52,60)])) #파이토치 indexing을 구체적으로 하는방법
        out.to(device)
        return out
153/67: from torch.nn import L1Loss
153/68:
model = VanillaRNN(input_size = input_size, hidden_size = hidden_size, sequence_length = sequence_length,
                  num_layers = num_layers, device=device).to(device) # GPU 연산을 위한 to(device)
criterion = L1Loss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3,)
153/69:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,target = dt
        
        out = model(seq)
        print('out',out.size())
        print('target',target.size())
        break
        loss = criterion(out,target)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    print('mae',running_loss/n)
end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았음
153/70:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,target = dt
        print(seq)
        break
        out = model(seq)

    
        loss = criterion(out,target)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
    break
    loss_graph.append(running_loss/n)
    print('mae',running_loss/n)
end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았음
153/71: data
153/72:
batch_size = 64
train = TensorDataset(x_seq,target)
train_loader = DataLoader(dataset = train,batch_size = batch_size)
# shuffle 제거
153/73:
data = pd.read_csv('../traffic/data/5.csv')
data
153/74:
data['datetime'] = pd.to_datetime(data['datetime'])
data['datetime'].dtypes
153/75: data.drop(columns=['unknown','service_name','packets'],inplace = True)
153/76:
scaler = MinMaxScaler()
data[['volumn']] = scaler.fit_transform(data[['volumn']])
data
153/77: device = torch.device("cuda:0")
153/78:
def seq_data(x,y,sequence_length,pred_len):
    x_seq = []
    target = []
    for i in range(len(x)-sequence_length):
        x_seq.append(x[i:i+sequence_length])
        target.append(y[i+sequence_length:i+sequence_length+pred_len])
    return torch.FloatTensor(x_seq).to(device), torch.FloatTensor(target).to(device).view(-1,pred_len,1)
153/79:
split = 1440
sequence_length = 60
hidden_size = pred_len = 8
X = data[['volumn']].values[:-split]
Y = data['volumn'].values
x_seq, target = seq_data(x=X,y=Y,sequence_length=sequence_length,pred_len=pred_len)
153/80: target
153/81: data['volumn'].iloc[60]
153/82: data['volumn'].iloc[59]
153/83:
batch_size = 64
train = TensorDataset(x_seq,target)
train_loader = DataLoader(dataset = train,batch_size = batch_size)
# shuffle 제거
153/84:
input_size = x_seq.size(2)
num_layers = 2
hidden_size = 8 #batch_first = True로 해두면 (batch_size X sequence_length X hidden_size)로 나오게 된다
153/85:
class VanillaRNN(nn.Module):
    
    def __init__(self,input_size,hidden_size,sequence_length,num_layers,device):
        super(VanillaRNN,self).__init__()
        self.device = device
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.rnn = nn.RNN(input_size,hidden_size,num_layers,batch_first = True)
        # input_size는 feature 개수를 의미함
        # hidden_size는 hidden_state의 feature 개수를 의미함 즉 hidden_vector의 크기
        self.fc = nn.Sequential(nn.Linear(hidden_size,1))
   
    def forward(self,x):
        h0 = torch.zeros(self.num_layers,x.size()[0],self.hidden_size).to(self.device)
        out,h_n = self.rnn(x,h0) #out 크기 (batchsize,length,feature size)
        # out크기를 보니 여기서 rnn은 many to many 즉 전체 입력에 대해 전체 출력이 나온다 
        # out = out.reshape(out.shape[0],-1)
        out = self.fc(out) 
        out = torch.index_select(out.cpu(),dim=1,index=torch.LongTensor([i for i in range(52,60)])) #파이토치 indexing을 구체적으로 하는방법
        out.to(device)
        return out
153/86: from torch.nn import L1Loss
153/87:
model = VanillaRNN(input_size = input_size, hidden_size = hidden_size, sequence_length = sequence_length,
                  num_layers = num_layers, device=device).to(device) # GPU 연산을 위한 to(device)
criterion = L1Loss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3,)
153/88:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,target = dt
        print(seq)
        break
        out = model(seq)

    
        loss = criterion(out,target)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
    break
    loss_graph.append(running_loss/n)
    print('mae',running_loss/n)
end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았음
153/89:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,target = dt
        
        out = model(seq)
    
        loss = criterion(out,target)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
    
    loss_graph.append(running_loss/n)
    print('mae',running_loss/n)
end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았음
153/90:
def seq_data(x,y,sequence_length,pred_len):
    x_seq = []
    target = []
    for i in range(len(x)-sequence_length):
        x_seq.append(x[i:i+sequence_length])
        target.append(y[i+sequence_length:i+sequence_length+pred_len])
    return torch.FloatTensor(x_seq).to(device), torch.FloatTensor(target).view(-1,pred_len,1).to(device)
153/91:
split = 1440
sequence_length = 60
hidden_size = pred_len = 8
X = data[['volumn']].values[:-split]
Y = data['volumn'].values
x_seq, target = seq_data(x=X,y=Y,sequence_length=sequence_length,pred_len=pred_len)
153/92: x_seq
153/93: target
153/94: data['volumn'].iloc[60]
153/95: data['volumn'].iloc[59]
153/96:
batch_size = 64
train = TensorDataset(x_seq,target)
train_loader = DataLoader(dataset = train,batch_size = batch_size)
# shuffle 제거
153/97:
input_size = x_seq.size(2)
num_layers = 2
hidden_size = 8 #batch_first = True로 해두면 (batch_size X sequence_length X hidden_size)로 나오게 된다
153/98:
class VanillaRNN(nn.Module):
    
    def __init__(self,input_size,hidden_size,sequence_length,num_layers,device):
        super(VanillaRNN,self).__init__()
        self.device = device
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.rnn = nn.RNN(input_size,hidden_size,num_layers,batch_first = True)
        # input_size는 feature 개수를 의미함
        # hidden_size는 hidden_state의 feature 개수를 의미함 즉 hidden_vector의 크기
        self.fc = nn.Sequential(nn.Linear(hidden_size,1))
   
    def forward(self,x):
        h0 = torch.zeros(self.num_layers,x.size()[0],self.hidden_size).to(self.device)
        out,h_n = self.rnn(x,h0) #out 크기 (batchsize,length,feature size)
        # out크기를 보니 여기서 rnn은 many to many 즉 전체 입력에 대해 전체 출력이 나온다 
        # out = out.reshape(out.shape[0],-1)
        out = self.fc(out) 
        out = torch.index_select(out.cpu(),dim=1,index=torch.LongTensor([i for i in range(52,60)])) #파이토치 indexing을 구체적으로 하는방법
        out.to(device)
        return out
153/99: from torch.nn import L1Loss
153/100:
model = VanillaRNN(input_size = input_size, hidden_size = hidden_size, sequence_length = sequence_length,
                  num_layers = num_layers, device=device).to(device) # GPU 연산을 위한 to(device)
criterion = L1Loss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3,)
153/101:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,target = dt
        
        out = model(seq)
    
        loss = criterion(out,target)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
    
    loss_graph.append(running_loss/n)
    print('mae',running_loss/n)
end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았음
153/102:
class VanillaRNN(nn.Module):
    
    def __init__(self,input_size,hidden_size,sequence_length,num_layers,device):
        super(VanillaRNN,self).__init__()
        self.device = device
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.rnn = nn.RNN(input_size,hidden_size,num_layers,batch_first = True)
        # input_size는 feature 개수를 의미함
        # hidden_size는 hidden_state의 feature 개수를 의미함 즉 hidden_vector의 크기
        self.fc = nn.Sequential(nn.Linear(hidden_size,1))
   
    def forward(self,x):
        h0 = torch.zeros(self.num_layers,x.size()[0],self.hidden_size).to(self.device)
        out,h_n = self.rnn(x,h0) #out 크기 (batchsize,length,feature size)
        # out크기를 보니 여기서 rnn은 many to many 즉 전체 입력에 대해 전체 출력이 나온다 
        # out = out.reshape(out.shape[0],-1)
        out = self.fc(out) 
        out = torch.index_select(out.cpu(),dim=1,index=torch.LongTensor([i for i in range(52,60)])) #파이토치 indexing을 구체적으로 하는방법
        out = out.to(device)
        return out
153/103: from torch.nn import L1Loss
153/104:
model = VanillaRNN(input_size = input_size, hidden_size = hidden_size, sequence_length = sequence_length,
                  num_layers = num_layers, device=device).to(device) # GPU 연산을 위한 to(device)
criterion = L1Loss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3,)
153/105:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,target = dt
        
        out = model(seq)
    
        loss = criterion(out,target)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
    
    loss_graph.append(running_loss/n)
    print('mae',running_loss/n)
end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았음
153/106:
out_list=[]

with torch.no_grad():
    model.eval()
    new_tr = torch.FloatTensor(new_train).unsqueeze(0).to(device)
    out = model(new_tr)
    # out_list.append(out)

    #       new_train = torch.FloatTensor(new_train[1:].tolist()+out.cpu().tolist())#.view(-1,1).unsqueeze(0) data;pader를 안쓰고도 가능
    #       print(new_train)
153/107:
new_train = X[-60:]
len(new_train)
153/108: new_train[1:]
153/109: len(new_train)
153/110:
out_list=[]

with torch.no_grad():
    model.eval()
    new_tr = torch.FloatTensor(new_train).unsqueeze(0).to(device)
    out = model(new_tr)
    # out_list.append(out)

    #       new_train = torch.FloatTensor(new_train[1:].tolist()+out.cpu().tolist())#.view(-1,1).unsqueeze(0) data;pader를 안쓰고도 가능
    #       print(new_train)
153/111:
predictions = [out.cpu().item()]
predictions
153/112:
predictions = [out.cpu().view(1,-1)]
predictions
153/113:
fig,axes = plt.subplots(1,1,figsize=(14,8))
axes.plot(np.arange(pred_len),data[['volumn']].values[-split+pred_len:])
axes.plot(np.arange(pred_len),predictions)
153/114:
fig,axes = plt.subplots(1,1,figsize=(14,8))
axes.plot(np.arange(pred_len),data[['volumn']].values[-split+pred_len:-split+pred_len+pred_len])
axes.plot(np.arange(pred_len),predictions)
153/115:
predictions = np.array(out.cpu().view(1,-1))
predictions
153/116:
fig,axes = plt.subplots(1,1,figsize=(14,8))
axes.plot(np.arange(pred_len),data[['volumn']].values[-split+pred_len:-split+pred_len+pred_len])
axes.plot(np.arange(pred_len),predictions)
153/117:
fig,axes = plt.subplots(1,1,figsize=(14,8))
axes.plot(np.arange(pred_len),data[['volumn']].values[-split+pred_len:-split+pred_len+pred_len])
axes.plot(np.arange(pred_len),predictions[0])
153/118:
class VanillaRNN(nn.Module):
    
    def __init__(self,input_size,hidden_size,sequence_length,num_layers,device):
        super(VanillaRNN,self).__init__()
        self.device = device
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.rnn = nn.RNN(input_size,hidden_size,num_layers,batch_first = True)
        # input_size는 feature 개수를 의미함
        # hidden_size는 hidden_state의 feature 개수를 의미함 즉 hidden_vector의 크기
        self.fc = nn.Sequential(nn.Linear(hidden_size,1))
   
    def forward(self,x):
        h0 = torch.zeros(self.num_layers,x.size()[0],self.hidden_size).to(self.device)
        out,h_n = self.rnn(x,h0) #out 크기 (batchsize,length,feature size)
        # out크기를 보니 여기서 rnn은 many to many 즉 전체 입력에 대해 전체 출력이 나온다 
        # out = out.reshape(out.shape[0],-1)
        out = self.fc(out) 
        out = torch.index_select(out.cpu(),dim=1,index=torch.LongTensor([i for i in range(20,60)])) #파이토치 indexing을 구체적으로 하는방법
        out = out.to(device)
        return out
153/119:
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import torch
import torch.nn as nn
import torch.optim as optim
import torch.functional as F
from torch.utils.data import TensorDataset,DataLoader
import matplotlib.pyplot as plt
153/120:
data = pd.read_csv('../traffic/data/5.csv')
data
153/121:
data['datetime'] = pd.to_datetime(data['datetime'])
data['datetime'].dtypes
153/122: data.drop(columns=['unknown','service_name','packets'],inplace = True)
153/123:
scaler = MinMaxScaler()
data[['volumn']] = scaler.fit_transform(data[['volumn']])
data
153/124: device = torch.device("cuda:0")
153/125:
def seq_data(x,y,sequence_length,pred_len):
    x_seq = []
    target = []
    for i in range(len(x)-sequence_length):
        x_seq.append(x[i:i+sequence_length])
        target.append(y[i+sequence_length:i+sequence_length+pred_len])
    return torch.FloatTensor(x_seq).to(device), torch.FloatTensor(target).view(-1,pred_len,1).to(device)
153/126:
split = 1440
sequence_length = 60
hidden_size = 8
X = data[['volumn']].values[:-split]
Y = data['volumn'].values
x_seq, target = seq_data(x=X,y=Y,sequence_length=sequence_length,pred_len=pred_len)
153/127: x_seq
153/128: target
153/129: data['volumn'].iloc[60]
153/130: data['volumn'].iloc[59]
153/131:
batch_size = 64
train = TensorDataset(x_seq,target)
train_loader = DataLoader(dataset = train,batch_size = batch_size)
# shuffle 제거
153/132:
input_size = x_seq.size(2)
num_layers = 2
hidden_size = 8 #batch_first = True로 해두면 (batch_size X sequence_length X hidden_size)로 나오게 된다
153/133:
class VanillaRNN(nn.Module):
    
    def __init__(self,input_size,hidden_size,sequence_length,num_layers,device):
        super(VanillaRNN,self).__init__()
        self.device = device
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.rnn = nn.RNN(input_size,hidden_size,num_layers,batch_first = True)
        # input_size는 feature 개수를 의미함
        # hidden_size는 hidden_state의 feature 개수를 의미함 즉 hidden_vector의 크기
        self.fc = nn.Sequential(nn.Linear(hidden_size,1))
   
    def forward(self,x):
        h0 = torch.zeros(self.num_layers,x.size()[0],self.hidden_size).to(self.device)
        out,h_n = self.rnn(x,h0) #out 크기 (batchsize,length,feature size)
        # out크기를 보니 여기서 rnn은 many to many 즉 전체 입력에 대해 전체 출력이 나온다 
        # out = out.reshape(out.shape[0],-1)
        out = self.fc(out) 
        out = torch.index_select(out.cpu(),dim=1,index=torch.LongTensor([i for i in range(20,60)])) #파이토치 indexing을 구체적으로 하는방법
        out = out.to(device)
        return out
153/134: from torch.nn import L1Loss
153/135:
model = VanillaRNN(input_size = input_size, hidden_size = hidden_size, sequence_length = sequence_length,
                  num_layers = num_layers, device=device).to(device) # GPU 연산을 위한 to(device)
criterion = L1Loss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3,)
153/136:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,target = dt
        
        out = model(seq)
    
        loss = criterion(out,target)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
    
    loss_graph.append(running_loss/n)
    print('mae',running_loss/n)
end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았음
153/137:
split = 1440
sequence_length = 60
hidden_size = 8
pred_len = 40
X = data[['volumn']].values[:-split]
Y = data['volumn'].values
x_seq, target = seq_data(x=X,y=Y,sequence_length=sequence_length,pred_len=pred_len)
153/138: x_seq
153/139: target
153/140: data['volumn'].iloc[60]
153/141: data['volumn'].iloc[59]
153/142:
batch_size = 64
train = TensorDataset(x_seq,target)
train_loader = DataLoader(dataset = train,batch_size = batch_size)
# shuffle 제거
153/143:
input_size = x_seq.size(2)
num_layers = 2
hidden_size = 8 #batch_first = True로 해두면 (batch_size X sequence_length X hidden_size)로 나오게 된다
153/144:
class VanillaRNN(nn.Module):
    
    def __init__(self,input_size,hidden_size,sequence_length,num_layers,device):
        super(VanillaRNN,self).__init__()
        self.device = device
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.rnn = nn.RNN(input_size,hidden_size,num_layers,batch_first = True)
        # input_size는 feature 개수를 의미함
        # hidden_size는 hidden_state의 feature 개수를 의미함 즉 hidden_vector의 크기
        self.fc = nn.Sequential(nn.Linear(hidden_size,1))
   
    def forward(self,x):
        h0 = torch.zeros(self.num_layers,x.size()[0],self.hidden_size).to(self.device)
        out,h_n = self.rnn(x,h0) #out 크기 (batchsize,length,feature size)
        # out크기를 보니 여기서 rnn은 many to many 즉 전체 입력에 대해 전체 출력이 나온다 
        # out = out.reshape(out.shape[0],-1)
        out = self.fc(out) 
        out = torch.index_select(out.cpu(),dim=1,index=torch.LongTensor([i for i in range(20,60)])) #파이토치 indexing을 구체적으로 하는방법
        out = out.to(device)
        return out
153/145: from torch.nn import L1Loss
153/146:
model = VanillaRNN(input_size = input_size, hidden_size = hidden_size, sequence_length = sequence_length,
                  num_layers = num_layers, device=device).to(device) # GPU 연산을 위한 to(device)
criterion = L1Loss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3,)
153/147:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,target = dt
        
        out = model(seq)
    
        loss = criterion(out,target)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
    
    loss_graph.append(running_loss/n)
    print('mae',running_loss/n)
end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았음
153/148:
new_train = X[-60:]
len(new_train)
153/149: new_train[1:]
153/150: len(new_train)
153/151:
out_list=[]

with torch.no_grad():
    model.eval()
    new_tr = torch.FloatTensor(new_train).unsqueeze(0).to(device)
    out = model(new_tr)
    # out_list.append(out)

    #       new_train = torch.FloatTensor(new_train[1:].tolist()+out.cpu().tolist())#.view(-1,1).unsqueeze(0) data;pader를 안쓰고도 가능
    #       print(new_train)
153/152:
predictions = np.array(out.cpu().view(1,-1))
predictions
153/153:
fig,axes = plt.subplots(1,1,figsize=(14,8))
axes.plot(np.arange(pred_len),data[['volumn']].values[-split+pred_len:-split+pred_len+pred_len])
axes.plot(np.arange(pred_len),predictions[0])
153/154:
class VanillaRNN(nn.Module):
    
    def __init__(self,input_size,hidden_size,sequence_length,num_layers,device):
        super(VanillaRNN,self).__init__()
        self.device = device
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.rnn = nn.RNN(input_size,hidden_size,num_layers,batch_first = True)
        # input_size는 feature 개수를 의미함
        # hidden_size는 hidden_state의 feature 개수를 의미함 즉 hidden_vector의 크기
        self.fc = nn.Sequential(nn.Linear(hidden_size,1))
   
    def forward(self,x):
        h0 = torch.zeros(self.num_layers,x.size()[0],self.hidden_size).to(self.device)
        out,h_n = self.rnn(x,h0) #out 크기 (batchsize,length,feature size)
        # out크기를 보니 여기서 rnn은 many to many 즉 전체 입력에 대해 전체 출력이 나온다 
        # out = out.reshape(out.shape[0],-1)
        out = self.fc(out) 
        out = torch.index_select(out.cpu(),dim=1,index=torch.LongTensor([i for i in range(sequence_length-pred_len,sequence_length)])) #파이토치 indexing을 구체적으로 하는방법
        out = out.to(device)
        return out
153/155: from torch.nn import L1Loss
153/156:
model = VanillaRNN(input_size = input_size, hidden_size = hidden_size, sequence_length = sequence_length,
                  num_layers = num_layers, device=device).to(device) # GPU 연산을 위한 to(device)
criterion = L1Loss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3,)
153/157:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,target = dt
        
        out = model(seq)
    
        loss = criterion(out,target)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
    
    loss_graph.append(running_loss/n)
    print('mae',running_loss/n)
end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았음
153/158:
new_train = Y[-(sequence_length+pred_len):-(pred_le n)]
len(new_train)
153/159:
new_train = Y[-(sequence_length+pred_len):-(pred_len)]
len(new_train)
153/160: sequence_length+pred_len
153/161:
def seq_data(x,y,sequence_length,pred_len):
    x_seq = []
    target = []
    for i in range(len(x)-sequence_length):
        x_seq.append(x[i:i+sequence_length])
        target.append(y[i+sequence_length:i+sequence_length+pred_len])
    return torch.FloatTensor(x_seq).to(device), torch.FloatTensor(target).view(-1,pred_len,1).to(device)
153/162:
split = 1800
sequence_length = 1440
hidden_size = 8
pred_len = 360
X = data[['volumn']].values[:-split]
Y = data['volumn'].values
x_seq, target = seq_data(x=X,y=Y,sequence_length=sequence_length,pred_len=pred_len)
153/163: x_seq
153/164: target
153/165:
batch_size = 64
train = TensorDataset(x_seq,target)
train_loader = DataLoader(dataset = train,batch_size = batch_size)
# shuffle 제거
153/166:
input_size = x_seq.size(2)
num_layers = 2
hidden_size = 8 #batch_first = True로 해두면 (batch_size X sequence_length X hidden_size)로 나오게 된다
153/167:
class VanillaRNN(nn.Module):
    
    def __init__(self,input_size,hidden_size,sequence_length,num_layers,device):
        super(VanillaRNN,self).__init__()
        self.device = device
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.rnn = nn.RNN(input_size,hidden_size,num_layers,batch_first = True)
        # input_size는 feature 개수를 의미함
        # hidden_size는 hidden_state의 feature 개수를 의미함 즉 hidden_vector의 크기
        self.fc = nn.Sequential(nn.Linear(hidden_size,1))
   
    def forward(self,x):
        h0 = torch.zeros(self.num_layers,x.size()[0],self.hidden_size).to(self.device)
        out,h_n = self.rnn(x,h0) #out 크기 (batchsize,length,feature size)
        # out크기를 보니 여기서 rnn은 many to many 즉 전체 입력에 대해 전체 출력이 나온다 
        # out = out.reshape(out.shape[0],-1)
        out = self.fc(out) 
        out = torch.index_select(out.cpu(),dim=1,index=torch.LongTensor([i for i in range(sequence_length-pred_len,sequence_length)])) #파이토치 indexing을 구체적으로 하는방법
        out = out.to(device)
        return out
153/168: from torch.nn import L1Loss
153/169:
model = VanillaRNN(input_size = input_size, hidden_size = hidden_size, sequence_length = sequence_length,
                  num_layers = num_layers, device=device).to(device) # GPU 연산을 위한 to(device)
criterion = L1Loss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3,)
153/170:
import time
count = 0

loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,target = dt
        
        out = model(seq)
    
        loss = criterion(out,target)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
    
    loss_graph.append(running_loss/n)
    print('mae',running_loss/n)
end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았음
153/171: sequence_length+pred_len
153/172:
new_train = Y[-(sequence_length+pred_len):-(pred_len)]
len(new_train)
153/173: new_train[1:]
153/174: len(new_train)
153/175:
out_list=[]

with torch.no_grad():
    model.eval()
    new_tr = torch.FloatTensor(new_train).unsqueeze(0).to(device)
    out = model(new_tr)
    # out_list.append(out)

    #       new_train = torch.FloatTensor(new_train[1:].tolist()+out.cpu().tolist())#.view(-1,1).unsqueeze(0) data;pader를 안쓰고도 가능
    #       print(new_train)
153/176:
out_list=[]

with torch.no_grad():
    model.eval()
    new_tr = torch.FloatTensor(new_train).to(device)
    out = model(new_tr)
    # out_list.append(out)

    #       new_train = torch.FloatTensor(new_train[1:].tolist()+out.cpu().tolist())#.view(-1,1).unsqueeze(0) data;pader를 안쓰고도 가능
    #       print(new_train)
153/177:
new_train = Y[-(sequence_length+pred_len):-(pred_len)]
new_train
153/178:
out_list=[]

with torch.no_grad():
    model.eval()
    new_tr = torch.FloatTensor(new_train).unsqueeze(0).to(device)
    out = model(new_tr)
    # out_list.append(out)

    #       new_train = torch.FloatTensor(new_train[1:].tolist()+out.cpu().tolist())#.view(-1,1).unsqueeze(0) data;pader를 안쓰고도 가능
    #       print(new_train)
153/179:
out_list=[]

with torch.no_grad():
    model.eval()
    new_tr = torch.FloatTensor([new_train]).unsqueeze(0).to(device)
    out = model(new_tr)
    # out_list.append(out)

    #       new_train = torch.FloatTensor(new_train[1:].tolist()+out.cpu().tolist())#.view(-1,1).unsqueeze(0) data;pader를 안쓰고도 가능
    #       print(new_train)
153/180: torch.FloatTensor(new_train)
153/181: torch.FloatTensor([new_train])
153/182: torch.FloatTensor(new_train).unsqueeze(0)|
153/183: torch.FloatTensor(new_train).unsqueeze(0)
153/184: torch.FloatTensor(new_train).unsqueeze(1)
153/185:
out_list=[]

with torch.no_grad():
    model.eval()
    new_tr = torch.FloatTensor(new_train).unsqueeze(1).to(device)
    out = model(new_tr)
    # out_list.append(out)

    #       new_train = torch.FloatTensor(new_train[1:].tolist()+out.cpu().tolist())#.view(-1,1).unsqueeze(0) data;pader를 안쓰고도 가능
    #       print(new_train)
153/186: torch.FloatTensor([new_train]).unsqueeze(1)
153/187: torch.FloatTensor([new_train]).unsqueeze(0)
153/188: torch.FloatTensor([new_train]).unsqueeze(2)
153/189:
out_list=[]

with torch.no_grad():
    model.eval()
    new_tr = torch.FloatTensor([new_train]).unsqueeze(2).to(device)
    out = model(new_tr)
    # out_list.append(out)

    #       new_train = torch.FloatTensor(new_train[1:].tolist()+out.cpu().tolist())#.view(-1,1).unsqueeze(0) data;pader를 안쓰고도 가능
    #       print(new_train)
153/190:
predictions = np.array(out.cpu().view(1,-1))
predictions
153/191:
fig,axes = plt.subplots(1,1,figsize=(14,8))
axes.plot(np.arange(pred_len),data[['volumn']].values[-split+pred_len:-split+pred_len+pred_len])
axes.plot(np.arange(pred_len),predictions[0])
153/192:
fig,axes = plt.subplots(1,1,figsize=(14,8))
axes.plot(np.arange(pred_len),data[['volumn']].values[-split+pred_len:-split+pred_len+pred_len],label='true')
axes.plot(np.arange(pred_len),predictions[0],'pred')
axes.legend()
153/193:
fig,axes = plt.subplots(1,1,figsize=(14,8))
axes.plot(np.arange(pred_len),data[['volumn']].values[-split+pred_len:-split+pred_len+pred_len],label='true')
axes.plot(np.arange(pred_len),predictions[0],'pred')
plt.legend()
153/194:
fig,axes = plt.subplots(1,1,figsize=(14,8))
axes.plot(np.arange(pred_len),data[['volumn']].values[-split+pred_len:-split+pred_len+pred_len],label='true')
axes.plot(np.arange(pred_len),predictions[0],label='pred')
plt.legend()
158/1: import configparser
158/2: config = configparser.ConfigParser()
158/3:
config['DLinear_cfg2'] = {
    'seq_len': 1800,
    'pred_len':360,
    'enc_in':1,
    'individual':False
}
158/4:
with open('DLinear_cfg2', 'w') as f:
      config.write(f)
159/1:
import os
import configparser

config = configparser.ConfigParser()
print("work dir",os.getcwd())
159/2:
import os
import configparser

config = configparser.ConfigParser()
print("work dir",os.getcwd())
print("separator",os.sep)
159/3:
import os
import configparser

config = configparser.ConfigParser()
print("work dir",os.getcwd())
print("separator",os.sep)

config.read(os.getcwd()+os.sep+'DLinearConfig',encoding = 'utf-8')
159/4:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = configs.seq_len # configure의 sequence length
        self.pred_len = configs.pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs.individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = configs.enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
159/5:
import torch
import torch.nn as nn
import torch.nn.functional as F
# 이 모듈에는 torch.nn 라이브러리의 모든 함수가 포함되어 있다 다양한 손실 및
# 활성화 함수 뿐만 아니라, 풀링(pooling) 함수와 같이 신경망을 만드는데 편리한 몇 가지
# 함수도 여기에서 찾을 수 있다.
import numpy as np
import torch.optim as optim
import matplotlib.pyplot as plt
from torch.utils.data import TensorDataset,DataLoader
from torch import FloatTensor
from sklearn.preprocessing import MinMaxScaler
159/6:
import torch
import torch.nn as nn
import torch.nn.functional as F
# 이 모듈에는 torch.nn 라이브러리의 모든 함수가 포함되어 있다 다양한 손실 및
# 활성화 함수 뿐만 아니라, 풀링(pooling) 함수와 같이 신경망을 만드는데 편리한 몇 가지
# 함수도 여기에서 찾을 수 있다.
import numpy as np
import torch.optim as optim
import matplotlib.pyplot as plt
from torch.utils.data import TensorDataset,DataLoader
from torch import FloatTensor
from sklearn.preprocessing import MinMaxScaler
159/7:
# Pytorch로 설게하는 신경망은 반드시 따라야 하는 구조가 있다

'''
1. torch.nn.Module을 상속해야 한다
2. __init()__과 forward()를 override 해야한다.
    (1) override: torch.nn.Module(부모클래스)에서 정의한 메소드를 override 해야한다
    (2) __init()__ 에서는 모델에서 사용될 module(nn.Linear, nn.Conv2d), activation function
    (nn.functional.relu, nn.functional.sigmoid)등을 정의한다
    (3) forward()에서는 모델에서 실행되어야하는 계산을 정의한다. backward 계산은 backward()를
    이용시에 Pytorch가 알아서 해주니깐 forward()만을 정의해주면 된다
    input을 넣어서 어떤 계산을 진행해서 output이 나올지를 정의해준다고 이해하면 된다
'''
class moving_avg(nn.Module):
    "M.A.는 시계열의 trend에 집중하는 것을 막는다"
    def __init__(self,kernel_size,stride):
        super(moving_avg,self).__init__()
        # super(Model_Name,self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size = kernel_size, stride = stride, padding =0)
        # pooling 계산을 하는 1차원 평균으로 pooling계산
    
    def forward(self,x):
        " time series의 양 쪽 끝을 padding 한다"
        print(x[:,0:1])
        front = x[:,0:1,:].repeat(1,(self.kernel_size-1)//2,1)
        # Repeats this tensor along the specified dimensions.
        #print("front",front)
        
        end = x[:,-1,:].repeat(1,(self.kernel_size-1)//2,1)
        #print("end",end)
        
# front와 end를 추가해서 moving average를 계산해서 공백이 없도록
        
        x = torch.cat([front, x, end], dim=1)
        #print("cat",x,x.size())
        
        #print('before avg',x.permute(0,2,1))
        x = self.avg(x.permute(0,2,1)) #pooling 계산
        #print(x)
        
        x = x.permute(0,2,1) # 원상복귀
        #print(x)
        return x
159/8:
x=torch.FloatTensor([2,1,3,4]).view(1,-1,1) #4개의 데이터를 가진 것을 표현
print(x)
print('-----------------')
ma = moving_avg(3,1) 
x = ma.forward(x)
print(x)
159/9:
class series_decomp(nn.Module):
    "statsmodels.tsa의 seasonal_decompose와 역할이 똑같다"
    def __init__(self,kernel_size):
        super(series_decomp,self).__init__()
        self.moving_avg = moving_avg(kernel_size,stride = 1)
    
    def forward(self,x):
        moving_mean = self.moving_avg(x) #output은 (batch,sequence_length,input_size)
        res = x - moving_mean 
        # Classical ma중에서 additive model
        print(res.permute(1,2,0).size())
        print('----')
        print(moving_mean.permute(1,2,0).size())
        return res.permute(1,2,0),moving_mean.permute(1,2,0)
159/10:
x = torch.FloatTensor([1,2,3,4]).view(1,-1,1)
decomp = series_decomp(3) 
# kernel_size를 짝수로하면 오류 발생 (size가 맞지 않은 오류)
# 이경우는 짝수 ma를 두번진행하면 된다(아래 예시)
a,b=decomp.forward(x)
159/11:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = configs.seq_len # configure의 sequence length
        self.pred_len = configs.pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs.individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = configs.enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
159/12:
import torch
import torch.nn as nn
import torch.nn.functional as F
# 이 모듈에는 torch.nn 라이브러리의 모든 함수가 포함되어 있다 다양한 손실 및
# 활성화 함수 뿐만 아니라, 풀링(pooling) 함수와 같이 신경망을 만드는데 편리한 몇 가지
# 함수도 여기에서 찾을 수 있다.
import numpy as np
import pandas as pd
159/13:
import torch
import torch.nn as nn
import torch.nn.functional as F
# 이 모듈에는 torch.nn 라이브러리의 모든 함수가 포함되어 있다 다양한 손실 및
# 활성화 함수 뿐만 아니라, 풀링(pooling) 함수와 같이 신경망을 만드는데 편리한 몇 가지
# 함수도 여기에서 찾을 수 있다.
import numpy as np
import pandas as pd
159/14:
data = pd.read_csv('../traffic/data/5.csv')
data
159/15: data.drop(columns=['service_name','packets','unknown'])
159/16:
scaler = MinMaxScaler()
data[['volumn']] = scaler.fit_transform(data[['volumn']])
data
159/17: data['volumn'].plot(figsize=(14,8))
159/18:
def seq_data(x,y,sequence_length,pred_len):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length:i+sequence_length+pred_len].values)
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length:i+sequence_length+pred_len].values)
#             if i == 0:
#                  print(x.iloc[i:i+sequence_length].values)
#                  print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,pred_len).to(device)
159/19:
split = 1440 # split까지 학습을 하게 되면 split-pred_len까지의 정답은 알려준셈이 된다
sequence_length = 1800
pred_len = 720
individual = True
enc_in = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
Y = data['volumn']
x_seq, target = seq_data(x=X,y=Y,sequence_length=sequence_length,pred_len=pred_len)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
159/20:
device = torch.device("cuda:0")
torch_data = TensorDataset(x_seq,target)
159/21:
batch_size = 32
train_loader = DataLoader(torch_data,batch_size = batch_size)
159/22:
count =1
for i,j in train_loader:
    print(i.cpu().size())
    print(j.cpu().size())
    count+=1
print(count)
159/23:
model = Model(config).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
# Q) 왜 learning_rate를 조정하면 MSE값의 차이가 크게나지
159/24:
import os
import configparser

config = configparser.ConfigParser()
print("work dir",os.getcwd())
print("separator",os.sep)

config.read(os.getcwd()+os.sep+'DLinearConfig.ini',encoding = 'utf-8')
159/25:
import os
import configparser

config = configparser.ConfigParser()
print("work dir",os.getcwd())
print("separator",os.sep)

config.read(os.getcwd()+os.sep+'DLinear_cfg1.ini',encoding = 'utf-8')
159/26:
import os
import configparser

config = configparser.ConfigParser()
print("work dir",os.getcwd())
print("separator",os.sep)

config.read(os.getcwd()+os.sep+'DLinear_cfg1',encoding = 'utf-8')
159/27:
model = Model(config).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
# Q) 왜 learning_rate를 조정하면 MSE값의 차이가 크게나지
158/5:
with open('DLinear_cfg1', 'w') as f:
      config.write(f)
158/6: import configparser
158/7: config = configparser.ConfigParser()
158/8:
config['DLinear_cfg1'] = {
    'seq_len': 1800,
    'pred_len':360,
    'enc_in':1,
    'individual':False
}
158/9:
with open('DLinear_cfg1', 'w') as f:
      config.write(f)
159/28:
import os
import configparser

config = configparser.ConfigParser()
print("work dir",os.getcwd())
print("separator",os.sep)

config.read(os.getcwd()+os.sep+'DLinear_cfg1',encoding = 'utf-8')
159/29:
model = Model(config).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
# Q) 왜 learning_rate를 조정하면 MSE값의 차이가 크게나지
159/30:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = configs['seq_len'] # configure의 sequence length
        self.pred_len = configs.pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs.individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = configs.enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
159/31:
model = Model(config).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
# Q) 왜 learning_rate를 조정하면 MSE값의 차이가 크게나지
159/32:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = configs['DLinear_cfg1']['seq_len'] # configure의 sequence length
        self.pred_len = configs.pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs.individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = configs.enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
159/33:
model = Model(config).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
# Q) 왜 learning_rate를 조정하면 MSE값의 차이가 크게나지
159/34:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = configs['DLinear_cfg1']['seq_len'] # configure의 sequence length
        self.pred_len = configs['DLinear_cfg1']['pred_len'] # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs['DLinear_cfg1']['individual'] # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = configs['DLinear_cfg1']['enc_in'] # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
159/35:
model = Model(config).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
# Q) 왜 learning_rate를 조정하면 MSE값의 차이가 크게나지
159/36:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = configs['DLinear_cfg1']['seq_len'] # configure의 sequence length
        self.pred_len = configs['DLinear_cfg1']['pred_len'] # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs['DLinear_cfg1']['individual'] # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['DLinear_cfg1']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
159/37:
model = Model(config).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
# Q) 왜 learning_rate를 조정하면 MSE값의 차이가 크게나지
158/10:
config['DLinear_cfg1'] = {
    'seq_len': 1800,
    'pred_len':int(360),
    'enc_in':1,
    'individual':False
}
158/11:
with open('DLinear_cfg1', 'w') as f:
      config.write(f)
159/38:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['DLinear_cfg1']['seq_len']) # configure의 sequence length
        self.pred_len = configs['DLinear_cfg1']['pred_len'] # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = bool(configs['DLinear_cfg1']['individual']) # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['DLinear_cfg1']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
159/39:
model = Model(config).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
# Q) 왜 learning_rate를 조정하면 MSE값의 차이가 크게나지
159/40:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['DLinear_cfg1']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['DLinear_cfg1']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = bool(configs['DLinear_cfg1']['individual']) # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['DLinear_cfg1']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
159/41:
model = Model(config).to(device)
criterion = nn.MSELoss()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
# Q) 왜 learning_rate를 조정하면 MSE값의 차이가 크게나지
159/42:
import os
import configparser

config = configparser.ConfigParser()
print("work dir",os.getcwd())
print("separator",os.sep)

config.read(os.getcwd()+os.sep+'DLinear_cfg1',encoding = 'utf-8')
#configuration에 있는 속성을 사용할때 ['configfilename']['원하는 속성'] 하고 전체에 내가 받고 싶은 속성의 자료형을 씌워줘야한다
161/1:
import torch
import torch.nn as nn
import torch.nn.functional as F
# 이 모듈에는 torch.nn 라이브러리의 모든 함수가 포함되어 있다 다양한 손실 및
# 활성화 함수 뿐만 아니라, 풀링(pooling) 함수와 같이 신경망을 만드는데 편리한 몇 가지
# 함수도 여기에서 찾을 수 있다.
import numpy as np
import torch.optim as optim
import matplotlib.pyplot as plt
from torch.utils.data import TensorDataset,DataLoader
from torch import FloatTensor
from sklearn.preprocessing import MinMaxScaler
161/2:
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    """
    Decomposition-Linear
    """
    def __init__(self,seq_len,pred_len,individual,enc_in): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = seq_len # configure의 sequence length
        self.pred_len = pred_len # configure의 prediction length
        
        # Decomposition Kernel Size
        kernel_size = 5 #커널사이즈를 5로 변경
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = individual # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = enc_in # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
        

    def forward(self, x):
        # x: [Batch, Input length, Channel]
        seasonal_init, trend_init = self.decomposition(x)# res = seasonality 를 moving_mean은 trend를 의미
        # seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
        seasonal_init, trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1)
        if self.individual:
            seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],dtype=seasonal_init.dtype).to(seasonal_init.device)
             # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
            trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len],dtype=trend_init.dtype).to(trend_init.device)
            for i in range(self.channels):
                seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
                # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                # 이때 RNN과 달리 한번에 학습을 진행하기 때문에 seq의 배열 자체가 feature가 됨
        else:
            seasonal_output = self.Linear_Seasonal(seasonal_init)
            trend_output = self.Linear_Trend(trend_init)

        x = seasonal_output + trend_output
        return x.permute(0,2,1) # [batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)]로 변경해서 return
161/3:
data = pd.read_csv('../traffic/data/5.csv')
data
161/4:
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
import torch.optim as optim
import matplotlib.pyplot as plt
from torch.utils.data import TensorDataset,DataLoader
from torch import FloatTensor
from sklearn.preprocessing import MinMaxScaler
161/5:
data = pd.read_csv('../traffic/data/5.csv')
data
161/6: data.drop(columns=['service_name','packets','unknown'])
161/7:
def seq_data(x,y,sequence_length,pred_len):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length].values)
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length].values)
#             if i == 0:
#                  print(x.iloc[i:i+sequence_length].values)
#                  print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,pred_len).to(device)
161/8:
class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['DLinear_cfg1']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['DLinear_cfg1']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = bool(configs['DLinear_cfg1']['individual']) # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['DLinear_cfg1']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
163/1: import configparser
163/2: config = configparser.ConfigParser()
163/3:
config['1step_DLinear'] = {
    'seq_len':1440,
    'pred_len':1,
    'enc_in':1,
    'individual':False
}
163/4:
with open('1step_DLinear', 'w') as f:
      config.write(f)
161/9:
import os
import configparser

config = configparser.ConfigParser()
print("work dir",os.getcwd())
print("separator",os.sep)

config.read(os.getcwd()+os.sep+'1step_DLinear_config',encoding = 'utf-8')
161/10:
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['1step_DLinear_config']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['1step_DLinear_config']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = bool(configs['1step_DLinear_config']['individual']) # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['1step_DLinear_config']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
161/11:
split=1440
sequence_length = 1800
pred_len = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
Y = data['volumn'] # 정답 데이터
x_seq, target = seq_data(x=X,y=Y,sequence_length=sequence_length,pred_len=pred_len)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
161/12:
def seq_data(x,y,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length])
#             if i == 0:
#                  print(x.iloc[i:i+sequence_length].values)
#                  print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,pred_len).to(device)
161/13:
import os
import configparser

config = configparser.ConfigParser()
print("work dir",os.getcwd())
print("separator",os.sep)

config.read(os.getcwd()+os.sep+'1step_DLinear_config',encoding = 'utf-8')
161/14:
split=1440
sequence_length = 1800
pred_len = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
Y = data['volumn'] # 정답 데이터
x_seq, target = seq_data(x=X,y=Y,sequence_length=sequence_length,pred_len=pred_len)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
161/15:
split=1440
sequence_length = 1800
pred_len = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
Y = data['volumn'] # 정답 데이터
x_seq, target = seq_data(x=X,y=Y,sequence_length=sequence_length)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
161/16: torch_data = TensorDataset(x_seq,target)
161/17:
batch_size = 32
train_loader = DataLoader(torch_data,batch_size = batch_size)
161/18:
count =1
for i,j in train_loader:
    print(i.cpu().size())
    print(j.cpu().size())
    count+=1
print(count)
161/19: from pytorch_forecasting.metrics import SMAPE
161/20:
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
# Q) 왜 learning_rate를 조정하면 MSE값의 차이가 크게나지
163/5:
config['1step_DLinear'] = {
    'seq_len':1440,
    'pred_len':1,
    'enc_in':1,
    'individual':False
}
163/6:
with open('1step_DLinear_config', 'w') as f:
      config.write(f)
161/21:
import os
import configparser

config = configparser.ConfigParser()
print("work dir",os.getcwd())
print("separator",os.sep)

config.read(os.getcwd()+os.sep+'1step_DLinear_config',encoding = 'utf-8')
161/22:
split=1440
sequence_length = 1800
pred_len = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
Y = data['volumn'] # 정답 데이터
x_seq, target = seq_data(x=X,y=Y,sequence_length=sequence_length)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
161/23:
batch_size = 32
train_loader = DataLoader(torch_data,batch_size = batch_size)
161/24: from pytorch_forecasting.metrics import SMAPE
161/25: torch_data = TensorDataset(x_seq,target)
161/26:
batch_size = 32
train_loader = DataLoader(torch_data,batch_size = batch_size)
161/27: from pytorch_forecasting.metrics import SMAPE
161/28:
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
# Q) 왜 learning_rate를 조정하면 MSE값의 차이가 크게나지
161/29:
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
# Q) 왜 learning_rate를 조정하면 MSE값의 차이가 크게나지
161/30: from pytorch_forecasting.metrics import SMAPE
161/31:
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
# Q) 왜 learning_rate를 조정하면 MSE값의 차이가 크게나지
163/7: import configparser
163/8: config = configparser.ConfigParser()
163/9:
config['1step_DLinear'] = {
    'seq_len':1440,
    'pred_len':1,
    'enc_in':1,
    'individual':False
}
163/10:
with open('1step_DLinear_config', 'w') as f:
      config.write(f)
161/32:
import os
import configparser

config = configparser.ConfigParser()
print("work dir",os.getcwd())
print("separator",os.sep)

config.read(os.getcwd()+os.sep+'1step_DLinear_config',encoding = 'utf-8')
161/33:
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
# Q) 왜 learning_rate를 조정하면 MSE값의 차이가 크게나지
163/11: import configparser
163/12: config = configparser.ConfigParser()
163/13:
config['1step_DLinear_config'] = {
    'seq_len':1440,
    'pred_len':1,
    'enc_in':1,
    'individual':False
}
163/14:
with open('1step_DLinear_config', 'w') as f:
      config.write(f)
161/34:
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
# Q) 왜 learning_rate를 조정하면 MSE값의 차이가 크게나지
161/35:
import os
import configparser

config = configparser.ConfigParser()
print("work dir",os.getcwd())
print("separator",os.sep)

config.read(os.getcwd()+os.sep+'1step_DLinear_config',encoding = 'utf-8')
161/36:
split=1440
sequence_length = 1800
pred_len = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
Y = data['volumn'] # 정답 데이터
x_seq, target = seq_data(x=X,y=Y,sequence_length=sequence_length)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
161/37: torch_data = TensorDataset(x_seq,target)
161/38:
batch_size = 32
train_loader = DataLoader(torch_data,batch_size = batch_size)
161/39: from pytorch_forecasting.metrics import SMAPE
161/40:
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
# Q) 왜 learning_rate를 조정하면 MSE값의 차이가 크게나지
161/41:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
161/42:
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['1step_DLinear_config']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['1step_DLinear_config']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = bool(configs['1step_DLinear_config']['individual']) # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['1step_DLinear_config']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                print(1)
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
161/43:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
161/44:
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['1step_DLinear_config']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['1step_DLinear_config']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = bool(configs['1step_DLinear_config']['individual']) # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['1step_DLinear_config']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                print(1)
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
161/45:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
161/46:
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
161/47:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
161/48:
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['1step_DLinear_config']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['1step_DLinear_config']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = bool(configs['1step_DLinear_config']['individual']) # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['1step_DLinear_config']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                print(1)
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
161/49:
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
161/50:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
161/51:
import os
import configparser

config = configparser.ConfigParser()
print("work dir",os.getcwd())
print("separator",os.sep)

config.read(os.getcwd()+os.sep+'1step_DLinear_config',encoding = 'utf-8')
161/52:
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['1step_DLinear_config']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['1step_DLinear_config']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = bool(configs['1step_DLinear_config']['individual']) # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['1step_DLinear_config']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                print(1)
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
161/53:
def seq_data(x,y,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length])
#             if i == 0:
#                  print(x.iloc[i:i+sequence_length].values)
#                  print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,pred_len).to(device)
161/54:
import os
import configparser

config = configparser.ConfigParser()
print("work dir",os.getcwd())
print("separator",os.sep)

config.read(os.getcwd()+os.sep+'1step_DLinear_config',encoding = 'utf-8')
161/55:
import os
import configparser

config = configparser.ConfigParser()
print("work dir",os.getcwd())
print("separator",os.sep)

config.read(os.getcwd()+os.sep+'1step_DLinear_config',encoding = 'utf-8')
161/56:
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['1step_DLinear_config']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['1step_DLinear_config']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = bool(configs['1step_DLinear_config']['individual']) # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['1step_DLinear_config']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                print(1)
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
161/57:
def seq_data(x,y,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length])
#             if i == 0:
#                  print(x.iloc[i:i+sequence_length].values)
#                  print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,pred_len).to(device)
161/58:
split=1440
sequence_length = 1800
pred_len = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
Y = data['volumn'] # 정답 데이터
x_seq, target = seq_data(x=X,y=Y,sequence_length=sequence_length)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
161/59: torch_data = TensorDataset(x_seq,target)
161/60:
batch_size = 32
train_loader = DataLoader(torch_data,batch_size = batch_size)
161/61: from pytorch_forecasting.metrics import SMAPE
161/62:
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
161/63:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
161/64:
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['1step_DLinear_config']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['1step_DLinear_config']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = bool(configs['1step_DLinear_config']['individual']) # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['1step_DLinear_config']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
        print(self.individual)
            if self.individual:
                
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
161/66:
def seq_data(x,y,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length])
#             if i == 0:
#                  print(x.iloc[i:i+sequence_length].values)
#                  print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,pred_len).to(device)
161/67:
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['1step_DLinear_config']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['1step_DLinear_config']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = bool(configs['1step_DLinear_config']['individual']) # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['1step_DLinear_config']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            print(self.individual)
            if self.individual:
                
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
161/68:
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
161/69:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
163/15: import configparser
163/16: config = configparser.ConfigParser()
163/17:
config['1step_DLinear_config'] = {
    'seq_len':1440,
    'pred_len':1,
    'enc_in':1,
    'individual': bool(False)
}
163/18:
with open('1step_DLinear_config', 'w') as f:
      config.write(f)
161/70:
import os
import configparser

config = configparser.ConfigParser()
print("work dir",os.getcwd())
print("separator",os.sep)

config.read(os.getcwd()+os.sep+'1step_DLinear_config',encoding = 'utf-8')
161/71:
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['1step_DLinear_config']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['1step_DLinear_config']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = bool(configs['1step_DLinear_config']['individual']) # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['1step_DLinear_config']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            print(self.individual)
            if self.individual:
                
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
161/72: config['1step_DLinear_config']['individual']
161/73:
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['1step_DLinear_config']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['1step_DLinear_config']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = bool(configs['1step_DLinear_config']['individual']) # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['1step_DLinear_config']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            print(self.individual)
            if self.individual:
                
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
161/74:
def seq_data(x,y,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length])
#             if i == 0:
#                  print(x.iloc[i:i+sequence_length].values)
#                  print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,pred_len).to(device)
161/75:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
161/76:
import os
import configparser

config = configparser.ConfigParser()
print("work dir",os.getcwd())
print("separator",os.sep)

config=config.read(os.getcwd()+os.sep+'1step_DLinear_config',encoding = 'utf-8')
161/77:
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['1step_DLinear_config']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['1step_DLinear_config']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = bool(configs['1step_DLinear_config']['individual']) # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['1step_DLinear_config']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            print(self.individual)
            if self.individual:
                
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
161/78:
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
161/79:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
161/80:
import os
import configparser

config = configparser.ConfigParser()
print("work dir",os.getcwd())
print("separator",os.sep)

config.read(os.getcwd()+os.sep+'1step_DLinear_config',encoding = 'utf-8')
161/81:
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['1step_DLinear_config']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['1step_DLinear_config']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs['1step_DLinear_config']['individual'] # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['1step_DLinear_config']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            print(self.individual)
            if self.individual:
                
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
161/82:
def seq_data(x,y,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length])
#             if i == 0:
#                  print(x.iloc[i:i+sequence_length].values)
#                  print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,pred_len).to(device)
161/83:
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
161/84:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
161/85:
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['1step_DLinear_config']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['1step_DLinear_config']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs['1step_DLinear_config']['individual'] # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['1step_DLinear_config']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                print(seasonal_init)
                print(trend_init)
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
161/86:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
161/87:
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['1step_DLinear_config']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['1step_DLinear_config']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs['1step_DLinear_config']['individual'] # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['1step_DLinear_config']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                print(seasonal_init)
                print(trend_init)
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
161/88:
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
161/89:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
161/90:
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['1step_DLinear_config']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['1step_DLinear_config']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs['1step_DLinear_config']['individual'] # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['1step_DLinear_config']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            print(self.individual)
            if self.individual:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                print(seasonal_init)
                print(trend_init)
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
161/91:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
161/92:
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
161/93:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
161/94:
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['1step_DLinear_config']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['1step_DLinear_config']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs['1step_DLinear_config']['individual'] # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['1step_DLinear_config']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            #print(self.individual)
            if self.individual==True:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                print(seasonal_init)
                print(trend_init)
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
161/95:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
161/96:
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
161/97:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
161/98:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        print(seq)
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
161/99:
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        #x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['1step_DLinear_config']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['1step_DLinear_config']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs['1step_DLinear_config']['individual'] # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['1step_DLinear_config']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            #print(self.individual)
            if self.individual==True:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                print(seasonal_init)
                print(trend_init)
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
161/100:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq)
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
161/101:
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
161/102:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq)
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
161/103:
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        #x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['1step_DLinear_config']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['1step_DLinear_config']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs['1step_DLinear_config']['individual'] # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['1step_DLinear_config']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual==True: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            #print(self.individual)
            if self.individual==True:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
#                 print(seasonal_init)
#                 print(trend_init)
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
161/104:
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
161/105:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq)
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
161/106:
split=1440
sequence_length = 1440
pred_len = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
Y = data['volumn'] # 정답 데이터
x_seq, target = seq_data(x=X,y=Y,sequence_length=sequence_length)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
161/107:
split=1440
sequence_length = 1440
pred_len = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
Y = data['volumn'] # 정답 데이터
x_seq, target = seq_data(x=X,y=Y,sequence_length=sequence_length)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
164/1:
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
import torch.optim as optim
import matplotlib.pyplot as plt
from torch.utils.data import TensorDataset,DataLoader
from torch import FloatTensor
from sklearn.preprocessing import MinMaxScaler
164/2:
data = pd.read_csv('../traffic/data/5.csv')
data
164/3: data.drop(columns=['service_name','packets','unknown'])
164/4:
import os
import configparser

config = configparser.ConfigParser()
print("work dir",os.getcwd())
print("separator",os.sep)

config.read(os.getcwd()+os.sep+'1step_DLinear_config',encoding = 'utf-8')
164/5:
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        #x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['1step_DLinear_config']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['1step_DLinear_config']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs['1step_DLinear_config']['individual'] # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['1step_DLinear_config']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual==True: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            #print(self.individual)
            if self.individual==True:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
#                 print(seasonal_init)
#                 print(trend_init)
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
164/6:
def seq_data(x,y,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length])
#             if i == 0:
#                  print(x.iloc[i:i+sequence_length].values)
#                  print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,pred_len).to(device)
164/7:
split=1440
sequence_length = 1440
pred_len = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
Y = data['volumn'] # 정답 데이터
x_seq, target = seq_data(x=X,y=Y,sequence_length=sequence_length)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
164/8: torch_data = TensorDataset(x_seq,target)
164/9:
batch_size = 32
train_loader = DataLoader(torch_data,batch_size = batch_size)
164/10: from pytorch_forecasting.metrics import SMAPE
164/11:
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
164/12:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq)
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
164/13:
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['1step_DLinear_config']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['1step_DLinear_config']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs['1step_DLinear_config']['individual'] # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['1step_DLinear_config']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual==True: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            #print(self.individual)
            if self.individual==True:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
#                 print(seasonal_init)
#                 print(trend_init)
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
164/14:
split=1440
sequence_length = 1440
pred_len = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
Y = data['volumn'] # 정답 데이터
x_seq, target = seq_data(x=X,y=Y,sequence_length=sequence_length)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
164/15:
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
164/16:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq)
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
164/17:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        print(seq)
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
164/18:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        print(seq.size())
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
164/19:
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['1step_DLinear_config']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['1step_DLinear_config']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs['1step_DLinear_config']['individual'] # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['1step_DLinear_config']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual==True: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            #print(self.individual)
            if self.individual==True:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
#                 print(seasonal_init)
#                 print(trend_init)
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
164/20:
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
164/21:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        print(seq.size())
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
164/22:
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['1step_DLinear_config']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['1step_DLinear_config']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs['1step_DLinear_config']['individual'] # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['1step_DLinear_config']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual==True: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            print(seasonal_init,trend_init)
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            #print(self.individual)
            if self.individual==True:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
#                 print(seasonal_init)
#                 print(trend_init)
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
164/23:
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
164/24:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        print(seq.size())
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
164/25:
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['1step_DLinear_config']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['1step_DLinear_config']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs['1step_DLinear_config']['individual'] # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['1step_DLinear_config']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual==True: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            print(seasonal_init.size(),trend_init.size())
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            #print(self.individual)
            if self.individual==True:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
#                 print(seasonal_init)
#                 print(trend_init)
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
164/26:
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
164/27:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        print(seq.size())
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
164/28:
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['1step_DLinear_config']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['1step_DLinear_config']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs['1step_DLinear_config']['individual'] # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['1step_DLinear_config']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual==True: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            print(seasonal_init.size(),trend_init.size()) # 32,1440,1
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            #print(self.individual)
            if self.individual==True:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                print(1)
#                 print(seasonal_init)
#                 print(trend_init)
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
164/29:
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
164/30:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        print(seq.size())
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
164/31:
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['1step_DLinear_config']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['1step_DLinear_config']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs['1step_DLinear_config']['individual'] # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['1step_DLinear_config']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual==True: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(32self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            print(seasonal_init.size(),trend_init.size()) # 32,1440,1
            #seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            #print(self.individual)
            if self.individual==True:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                print(1)
#                 print(seasonal_init)
#                 print(trend_init)
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
164/32:
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['1step_DLinear_config']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['1step_DLinear_config']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs['1step_DLinear_config']['individual'] # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['1step_DLinear_config']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual==True: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            print(seasonal_init.size(),trend_init.size()) # 32,1440,1
            #seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            #print(self.individual)
            if self.individual==True:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                print(1)
#                 print(seasonal_init)
#                 print(trend_init)
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
164/33:
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
164/34:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        print(seq.size())
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
164/35:
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['1step_DLinear_config']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['1step_DLinear_config']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs['1step_DLinear_config']['individual'] # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['1step_DLinear_config']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual==True: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            print(seasonal_init.size(),trend_init.size()) # 32,1440,1
            seasoanl_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
            print(seasonal_init.size(),trend_init.size())
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            #print(self.individual)
            if self.individual==True:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                print(1)
#                 print(seasonal_init)
#                 print(trend_init)
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
164/36:
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
164/37:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        print(seq.size())
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
164/38:
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['1step_DLinear_config']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['1step_DLinear_config']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs['1step_DLinear_config']['individual'] # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['1step_DLinear_config']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual==True: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            print(seasonal_init.size(),trend_init.size()) # 32,1440,1
            seasoanl_init = seasonal_init.permute(0,2,1)# 새로로 펼친걸 가로로 늘림
            trend_init =  trend_init.permute(0,2,1) 
            print(seasonal_init.size(),trend_init.size())
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            #print(self.individual)
            if self.individual==True:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                print(1)
#                 print(seasonal_init)
#                 print(trend_init)
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
164/39:
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
164/40:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        print(seq.size())
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
164/41:
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['1step_DLinear_config']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['1step_DLinear_config']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs['1step_DLinear_config']['individual'] # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['1step_DLinear_config']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual==True: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            print(seasonal_init.size(),trend_init.size()) # 32,1440,1
            seasoanl_init = seasonal_init.view(0,2,1)# 새로로 펼친걸 가로로 늘림
            trend_init =  trend_init.permute(0,2,1) 
            print(seasonal_init.size(),trend_init.size())
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            #print(self.individual)
            if self.individual==True:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                print(1)
#                 print(seasonal_init)
#                 print(trend_init)
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
164/42:
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
164/43:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        print(seq.size())
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
164/44:
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['1step_DLinear_config']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['1step_DLinear_config']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs['1step_DLinear_config']['individual'] # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['1step_DLinear_config']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual==True: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            print(seasonal_init.size(),trend_init.size()) # 32,1440,1
            seasoanl_init = seasonal_init.view(32,1,1440)# 새로로 펼친걸 가로로 늘림
            trend_init =  trend_init.permute(0,2,1) 
            print(seasonal_init.size(),trend_init.size())
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            #print(self.individual)
            if self.individual==True:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                print(1)
#                 print(seasonal_init)
#                 print(trend_init)
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
164/45:
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
164/46:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        print(seq.size())
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
164/47:
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['1step_DLinear_config']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['1step_DLinear_config']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs['1step_DLinear_config']['individual'] # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['1step_DLinear_config']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual==True: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            print(seasonal_init.size(),trend_init.size()) # 32,1440,1
            seasonal_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
            print(seasonal_init.size(),trend_init.size())
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            #print(self.individual)
            if self.individual==True:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                print(1)
#                 print(seasonal_init)
#                 print(trend_init)
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
164/48:
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['1step_DLinear_config']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['1step_DLinear_config']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs['1step_DLinear_config']['individual'] # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['1step_DLinear_config']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual==True: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            #print(seasonal_init.size(),trend_init.size()) # 32,1440,1
            seasonal_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
            #print(seasonal_init.size(),trend_init.size())
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            #print(self.individual)
            if self.individual==True:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                print(1)
#                 print(seasonal_init)
#                 print(trend_init)
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
164/49:
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
164/50:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        print(seq.size())
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
164/51:
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['1step_DLinear_config']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['1step_DLinear_config']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs['1step_DLinear_config']['individual'] # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['1step_DLinear_config']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual==True: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasonal_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            #print(self.individual)
            if self.individual==True:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
164/52:
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
164/53:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('mse',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
164/54:
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
164/55:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n) 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
164/56:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
164/57:
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
164/58:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
164/59:
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['1step_DLinear_config']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['1step_DLinear_config']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs['1step_DLinear_config']['individual'] # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['1step_DLinear_config']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual==True: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasonal_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            #print(self.individual)
            if self.individual==True:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
164/60:
split=1440
sequence_length = 1440
pred_len = 1
device = torch.device("cuda:0")
X = data['volumn'].iloc[:-split]
Y = data['volumn'] # 정답 데이터
x_seq, target = seq_data(x=X,y=Y,sequence_length=sequence_length)
x_seq,target = x_seq.permute(0,2,1),target.permute(0,2,1)
print(x_seq.shape,target.shape)
164/61: torch_data = TensorDataset(x_seq,target)
164/62:
batch_size = 32
train_loader = DataLoader(torch_data,batch_size = batch_size)
164/63: from pytorch_forecasting.metrics import SMAPE
164/64:
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
164/65:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
164/66:
new_First_train = X[-sequence_length+1:]
new_First_train
164/67:
new_First_train = pd.concat([X[-sequence_length+1:],Y[0]],dim=0,ignore_index=True)
new_First_train
164/68:
new_First_train = pd.concat([X[-sequence_length+1:],Y[0]],axis=0,ignore_index=True)
new_First_train
164/69:
new_First_train = pd.concat([X[-sequence_length+1:],Y[0:1]],axis=0,ignore_index=True)
new_First_train
164/70: Y[0:1]
164/71: Y[index==0]
164/72: Y[Y.index==0]
164/73: Y
164/74:
target = Y[-split:]
target
164/75:
new_First_train = pd.concat([X[-sequence_length+1:],target[0:1]],axis=0,ignore_index=True)
new_First_train
164/76: import time
164/77:
import time
time.begin()
time.stop()
164/78:
pred_list = []
time_list = []
for idx in range(split):
    start = time.time()
    model(new)
164/79: import time
164/80: new_train = pd.concat([X[-sequence_length+1:],target[0:1]],axis=0,ignore_index=True).values
164/81:
new_train = pd.concat([X[-sequence_length+1:],target[0:1]],axis=0,ignore_index=True).values
new_trian
164/82:
new_train = pd.concat([X[-sequence_length+1:],target[0:1]],axis=0,ignore_index=True).values
new_traiN
164/83:
new_train = pd.concat([X[-sequence_length+1:],target[0:1]],axis=0,ignore_index=True).values
new_train
164/84:
new_train = pd.concat([X[-sequence_length+1:],target[0:1]],axis=0,ignore_index=True).values
n=torch.FloatTensor(new_train),unsqueeze(1)
n
164/85:
new_train = pd.concat([X[-sequence_length+1:],target[0:1]],axis=0,ignore_index=True).values
n=torch.FloatTensor(new_train).unsqueeze(1)
n
164/86: print(seq.size())
164/87:
new_train = pd.concat([X[-sequence_length+1:],target[0:1]],axis=0,ignore_index=True).values
n=torch.FloatTensor(new_train).view(1,1440,1)
n
164/88:
new_train = pd.concat([X[-sequence_length+1:],target[0:1]],axis=0,ignore_index=True).values
n=torch.FloatTensor(new_train).unsqueeze(1)
n
164/89:
new_train = pd.concat([X[-sequence_length+1:],target[0:1]],axis=0,ignore_index=True).values
n=torch.FloatTensor(new_train).view(1,1440,1)
n
164/90:
pred_list = []
time_list = []
for idx in range(split):
    new_train = torch.FloatTensor(
        [pd.concat([X[-sequence_length+idx:],target[idx:idx+1]],axis=0,ignore_index=True).values]
    ).view(1,sequence_length,1)
    # 3차원 텐서로 만들어주는 과정 내가 원하는 모양 (1,1440,1)
    start = time.time()
    out = model(new_traiin)
    end = time.time()
    time_list.append(end-start)
164/91: out
164/92: tg
164/93: tg.size()
164/94:
pred_list = []
time_list = []
for idx in range(split):
    if idx == 0:
         new_train = torch.FloatTensor(
        [X[-sequence_length+idx:].values]
    ).view(1,sequence_length,1)
    else:
        new_train = torch.FloatTensor(
            [pd.concat([X[-sequence_length+idx:],target[idx:idx+1]],axis=0,ignore_index=True).values]
        ).view(1,sequence_length,1)
    # 3차원 텐서로 만들어주는 과정 내가 원하는 모양 (1,1440,1)
    start = time.time()
    out = model(new_traiin)
    end = time.time()
    time_list.append(end-start)
164/95:
pred_list = []
time_list = []
for idx in range(split):
    if idx == 0:
         new_train = torch.FloatTensor(
        [X[-sequence_length+idx:].values]
    ).view(1,sequence_length,1)
    else:
        new_train = torch.FloatTensor(
            [pd.concat([X[-sequence_length+idx:],target[idx:idx+1]],axis=0,ignore_index=True).values]
        ).view(1,sequence_length,1)
    # 3차원 텐서로 만들어주는 과정 내가 원하는 모양 (1,1440,1)
    start = time.time()
    out = model(new_train)
    end = time.time()
    pred_list.append(out.cpu().view(1).item())
    time_list.append(end-start)
164/96:
pred_list = []
time_list = []
for idx in range(split):
    if idx == 0:
         new_train = torch.FloatTensor(
        [X[-sequence_length+idx:].values]
    ).view(1,sequence_length,1).to(device)
    else:
        new_train = torch.FloatTensor(
            [pd.concat([X[-sequence_length+idx:],target[idx:idx+1]],axis=0,ignore_index=True).values]
        ).view(1,sequence_length,1).to(device)
    # 3차원 텐서로 만들어주는 과정 내가 원하는 모양 (1,1440,1)
    start = time.time()
    out = model(new_train)
    end = time.time()
    pred_list.append(out.cpu().view(1).item())
    time_list.append(end-start)
164/97:
pred_list = []
time_list = []
for idx in range(split):
    if idx == 0:
         new_train = torch.FloatTensor(
        [X[-sequence_length+idx:].values]
    ).view(1,sequence_length,1).to(device)
    else:
        new_train = torch.FloatTensor(
            [pd.concat([X[-sequence_length+idx:],target[0:idx+1]],axis=0,ignore_index=True).values]
        ).view(1,sequence_length,1).to(device)
    # 3차원 텐서로 만들어주는 과정 내가 원하는 모양 (1,1440,1)
    start = time.time()
    out = model(new_train)
    end = time.time()
    pred_list.append(out.cpu().view(1).item())
    time_list.append(end-start)
164/98:
pred_list = []
time_list = []
for idx in range(split):
    if idx == 0:
         new_train = torch.FloatTensor(
        [X[-sequence_length+idx:].values]
    ).view(1,sequence_length,1).to(device)
    else:
        new_train = torch.FloatTensor(
            [pd.concat([X[-sequence_length+idx:],target[0:idx]],axis=0,ignore_index=True).values]
        ).view(1,sequence_length,1).to(device)
    # 3차원 텐서로 만들어주는 과정 내가 원하는 모양 (1,1440,1)
    start = time.time()
    out = model(new_train)
    end = time.time()
    pred_list.append(out.cpu().view(1).item())
    time_list.append(end-start)
164/99: pred_list
164/100: len(pred_list)
164/101:
fig,axes = plt.subplots(1,1,figsize(12,8))
axes.plot(np.arange(1,1440),pred_list,label='prediction')
axes.plot(np.arrange(1,1440),target,label='target')
plt.legend()
164/102:
fig,axes = plt.subplots(1,1,figsize=(12,8))
axes.plot(np.arange(1,1440),pred_list,label='prediction')
axes.plot(np.arrange(1,1440),target,label='target')
plt.legend()
164/103:
fig,axes = plt.subplots(1,1,figsize=(12,8))
axes.plot(np.arange(1,1441),pred_list,label='prediction')
axes.plot(np.arrange(1,1441),target,label='target')
plt.legend()
164/104:
fig,axes = plt.subplots(1,1,figsize=(12,8))
axes.plot(np.arange(1,1441),pred_list,label='prediction')
axes.plot(np.arange(1,1441),target,label='target')
plt.legend()
164/105:
fig,axes = plt.subplots(1,1,figsize=(12,8))
axes.plot(np.arange(1,1441),pred_list,label='prediction')
axes.plot(np.arange(1,1441),target,label='target')
axes.legend()
164/106:
mean_pred_time = np.mean(time_list)
mean_pred_time
164/107: SMAPE()(pred_list,target)
164/108:
score = SMAPE()([pred_list],[target]) * 100
score
164/109:
score = SMAPE()([[pred_list],[target]]) * 100
score
164/110: target
164/111:
score = SMAPE()([pred_list],[target.values]) * 100
score
164/112:
score = SMAPE()([pred_list,target.values]) * 100
score
164/113:
score = SMAPE()(pred_list,target.values) * 100
score
164/114:
score = SMAPE()(np.array(pred_list),np.array(target.values)) * 100
score
164/115:
score = SMAPE()(torch.FloatTensor(pred_list),torch.FloatTensor(target.values)) * 100
score
164/116:
score = SMAPE()(torch.FloatTensor(pred_list).view(-1,1),torch.FloatTensor(target.values).view(-1,1)) * 100
score
164/117:
score = SMAPE()(torch.FloatTensor(pred_list).view(-1,1),torch.FloatTensor(target.values).view(-1,1)) * 100
score.item()
164/118:
score = SMAPE()(torch.FloatTensor(pred_list).view(-1,1),torch.FloatTensor(target.values).view(-1,1))
score.item()
164/119:
score = SMAPE()(torch.FloatTensor(pred_list).view(-1,1),torch.FloatTensor(target.values).view(-1,1)) * 100
score.item()
164/120: from pytorch_forecasting.metrics import RMSE
164/121:
score = RMSE()(torch.FloatTensor(pred_list).view(-1,1),torch.FloatTensor(target.values).view(-1,1)) * 100
score.item()
164/122: pred_list
164/123: target
164/124:
score = RMSE()(torch.FloatTensor(pred_list).view(-1,1),torch.FloatTensor(target.values).view(-1,1))
score.item()
164/125:
score = SMAPE()(torch.FloatTensor(target.values).view(-1,1),torch.FloatTensor(pred_list).view(-1,1)) * 100
score.item()
164/126:
score = SMAPE()(torch.FloatTensor(target.values).view(-1,1),torch.FloatTensor(pred_list).view(-1,1)) * 10
score.item()
164/127:
score = SMAPE()(torch.FloatTensor(target.values).view(-1,1),torch.FloatTensor(pred_list).view(-1,1)) * 100
score.item()
166/1:
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
import torch.optim as optim
import matplotlib.pyplot as plt
from torch.utils.data import TensorDataset,DataLoader
from torch import FloatTensor
from sklearn.preprocessing import MinMaxScaler
166/2: False
166/3:
import configparser
import os
config = configparser()
config['NLinar_config'] = {
    'seq_len':1440,
    'pred_len':1.
    'enc_in':1,
    'individual':bool(False)
}
with open('NLinear_config','w') as f:
    config.write(f)
166/4:
import configparser
import os
config = configparser()
config['NLinar_config'] = {
    'seq_len':1440,
    'pred_len':1,
    'enc_in':1,
    'individual':bool(False)
}
with open('NLinear_config','w') as f:
    config.write(f)
166/5:
import configparser
config = configparser.ConfigParser()
config['NLinar_config'] = {
    'seq_len':1440,
    'pred_len':1,
    'enc_in':1,
    'individual':bool(False)
}
with open('NLinear_config','w') as f:
    config.write(f)
166/6: config.read(os.getcwd()+os.sep+'NLinear_config',encoding = 'utf-8')
166/7:
x = torch.FloatTensor([[[1,2,3],[4,5,6][7,8,9]]])
x[:,:,i]
166/8:
x = torch.FloatTensor([[[1,2,3],[4,5,6][7,8,9]]])
x[:,:,2]
166/9:
x = torch.FloatTensor([[[1,2,3],[4,5,6],[7,8,9]]])
x[:,:,2]
166/10:
x = torch.FloatTensor([[[1,2,3],[4,5,6],[7,8,9]],[[2,3,4],[5,6,7],[8,9,10]]])
x[:,:,2]
166/11: torch.FloatTensor([[[1,2,3],[4,5,6],[7,8,9]],[[2,3,4],[5,6,7],[8,9,10]]])+torch.FloatTensor([[[1]]])
166/12: torch.FloatTensor([[[1,2,3],[4,5,6],[7,8,9]],[[2,3,4],[5,6,7],[8,9,10]]])+torch.FloatTensor([[1]])
166/13: torch.FloatTensor([[[1,2,3],[4,5,6],[7,8,9]],[[2,3,4],[5,6,7],[8,9,10]]])+torch.FloatTensor([1])
166/14: torch.FloatTensor([[[1,2,3],[4,5,6],[7,8,9]],[[2,3,4],[5,6,7],[8,9,10]]])+torch.FloatTensor([1,2,3])
166/15: torch.FloatTensor([[[1,2,3],[4,5,6],[7,8,9]],[[2,3,4],[5,6,7],[8,9,10]]])+torch.FloatTensor([1,2])
166/16: torch.FloatTensor([[[1,2,3],[4,5,6],[7,8,9]],[[2,3,4],[5,6,7],[8,9,10]]])+torch.FloatTensor([1])
166/17: torch.FloatTensor([[[1,2,3],[4,5,6],[7,8,9]],[[2,3,4],[5,6,7],[8,9,10]]])+torch.FloatTensor([[1],[2],[3]])
166/18: torch.FloatTensor([[[1,2,3],[4,5,6],[7,8,9]],[[2,3,4],[5,6,7],[8,9,10]]])+torch.FloatTensor([1],[2],[3])
166/19: torch.FloatTensor([[[1,2,3],[4,5,6],[7,8,9]],[[2,3,4],[5,6,7],[8,9,10]]])+torch.FloatTensor([1,2,3])
166/20:
class Model(nn.Module):
    def __init__(self,configs):
        super(Model,self).__init__()
        self.seq_len = configs.seq_len
        self.pred_len = configs.pred_len
        self.channels = configs.enc_in
        self.individual = configs.individual
        if self.individual == True:
            self.Linear = nn.ModuleList()
            for i in range(self.channels):
                self.Linear.append(nn.Linear(self.seq_len,self.pred_len))
        else:
            self.Linear = nn.Linear(self.seq_len,self.pred_len)
    
    def forward(self,x):
        # x는 RNN Layer처럼 하나의 값이 1차원의 텐서에 들어가 있도록
        seq_last = x[:,-1,:].detach()
        x = x-seq_last
        if self.individual == True:
            output = torch.zeros([x.size(0),self.pred_len,x.size(2)],dtype = x.dtype).to(x.device)
            for i in range(self.channels):
                output[:,:,i] = self.Linear[i](x[:,:,i])# 하나의 특성마다 각기 다른 Linear layer에 집어넣어줌
                # 이렇게 인덱싱하면 위에 처럼 linear layer에 넣기 좋게 나온다
            x = output
        else:
            x = self.Linear(x.permute(0,2,1)).permute(0,2,1)
        x = x+seq_last
        return x #[Batch, Output length, Channel]
166/21:
data = pd.read_csv('../traffic/data/5.csv')
data
166/22: data.drop(['service_name','datetime','packets','unknown'],inplace=True)
166/23: data.drop(['service_name','datetime','packets','unknown'],inplace=True,axis=1)
166/24: data
166/25:
def seq_data(x,y,sequence_length):
    if (type(x)==pd.series)|(type(x)==pd.DataFrame):
        for i in range(len(x)-sequence_length):
            x_temp = torch.FloatTensor(x.iloc[i:i+sequence_length].values).view(1,-1,1)
            y_temp= torch.FloatTensor(y.iloc[i+sequence_length]).view(1,-1,1)
            
            if i>=1:
                x_seq,y_seq=torch.cat((x_seq1,x_temp),dim = 1),torch.cat((y_seq1,y_temp),dim = 1)
            x_seq1,y_seq1 = x_temp,y_temp
        
    else: print()
    return x_seq,y_seq
166/26:
def seq_data(x,y,sequence_length):
    if (type(x)==pd.series)|(type(x)==pd.DataFrame):
        for i in range(len(x)-sequence_length):
            x_temp = torch.FloatTensor(x.iloc[i:i+sequence_length].values).view(1,-1,1)
            y_temp= torch.FloatTensor(y.iloc[i+sequence_length]).view(1,-1,1)
            
            if i>=1:
                x_seq,y_seq=torch.cat((x_seq1,x_temp),dim = 1),torch.cat((y_seq1,y_temp),dim = 1)
            x_seq1,y_seq1 = x_temp,y_temp
        
    else: print("make to Series or DataFrame")
    return x_seq,y_seq
166/27:
data = pd.read_csv('../traffic/data/5.csv')
data
166/28: data.drop(['service_name','datetime','packets','unknown'],inplace=True,axis=1)
166/29: data
166/30:
split = 1440
device = torch.device('cuda:0')
X = data['volumn'].iloc[:-split]
Y = data['volumn']
x_seq,target
166/31:
split = 1440
device = torch.device('cuda:0')
X = data['volumn'].iloc[:-split]
Y = data['volumn']
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
166/32:
split = 1440
sequence_length = 1440
device = torch.device('cuda:0')
X = data['volumn'].iloc[:-split]
Y = data['volumn']
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
166/33:
def seq_data(x,y,sequence_length):
    if (type(x)==pd.Series|(type(x)==pd.DataFrame):
        for i in range(len(x)-sequence_length):
            x_temp = torch.FloatTensor(x.iloc[i:i+sequence_length].values).view(1,-1,1)
            y_temp= torch.FloatTensor(y.iloc[i+sequence_length]).view(1,-1,1)
            
            if i>=1:
                x_seq,y_seq=torch.cat((x_seq1,x_temp),dim = 1),torch.cat((y_seq1,y_temp),dim = 1)
            x_seq1,y_seq1 = x_temp,y_temp
        
    else: print("make to Series or DataFrame")
    return x_seq,y_seq
166/34:
def seq_data(x,y,sequence_length):
    if (type(x)==pd.Series)|(type(x)==pd.DataFrame):
        for i in range(len(x)-sequence_length):
            x_temp = torch.FloatTensor(x.iloc[i:i+sequence_length].values).view(1,-1,1)
            y_temp= torch.FloatTensor(y.iloc[i+sequence_length]).view(1,-1,1)
            
            if i>=1:
                x_seq,y_seq=torch.cat((x_seq1,x_temp),dim = 1),torch.cat((y_seq1,y_temp),dim = 1)
            x_seq1,y_seq1 = x_temp,y_temp
        
    else: print("make to Series or DataFrame")
    return x_seq,y_seq
166/35:
split = 1440
sequence_length = 1440
device = torch.device('cuda:0')
X = data['volumn'].iloc[:-split]
Y = data['volumn']
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
166/36:
def seq_data(x,y,sequence_length):
    if (type(x)==pd.Series)|(type(x)==pd.DataFrame):
        for i in range(len(x)-sequence_length):
            x_temp = torch.FloatTensor(x.iloc[i:i+sequence_length].values).view(1,-1,1)
            y_temp= torch.FloatTensor(y.iloc[i+sequence_length]).view(1,-1,1)
            
            if i==0: x_seq,y_seq = x_temp,y_temp
                
            else:
                x_seq,y_seq=torch.cat((x_seq,x_temp),dim = 1),torch.cat((y_seq,y_temp),dim = 1)
                
    else: print("make to Series or DataFrame")
    return x_seq,y_seq
166/37:
split = 1440
sequence_length = 1440
device = torch.device('cuda:0')
X = data['volumn'].iloc[:-split]
Y = data['volumn']
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
166/38:
def seq_data(x,y,sequence_length):
    if (type(x)==pd.Series)|(type(x)==pd.DataFrame):
        for i in range(len(x)-sequence_length):
            x_temp = torch.FloatTensor(x.iloc[i:i+sequence_length].values).view(1,-1,1)
            y_temp= torch.FloatTensor(y.iloc[i+sequence_length]).view(1,-1,1)
            
            if i==0: x_seq,y_seq = x_temp,y_temp
                
            else:
                x_seq,y_seq=torch.cat((x_seq,x_temp),dim = 1),torch.cat((y_seq,y_temp),dim = 1)
                
    else: print("make to Series or DataFrame")
    return x_seq,y_seq
166/39:
data = pd.read_csv('../traffic/data/5.csv')
data
166/40: data.drop(['service_name','datetime','packets','unknown'],inplace=True,axis=1)
166/41: data
166/42:
split = 1440
sequence_length = 1440
device = torch.device('cuda:0')
X = data['volumn'].iloc[:-split]
Y = data['volumn']
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
166/43:
def seq_data(x,y,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length])
#             if i == 0:
#                  print(x.iloc[i:i+sequence_length].values)
#                  print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,pred_len).to(device)
166/44:
split = 1440
sequence_length = 1440
device = torch.device('cuda:0')
X = data['volumn'].iloc[:-split]
Y = data['volumn']
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
166/45:
def seq_data(x,y,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length])
#             if i == 0:
#                  print(x.iloc[i:i+sequence_length].values)
#                  print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).view(1,-1,1),FloatTensor(target_list).view(1,-1,1)
166/46:
split = 1440
sequence_length = 1440
device = torch.device('cuda:0')
X = data['volumn'].iloc[:-split]
Y = data['volumn']
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
166/47: torch_data = TensorDataset(x_seq,target)
166/48:
batch_size = 32
train_loader = DataLoader(torch_data,batch_size = batch_size)
166/49:
for i,j in train_loader:
    print(i)
    break
166/50:
for i,j in train_loader:
    print(i.size()
    break
166/51:
for i,j in train_loader:
    print(i.size())
    break
166/52:
for i,j in train_loader:
    print(i.size())
    print(j.size())
    break
166/53:
def seq_data(x,y,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length])
#             if i == 0:
#                  print(x.iloc[i:i+sequence_length].values)
#                  print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).view(1,-1,sequence_length),FloatTensor(target_list).view(1,-1,sequence_length)
166/54:
split = 1440
sequence_length = 1440
device = torch.device('cuda:0')
X = data['volumn'].iloc[:-split]
Y = data['volumn']
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
166/55:
def seq_data(x,y,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length])
#             if i == 0:
#                  print(x.iloc[i:i+sequence_length].values)
#                  print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).view(1,-1,sequence_length),FloatTensor(target_list).view(1,-1,1)
166/56:
batch_size = 32
train_loader = DataLoader(torch_data,batch_size = batch_size)
166/57: torch_data = TensorDataset(x_seq,target)
166/58:
batch_size = 32
train_loader = DataLoader(torch_data,batch_size = batch_size)
166/59:
for i,j in train_loader:
    print(i.size())
    print(j.size())
    break
166/60:
def seq_data(x,y,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length])
#             if i == 0:
#                  print(x.iloc[i:i+sequence_length].values)
#                  print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).view(-1,sequence_length,1),FloatTensor(target_list).view(-1,1,1)
166/61:
split = 1440
sequence_length = 1440
device = torch.device('cuda:0')
X = data['volumn'].iloc[:-split]
Y = data['volumn']
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
166/62: torch_data = TensorDataset(x_seq,target)
166/63:
batch_size = 32
train_loader = DataLoader(torch_data,batch_size = batch_size)
166/64:
for i,j in train_loader:
    print(i.size())
    print(j.size())
    break
166/65:
from pytorch_forecasting.metrics import SMAPE
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
166/66:
import configparser
config = configparser.ConfigParser()
config['NLinear_config'] = {
    'seq_len':1440,
    'pred_len':1,
    'enc_in':1,
    'individual':bool(False)
}
with open('NLinear_config','w') as f:
    config.write(f)
166/67: config.read(os.getcwd()+os.sep+'NLinear_config',encoding = 'utf-8')
166/68:
from pytorch_forecasting.metrics import SMAPE
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
166/69:
import configparser
config = configparser.ConfigParser()
config['NLinear_config'] = {
    'seq_len':1440,
    'pred_len':1,
    'enc_in':1,
    'individual':bool(False)
}
with open('NLinear_config','w') as f:
    config.write(f)
166/70: config.read(os.getcwd()+os.sep+'NLinear_config',encoding = 'utf-8')
166/71:
from pytorch_forecasting.metrics import SMAPE
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
166/72:
class Model(nn.Module):
    def __init__(self,configs):
        super(Model,self).__init__()
        self.seq_len = configs['NLinear_config']['seq_len']
        self.pred_len = configs['NLinear_config']['pred_len']
        self.channels = configs['NLinear_config']['enc_in']
        self.individual = configs['NLinear_config']['individual']
        if self.individual == True:
            self.Linear = nn.ModuleList()
            for i in range(self.channels):
                self.Linear.append(nn.Linear(self.seq_len,self.pred_len))
        else:
            self.Linear = nn.Linear(self.seq_len,self.pred_len)
    
    def forward(self,x):
        # x는 RNN Layer처럼 하나의 값이 하나의 텐서안에 들어가 있도록 즉 (batch_size,sequence_length,channe)
        seq_last = x[:,-1,:].detach()
        x = x-seq_last
        if self.individual == True:
            output = torch.zeros([x.size(0),self.pred_len,x.size(2)],dtype = x.dtype).to(x.device)
            for i in range(self.channels):
                output[:,:,i] = self.Linear[i](x[:,:,i])# 하나의 특성마다 각기 다른 Linear layer에 집어넣어줌
                # 이렇게 인덱싱하면 위에 처럼 linear layer에 넣기 좋게 나온다
            x = output
        else:
            x = self.Linear(x.permute(0,2,1)).permute(0,2,1)
        x = x+seq_last
        return x #[Batch, Output length, Channel]
166/73:
from pytorch_forecasting.metrics import SMAPE
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
166/74:
class Model(nn.Module):
    def __init__(self,configs):
        super(Model,self).__init__()
        self.seq_len = int(configs['NLinear_config']['seq_len'])
        self.pred_len = int(configs['NLinear_config']['pred_len'])
        self.channels = int(configs['NLinear_config']['enc_in'])
        self.individual = configs['NLinear_config']['individual']
        if self.individual == True:
            self.Linear = nn.ModuleList()
            for i in range(self.channels):
                self.Linear.append(nn.Linear(self.seq_len,self.pred_len))
        else:
            self.Linear = nn.Linear(self.seq_len,self.pred_len)
    
    def forward(self,x):
        # x는 RNN Layer처럼 하나의 값이 하나의 텐서안에 들어가 있도록 즉 (batch_size,sequence_length,channe)
        seq_last = x[:,-1,:].detach()
        x = x-seq_last
        if self.individual == True:
            output = torch.zeros([x.size(0),self.pred_len,x.size(2)],dtype = x.dtype).to(x.device)
            for i in range(self.channels):
                output[:,:,i] = self.Linear[i](x[:,:,i])# 하나의 특성마다 각기 다른 Linear layer에 집어넣어줌
                # 이렇게 인덱싱하면 위에 처럼 linear layer에 넣기 좋게 나온다
            x = output
        else:
            x = self.Linear(x.permute(0,2,1)).permute(0,2,1)
        x = x+seq_last
        return x #[Batch, Output length, Channel]
166/75:
from pytorch_forecasting.metrics import SMAPE
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
166/76:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
        if epoch == 1:
            print(out.size())
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
166/77:
class Model(nn.Module):
    def __init__(self,configs):
        super(Model,self).__init__()
        self.seq_len = int(configs['NLinear_config']['seq_len'])
        self.pred_len = int(configs['NLinear_config']['pred_len'])
        self.channels = int(configs['NLinear_config']['enc_in'])
        self.individual = configs['NLinear_config']['individual']
        if self.individual == True:
            self.Linear = nn.ModuleList()
            for i in range(self.channels):
                self.Linear.append(nn.Linear(self.seq_len,self.pred_len))
        else:
            self.Linear = nn.Linear(self.seq_len,self.pred_len)
    
    def forward(self,x):
        # x는 RNN Layer처럼 하나의 값이 하나의 텐서안에 들어가 있도록 즉 (batch_size,sequence_length,channe)
        seq_last = x[:,-1,:].detach()
        print(seq_last)
        x = x-seq_last
        if self.individual == True:
            output = torch.zeros([x.size(0),self.pred_len,x.size(2)],dtype = x.dtype).to(x.device)
            for i in range(self.channels):
                output[:,:,i] = self.Linear[i](x[:,:,i])# 하나의 특성마다 각기 다른 Linear layer에 집어넣어줌
                # 이렇게 인덱싱하면 위에 처럼 linear layer에 넣기 좋게 나온다
            x = output
        else:
            x = self.Linear(x.permute(0,2,1)).permute(0,2,1)
        x = x+seq_last
        return x #[Batch, Output length, Channel]
166/78:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
        if epoch == 1:
            print(out.size())
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
166/79:
from pytorch_forecasting.metrics import SMAPE
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
166/80:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
        if epoch == 1:
            print(out.size())
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
166/81:
for i,j in train_loader:
    print(i.size())
    print(j.size())
166/82:
class Model(nn.Module):
    def __init__(self,configs):
        super(Model,self).__init__()
        self.seq_len = int(configs['NLinear_config']['seq_len'])
        self.pred_len = int(configs['NLinear_config']['pred_len'])
        self.channels = int(configs['NLinear_config']['enc_in'])
        self.individual = configs['NLinear_config']['individual']
        if self.individual == True:
            self.Linear = nn.ModuleList()
            for i in range(self.channels):
                self.Linear.append(nn.Linear(self.seq_len,self.pred_len))
        else:
            self.Linear = nn.Linear(self.seq_len,self.pred_len)
    
    def forward(self,x):
        # x는 RNN Layer처럼 하나의 값이 하나의 텐서안에 들어가 있도록 즉 (batch_size,sequence_length,channe)
        seq_last = x[:,-1,:].detach()
        print(seq_last.size())
        x = x-seq_last
        if self.individual == True:
            output = torch.zeros([x.size(0),self.pred_len,x.size(2)],dtype = x.dtype).to(x.device)
            for i in range(self.channels):
                output[:,:,i] = self.Linear[i](x[:,:,i])# 하나의 특성마다 각기 다른 Linear layer에 집어넣어줌
                # 이렇게 인덱싱하면 위에 처럼 linear layer에 넣기 좋게 나온다
            x = output
        else:
            x = self.Linear(x.permute(0,2,1)).permute(0,2,1)
        x = x+seq_last
        return x #[Batch, Output length, Channel]
166/83:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
        if epoch == 1:
            print(out.size())
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
166/84:
from pytorch_forecasting.metrics import SMAPE
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
166/85:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
        if epoch == 1:
            print(out.size())
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
166/86:
class Model(nn.Module):
    def __init__(self,configs):
        super(Model,self).__init__()
        self.seq_len = int(configs['NLinear_config']['seq_len'])
        self.pred_len = int(configs['NLinear_config']['pred_len'])
        self.channels = int(configs['NLinear_config']['enc_in'])
        self.individual = configs['NLinear_config']['individual']
        if self.individual == True:
            self.Linear = nn.ModuleList()
            for i in range(self.channels):
                self.Linear.append(nn.Linear(self.seq_len,self.pred_len))
        else:
            self.Linear = nn.Linear(self.seq_len,self.pred_len)
    
    def forward(self,x):
        # x는 RNN Layer처럼 하나의 값이 하나의 텐서안에 들어가 있도록 즉 (batch_size,sequence_length,channe)
        seq_last = x[:,-1,:]
        print(seq_last.size())
        x = x-seq_last
        if self.individual == True:
            output = torch.zeros([x.size(0),self.pred_len,x.size(2)],dtype = x.dtype).to(x.device)
            for i in range(self.channels):
                output[:,:,i] = self.Linear[i](x[:,:,i])# 하나의 특성마다 각기 다른 Linear layer에 집어넣어줌
                # 이렇게 인덱싱하면 위에 처럼 linear layer에 넣기 좋게 나온다
            x = output
        else:
            x = self.Linear(x.permute(0,2,1)).permute(0,2,1)
        x = x+seq_last
        return x #[Batch, Output length, Channel]
166/87:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
        if epoch == 1:
            print(out.size())
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
166/88:
from pytorch_forecasting.metrics import SMAPE
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
166/89:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
        if epoch == 1:
            print(out.size())
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
166/90:
class Model(nn.Module):
    def __init__(self,configs):
        super(Model,self).__init__()
        self.seq_len = int(configs['NLinear_config']['seq_len'])
        self.pred_len = int(configs['NLinear_config']['pred_len'])
        self.channels = int(configs['NLinear_config']['enc_in'])
        self.individual = configs['NLinear_config']['individual']
        if self.individual == True:
            self.Linear = nn.ModuleList()
            for i in range(self.channels):
                self.Linear.append(nn.Linear(self.seq_len,self.pred_len))
        else:
            self.Linear = nn.Linear(self.seq_len,self.pred_len)
    
    def forward(self,x):
        # x는 RNN Layer처럼 하나의 값이 하나의 텐서안에 들어가 있도록 즉 (batch_size,sequence_length,channe)
        seq_last = x[:,-1,:].unsqueeze(2)
        x = x-seq_last
        if self.individual == True:
            output = torch.zeros([x.size(0),self.pred_len,x.size(2)],dtype = x.dtype).to(x.device)
            for i in range(self.channels):
                output[:,:,i] = self.Linear[i](x[:,:,i])# 하나의 특성마다 각기 다른 Linear layer에 집어넣어줌
                # 이렇게 인덱싱하면 위에 처럼 linear layer에 넣기 좋게 나온다
            x = output
        else:
            x = self.Linear(x.permute(0,2,1)).permute(0,2,1)
        x = x+seq_last
        return x #[Batch, Output length, Channel]
166/91:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
        if epoch == 1:
            print(out.size())
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
166/92:
from pytorch_forecasting.metrics import SMAPE
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
166/93:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
        if epoch == 1:
            print(out.size())
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
166/94:
class Model(nn.Module):
    def __init__(self,configs):
        super(Model,self).__init__()
        self.seq_len = int(configs['NLinear_config']['seq_len'])
        self.pred_len = int(configs['NLinear_config']['pred_len'])
        self.channels = int(configs['NLinear_config']['enc_in'])
        self.individual = configs['NLinear_config']['individual']
        if self.individual == True:
            self.Linear = nn.ModuleList()
            for i in range(self.channels):
                self.Linear.append(nn.Linear(self.seq_len,self.pred_len))
        else:
            self.Linear = nn.Linear(self.seq_len,self.pred_len)
    
    def forward(self,x):
        # x는 RNN Layer처럼 하나의 값이 하나의 텐서안에 들어가 있도록 즉 (batch_size,sequence_length,channe)
        seq_last = x[:,-1,:].unsqueeze(2)
        print(seq_last)
        x = x-seq_last
        if self.individual == True:
            output = torch.zeros([x.size(0),self.pred_len,x.size(2)],dtype = x.dtype).to(x.device)
            for i in range(self.channels):
                output[:,:,i] = self.Linear[i](x[:,:,i])# 하나의 특성마다 각기 다른 Linear layer에 집어넣어줌
                # 이렇게 인덱싱하면 위에 처럼 linear layer에 넣기 좋게 나온다
            x = output
        else:
            x = self.Linear(x.permute(0,2,1)).permute(0,2,1)
        x = x+seq_last
        return x #[Batch, Output length, Channel]
166/95:
from pytorch_forecasting.metrics import SMAPE
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
166/96:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
        if epoch == 1:
            print(out.size())
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
166/97:
class Model(nn.Module):
    def __init__(self,configs):
        super(Model,self).__init__()
        self.seq_len = int(configs['NLinear_config']['seq_len'])
        self.pred_len = int(configs['NLinear_config']['pred_len'])
        self.channels = int(configs['NLinear_config']['enc_in'])
        self.individual = configs['NLinear_config']['individual']
        if self.individual == True:
            self.Linear = nn.ModuleList()
            for i in range(self.channels):
                self.Linear.append(nn.Linear(self.seq_len,self.pred_len))
        else:
            self.Linear = nn.Linear(self.seq_len,self.pred_len)
    
    def forward(self,x):
        # x는 RNN Layer처럼 하나의 값이 하나의 텐서안에 들어가 있도록 즉 (batch_size,sequence_length,channe)
        device = torch.device("cuda:0")
        seq_last = x[:,-1,:].unsqueeze(2)
        print(seq_last)
        x = (x-seq_last).to(device)
        if self.individual == True:
            output = torch.zeros([x.size(0),self.pred_len,x.size(2)],dtype = x.dtype).to(x.device)
            for i in range(self.channels):
                output[:,:,i] = self.Linear[i](x[:,:,i])# 하나의 특성마다 각기 다른 Linear layer에 집어넣어줌
                # 이렇게 인덱싱하면 위에 처럼 linear layer에 넣기 좋게 나온다
            x = output
        else:
            x = self.Linear(x.permute(0,2,1)).permute(0,2,1)
        x = x+seq_last
        return x #[Batch, Output length, Channel]
166/98:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
        if epoch == 1:
            print(out.size())
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
166/99:
from pytorch_forecasting.metrics import SMAPE
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
166/100:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
        if epoch == 1:
            print(out.size())
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
166/101:
class Model(nn.Module):
    def __init__(self,configs):
        super(Model,self).__init__()
        self.seq_len = int(configs['NLinear_config']['seq_len'])
        self.pred_len = int(configs['NLinear_config']['pred_len'])
        self.channels = int(configs['NLinear_config']['enc_in'])
        self.individual = configs['NLinear_config']['individual']
        if self.individual == True:
            self.Linear = nn.ModuleList()
            for i in range(self.channels):
                self.Linear.append(nn.Linear(self.seq_len,self.pred_len))
        else:
            self.Linear = nn.Linear(self.seq_len,self.pred_len)
    
    def forward(self,x):
        # x는 RNN Layer처럼 하나의 값이 하나의 텐서안에 들어가 있도록 즉 (batch_size,sequence_length,channe)
        device = torch.device("cuda:0")
        x= x.to(device)
        seq_last = x[:,-1,:].unsqueeze(2).to(device)
        x = x-seq_last
        if self.individual == True:
            output = torch.zeros([x.size(0),self.pred_len,x.size(2)],dtype = x.dtype).to(x.device)
            for i in range(self.channels):
                output[:,:,i] = self.Linear[i](x[:,:,i])# 하나의 특성마다 각기 다른 Linear layer에 집어넣어줌
                # 이렇게 인덱싱하면 위에 처럼 linear layer에 넣기 좋게 나온다
            x = output
        else:
            x = self.Linear(x.permute(0,2,1)).permute(0,2,1)
        x = x+seq_last
        return x #[Batch, Output length, Channel]
166/102:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
        if epoch == 1:
            print(out.size())
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
166/103:
from pytorch_forecasting.metrics import SMAPE
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
166/104:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
        if epoch == 1:
            print(out.size())
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
166/105:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
166/106:
pred_list = []
time_list = []
for idx in range(split):
    if idx == 0:
         new_train = torch.FloatTensor(
        [X[-sequence_length+idx:].values]
    ).view(1,sequence_length,1).to(device)
    else:
        new_train = torch.FloatTensor(
            [pd.concat([X[-sequence_length+idx:],target[0:idx]],axis=0,ignore_index=True).values]
        ).view(1,sequence_length,1).to(device)
    # 3차원 텐서로 만들어주는 과정 내가 원하는 모양 (1,1440,1)
    start = time.time()
    out = model(new_train)
    end = time.time()
    pred_list.append(out.cpu().view(1).item())
    time_list.append(end-start)
166/107:
target = Y[-split:]
target
166/108:
pred_list = []
time_list = []
for idx in range(split):
    if idx == 0:
         new_train = torch.FloatTensor(
        [X[-sequence_length+idx:].values]
    ).view(1,sequence_length,1).to(device)
    else:
        new_train = torch.FloatTensor(
            [pd.concat([X[-sequence_length+idx:],target[0:idx]],axis=0,ignore_index=True).values]
        ).view(1,sequence_length,1).to(device)
    # 3차원 텐서로 만들어주는 과정 내가 원하는 모양 (1,1440,1)
    start = time.time()
    out = model(new_train)
    end = time.time()
    pred_list.append(out.cpu().view(1).item())
    time_list.append(end-start)
166/109: len(pred_list)
166/110:
fig,axes = plt.subplots(1,1,figsize=(12,8))
axes.plot(np.arange(1,1441),pred_list,label='prediction')
axes.plot(np.arange(1,1441),target,label='target')
axes.legend()
166/111:
mean_pred_time = np.mean(time_list)
mean_pred_time
166/112:
score = SMAPE()(torch.FloatTensor(target.values).view(-1,1),torch.FloatTensor(pred_list).view(-1,1)) * 100
score.item()
166/113:
score = RMSE()(torch.FloatTensor(pred_list).view(-1,1),torch.FloatTensor(target.values).view(-1,1))
score.item()
166/114: from pytorch_forecasting.metrics import RMSE
166/115:
score = RMSE()(torch.FloatTensor(pred_list).view(-1,1),torch.FloatTensor(target.values).view(-1,1))
score.item()
173/1: import torch
173/2:
import torch
import torch.nn
173/3: nn.LSTM()
173/4:
import torch
import torch.nn
173/5:
import torch
import torch.nn as nn
173/6: nn.LSTM()
173/7:
class Encoder(nn.Module):
    def __init__(self,configs):
        self.input_size = configs['seq2seq']['input_size']
        # input의 feature dimension을 넣어주어야 한다
        self.hidden_size = configs['seq2seq']['hidden_size']
        # 내부에서 feature dimension을 어떻게 바꿔주고 싶은지 넣어주면 된다
        # 가령 mxn matrix가 입력으로 들어왔을때 hidden size를 h라 한다면 mxh의 크기로 바꾼다
        self.num_layers = configs['seq2seq']['num_layers']
        self.lstm = nn.LSTM(input_size=self.input_size,hidden_size = self.hidden_size,
                           num_layers = self.num_layers,batch_first = True)

    
    def forward(self,x):
        lstm_out,hidden = self.lstm(x) #lstm의 output으로 나오는 hidden_state는 마지막 hidden_state값이다
        return lstm_out,hidden

class Decoder(nn.Module):
    def __init__(self,configs):
        super(Decoder,self).__init__()
        self.input_size = configs['seq2seq']['input_size']
        # input의 feature dimension을 넣어주어야 한다
        self.hidden_size = configs['seq2seq']['hidden_size']
        # 내부에서 feature dimension을 어떻게 바꿔주고 싶은지 넣어주면 된다
        # 가령 mxn matrix가 입력으로 들어왔을때 hidden size를 h라 한다면 mxh의 크기로 바꾼다
        self.num_layers = configs['seq2seq']['num_layers']
        self.lstm = nn.LSTM(input_size=self.input_size,hidden_size = self.hidden_size,
                           num_layers = self.num_layers,batch_first = True)
        self.linear = nn.Linear(self.hidden_size,self.input_size)
        
    def forward(self,x_in,encoder_hidden):
        lstm_out,hidden = self.lstm(x,encoder_hidden)
        output = self.linear(lstm_out)
    # 정리하자면 우리가 정답을 알고 있는 시점에서 1시점 전까지의 데이터를 encoder로 넣고
    # 알고 있는 마지막시점 정보와 encoder에 넣어서 나온 hidden state를 넣어줌으로써 다음시점을 예측
class seq2seq(nn.Module):
    def __init__(self,configs):
        super(seq2seq,self).__init__()
        self.encoder = Encoder(configs)
        self.decoder = Decoder(configs)
        
    def forward(self,x):
        outputs = torch.zeros(x.size(0),1,1) # feature가1개고 target도 1개인 데이터
        _,final_hidden = self.encoder(x)
        output,_ = self.decoder(x_in = x[:,-1,:],encoder_hidden = final_hidden)
        
        return output
173/8:
data = pd.read_csv('../traffic/data/5.csv')

data.drop(['service_name','datetime','packets','unknown'],inplace=True,axis=1)

split = 1440
sequence_length = 1440
device = torch.device('cuda:0')
X = data['volumn'].iloc[:-split]
Y = data['volumn']
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
173/9:
import torch
import torch.nn as nn
import pandas as pd
173/10:
data = pd.read_csv('../traffic/data/5.csv')

data.drop(['service_name','datetime','packets','unknown'],inplace=True,axis=1)

split = 1440
sequence_length = 1440
device = torch.device('cuda:0')
X = data['volumn'].iloc[:-split]
Y = data['volumn']
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
173/11:
def seq_data(x,y,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length])
#             if i == 0:
#                  print(x.iloc[i:i+sequence_length].values)
#                  print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).view(-1,sequence_length,1),FloatTensor(target_list).view(-1,1,1)
    # DataLoader로 batch_size만큼 데이터를 쪼개고 싶다면 [Batch, Input_length, Channel] 형태를 따라줘야됨
173/12:
data = pd.read_csv('../traffic/data/5.csv')

data.drop(['service_name','datetime','packets','unknown'],inplace=True,axis=1)

split = 1440
sequence_length = 1440
device = torch.device('cuda:0')
X = data['volumn'].iloc[:-split]
Y = data['volumn']
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
173/13:
import torch
import torch.nn as nn
import pandas as pd
from torch.optim import Adam
from torch import FloatTensor
173/14:
data = pd.read_csv('../traffic/data/5.csv')

data.drop(['service_name','datetime','packets','unknown'],inplace=True,axis=1)

split = 1440
sequence_length = 1440
device = torch.device('cuda:0')
X = data['volumn'].iloc[:-split]
Y = data['volumn']
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
173/15: torch_data = TensorDataset(x_seq,target)
173/16:
import torch
import torch.nn as nn
import pandas as pd
from torch.optim import Adam
from torch import FloatTensor
from torch.utils.data import TensorDataset,DataLoader
173/17:
torch_data = TensorDataset(x_seq,target)
batch_size = 32
train_loader = DataLoader(torch_data,batch_size = batch_size)
173/18:
import configparser
config = configparser.ConfigParser()
config['seq2seq']= {
    'input_size' : int(1),
    'hidden_size' : int(8),
    'num_layers' : int(2) 
}
with open('seq2seq','w') as f:
    config.write(f)
173/19:
import os
config = config.read(os.get()+os.sep()+'seq2seq')
173/20:
import os
config = config.read(os.getcwd()+os.sep()+'seq2seq')
173/21:
import os
config = config.read(os.getcwd()+os.sep+'seq2seq')
173/22:
from pytorch_forecasting.metrics import SMAPE
model = seq2seq(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
173/23:
import os
config = config.read(os.getcwd()+os.sep+'seq2seq',encoding='utf-8')
173/24:
from pytorch_forecasting.metrics import SMAPE
model = seq2seq(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
173/25:
import os
config.read(os.getcwd()+os.sep+'seq2seq',encoding='utf-8')
173/26:
from pytorch_forecasting.metrics import SMAPE
model = seq2seq(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
173/27:
import configparser
config = configparser.ConfigParser()
config['seq2seq']= {
    'input_size' : int(1),
    'hidden_size' : int(8),
    'num_layers' : int(2) 
}
with open('seq2seq','w') as f:
    config.write(f)
173/28:
import os
config.read(os.getcwd()+os.sep+'seq2seq',encoding='utf-8')
173/29:
from pytorch_forecasting.metrics import SMAPE
model = seq2seq(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
173/30:
class Encoder(nn.Module):
    def __init__(self,configs):
        self.input_size = int(configs['seq2seq']['input_size'])
        # input의 feature dimension을 넣어주어야 한다
        self.hidden_size = int(configs['seq2seq']['hidden_size'])
        # 내부에서 feature dimension을 어떻게 바꿔주고 싶은지 넣어주면 된다
        # 가령 mxn matrix가 입력으로 들어왔을때 hidden size를 h라 한다면 mxh의 크기로 바꾼다
        self.num_layers = int(configs['seq2seq']['num_layers'])
        self.lstm = nn.LSTM(input_size=self.input_size,hidden_size = self.hidden_size,
                           num_layers = self.num_layers,batch_first = True)

    
    def forward(self,x):
        lstm_out,hidden = self.lstm(x) #lstm의 output으로 나오는 hidden_state는 마지막 hidden_state값이다
        return lstm_out,hidden

class Decoder(nn.Module):
    def __init__(self,configs):
        super(Decoder,self).__init__()
        self.input_size = int(configs['seq2seq']['input_size'])
        # input의 feature dimension을 넣어주어야 한다
        self.hidden_size = int(configs['seq2seq']['hidden_size'])
        # 내부에서 feature dimension을 어떻게 바꿔주고 싶은지 넣어주면 된다
        # 가령 mxn matrix가 입력으로 들어왔을때 hidden size를 h라 한다면 mxh의 크기로 바꾼다
        self.num_layers = int(configs['seq2seq']['num_layers'])
        self.lstm = nn.LSTM(input_size=self.input_size,hidden_size = self.hidden_size,
                           num_layers = self.num_layers,batch_first = True)
        self.linear = nn.Linear(self.hidden_size,self.input_size)
        
    def forward(self,x_in,encoder_hidden):
        lstm_out,hidden = self.lstm(x,encoder_hidden)
        output = self.linear(lstm_out)
    # 정리하자면 우리가 정답을 알고 있는 시점에서 1시점 전까지의 데이터를 encoder로 넣고
    # 알고 있는 마지막시점 정보와 encoder에 넣어서 나온 hidden state를 넣어줌으로써 다음시점을 예측
class seq2seq(nn.Module):
    def __init__(self,configs):
        super(seq2seq,self).__init__()
        self.encoder = Encoder(configs)
        self.decoder = Decoder(configs)
        
    def forward(self,x):
        outputs = torch.zeros(x.size(0),1,1) # feature가1개고 target도 1개인 데이터
        _,final_hidden = self.encoder(x)
        output,_ = self.decoder(x_in = x[:,-1,:],encoder_hidden = final_hidden)
        
        return output
173/31:
from pytorch_forecasting.metrics import SMAPE
model = seq2seq(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
173/32:
class Encoder(nn.Module):
    def __init__(self,configs):
        self.input_size = int(configs['seq2seq']['input_size'])
        # input의 feature dimension을 넣어주어야 한다
        self.hidden_size = int(configs['seq2seq']['hidden_size'])
        # 내부에서 feature dimension을 어떻게 바꿔주고 싶은지 넣어주면 된다
        # 가령 mxn matrix가 입력으로 들어왔을때 hidden size를 h라 한다면 mxh의 크기로 바꾼다
        self.num_layers = int(configs['seq2seq']['num_layers'])
        self.lstm = nn.LSTM(input_size=self.input_size,hidden_size = self.hidden_size,
                           num_layers = self.num_layers,batch_first = True)

    
    def forward(self,x):
        lstm_out,hidden = self.lstm(x) #lstm의 output으로 나오는 hidden_state는 마지막 hidden_state값이다
        return lstm_out,hidden

class Decoder(nn.Module):
    def __init__(self,configs):
        super(Decoder,self).__init__()
        self.input_size = int(configs['seq2seq']['input_size'])
        # input의 feature dimension을 넣어주어야 한다
        self.hidden_size = int(configs['seq2seq']['hidden_size'])
        # 내부에서 feature dimension을 어떻게 바꿔주고 싶은지 넣어주면 된다
        # 가령 mxn matrix가 입력으로 들어왔을때 hidden size를 h라 한다면 mxh의 크기로 바꾼다
        self.num_layers = int(configs['seq2seq']['num_layers'])
        self.lstm = nn.LSTM(input_size=self.input_size,hidden_size = self.hidden_size,
                           num_layers = self.num_layers,batch_first = True)
        self.linear = nn.Linear(self.hidden_size,self.input_size)
        
    def forward(self,x_in,encoder_hidden):
        lstm_out,hidden = self.lstm(x,encoder_hidden)
        output = self.linear(lstm_out)
    # 정리하자면 우리가 정답을 알고 있는 시점에서 1시점 전까지의 데이터를 encoder로 넣고
    # 알고 있는 마지막시점 정보와 encoder에 넣어서 나온 hidden state를 넣어줌으로써 다음시점을 예측
173/33:
class seq2seq(nn.Module):
    def __init__(self,configs):
        super(seq2seq,self).__init__()
        self.encoder = Encoder(configs)
        self.decoder = Decoder(configs)
        
    def forward(self,x):
        outputs = torch.zeros(x.size(0),1,1) # feature가1개고 target도 1개인 데이터
        _,final_hidden = self.encoder(x)
        output,_ = self.decoder(x_in = x[:,-1,:],encoder_hidden = final_hidden)
        
        return output
173/34:
from pytorch_forecasting.metrics import SMAPE
model = seq2seq(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
173/35:
class Encoder(nn.Module):
    def __init__(self,configs):
        super(Encoder,self).__init__()
        self.input_size = int(configs['seq2seq']['input_size'])
        # input의 feature dimension을 넣어주어야 한다
        self.hidden_size = int(configs['seq2seq']['hidden_size'])
        # 내부에서 feature dimension을 어떻게 바꿔주고 싶은지 넣어주면 된다
        # 가령 mxn matrix가 입력으로 들어왔을때 hidden size를 h라 한다면 mxh의 크기로 바꾼다
        self.num_layers = int(configs['seq2seq']['num_layers'])
        self.lstm = nn.LSTM(input_size=self.input_size,hidden_size = self.hidden_size,
                           num_layers = self.num_layers,batch_first = True)

    
    def forward(self,x):
        lstm_out,hidden = self.lstm(x) #lstm의 output으로 나오는 hidden_state는 마지막 hidden_state값이다
        return lstm_out,hidden

class Decoder(nn.Module):
    def __init__(self,configs):
        super(Decoder,self).__init__()
        self.input_size = int(configs['seq2seq']['input_size'])
        # input의 feature dimension을 넣어주어야 한다
        self.hidden_size = int(configs['seq2seq']['hidden_size'])
        # 내부에서 feature dimension을 어떻게 바꿔주고 싶은지 넣어주면 된다
        # 가령 mxn matrix가 입력으로 들어왔을때 hidden size를 h라 한다면 mxh의 크기로 바꾼다
        self.num_layers = int(configs['seq2seq']['num_layers'])
        self.lstm = nn.LSTM(input_size=self.input_size,hidden_size = self.hidden_size,
                           num_layers = self.num_layers,batch_first = True)
        self.linear = nn.Linear(self.hidden_size,self.input_size)
        
    def forward(self,x_in,encoder_hidden):
        lstm_out,hidden = self.lstm(x,encoder_hidden)
        output = self.linear(lstm_out)
    # 정리하자면 우리가 정답을 알고 있는 시점에서 1시점 전까지의 데이터를 encoder로 넣고
    # 알고 있는 마지막시점 정보와 encoder에 넣어서 나온 hidden state를 넣어줌으로써 다음시점을 예측
173/36:
from pytorch_forecasting.metrics import SMAPE
model = seq2seq(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
173/37:
from pytorch_forecasting.metrics import SMAPE
model = seq2seq(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = Adam(model.parameters(),lr=1e-5)
173/38:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
173/39:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        seq.to(device)
        out = model(seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
173/40:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        seq=seq.to(device)
        out = model(seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
173/41:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        seq=seq.to(device)
        out = model(x=seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
173/42:
class Encoder(nn.Module):
    def __init__(self,configs):
        super(Encoder,self).__init__()
        self.input_size = int(configs['seq2seq']['input_size'])
        # input의 feature dimension을 넣어주어야 한다
        self.hidden_size = int(configs['seq2seq']['hidden_size'])
        # 내부에서 feature dimension을 어떻게 바꿔주고 싶은지 넣어주면 된다
        # 가령 mxn matrix가 입력으로 들어왔을때 hidden size를 h라 한다면 mxh의 크기로 바꾼다
        self.num_layers = int(configs['seq2seq']['num_layers'])
        self.lstm = nn.LSTM(input_size=self.input_size,hidden_size = self.hidden_size,
                           num_layers = self.num_layers,batch_first = True)

    
    def forward(self,x):
        lstm_out,hidden = self.lstm(x) #lstm의 output으로 나오는 hidden_state는 마지막 hidden_state값이다
        return lstm_out,hidden

class Decoder(nn.Module):
    def __init__(self,configs):
        super(Decoder,self).__init__()
        self.input_size = int(configs['seq2seq']['input_size'])
        # input의 feature dimension을 넣어주어야 한다
        self.hidden_size = int(configs['seq2seq']['hidden_size'])
        # 내부에서 feature dimension을 어떻게 바꿔주고 싶은지 넣어주면 된다
        # 가령 mxn matrix가 입력으로 들어왔을때 hidden size를 h라 한다면 mxh의 크기로 바꾼다
        self.num_layers = int(configs['seq2seq']['num_layers'])
        self.lstm = nn.LSTM(input_size=self.input_size,hidden_size = self.hidden_size,
                           num_layers = self.num_layers,batch_first = True)
        self.linear = nn.Linear(self.hidden_size,self.input_size)
        
    def forward(self,x,encoder_hidden):
        lstm_out,hidden = self.lstm(x,encoder_hidden)
        output = self.linear(lstm_out)
    # 정리하자면 우리가 정답을 알고 있는 시점에서 1시점 전까지의 데이터를 encoder로 넣고
    # 알고 있는 마지막시점 정보와 encoder에 넣어서 나온 hidden state를 넣어줌으로써 다음시점을 예측
173/43:
class seq2seq(nn.Module):
    def __init__(self,configs):
        super(seq2seq,self).__init__()
        self.encoder = Encoder(configs)
        self.decoder = Decoder(configs)
        
    def forward(self,x):
        outputs = torch.zeros(x.size(0),1,1) # feature가1개고 target도 1개인 데이터
        _,final_hidden = self.encoder(x)
        output,_ = self.decoder(x_in = x[:,-1,:],encoder_hidden = final_hidden)
        
        return output
173/44:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        seq=seq.to(device)
        out = model(x=seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
173/45:
class seq2seq(nn.Module):
    def __init__(self,configs):
        super(seq2seq,self).__init__()
        self.encoder = Encoder(configs)
        self.decoder = Decoder(configs)
        
    def forward(self,x):
        outputs = torch.zeros(x.size(0),1,1) # feature가1개고 target도 1개인 데이터
        _,final_hidden = self.encoder(x)
        output,_ = self.decoder(x = x[:,-1,:],encoder_hidden = final_hidden)
        
        return output
173/46:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        seq=seq.to(device)
        out = model(x=seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
173/47:
from pytorch_forecasting.metrics import SMAPE
model = seq2seq(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = Adam(model.parameters(),lr=1e-5)
173/48:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        seq=seq.to(device)
        out = model(x=seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
173/49:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        #seq=seq.to(device)
        out = model(x=seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
173/50:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        seq=seq.to(device)
        out = model(x=seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
173/51:
class seq2seq(nn.Module):
    def __init__(self,configs):
        super(seq2seq,self).__init__()
        self.encoder = Encoder(configs)
        self.decoder = Decoder(configs)
        
    def forward(self,x):
        outputs = torch.zeros(x.size(0),1,1) # feature가1개고 target도 1개인 데이터
        _,final_hidden = self.encoder(x)
        print(x[:,-1,:])
        output,_ = self.decoder(x = x[:,-1,:].unsqueeze,encoder_hidden = final_hidden)
        
        return output
173/52:
from pytorch_forecasting.metrics import SMAPE
model = seq2seq(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = Adam(model.parameters(),lr=1e-5)
173/53:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        seq=seq.to(device)
        out = model(x=seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
173/54:
class seq2seq(nn.Module):
    def __init__(self,configs):
        super(seq2seq,self).__init__()
        self.encoder = Encoder(configs)
        self.decoder = Decoder(configs)
        
    def forward(self,x):
        outputs = torch.zeros(x.size(0),1,1) # feature가1개고 target도 1개인 데이터
        _,final_hidden = self.encoder(x)
        output,_ = self.decoder(x = x[:,-1,:].unsqueeze(2).unsqueeze,encoder_hidden = final_hidden)
        
        return output
173/55:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        seq=seq.to(device)
        out = model(x=seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
173/56:
from pytorch_forecasting.metrics import SMAPE
model = seq2seq(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = Adam(model.parameters(),lr=1e-5)
173/57:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        seq=seq.to(device)
        out = model(x=seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
173/58:
class seq2seq(nn.Module):
    def __init__(self,configs):
        super(seq2seq,self).__init__()
        self.encoder = Encoder(configs)
        self.decoder = Decoder(configs)
        
    def forward(self,x):
        outputs = torch.zeros(x.size(0),1,1) # feature가1개고 target도 1개인 데이터
        _,final_hidden = self.encoder(x)
        output,_ = self.decoder(x = x[:,-1,:].unsqueeze(-1).unsqueeze,encoder_hidden = final_hidden)
        
        return output
173/59:
from pytorch_forecasting.metrics import SMAPE
model = seq2seq(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = Adam(model.parameters(),lr=1e-5)
173/60:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        seq=seq.to(device)
        out = model(x=seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
173/61: device = torch.device('cuda:0')
173/62:
class seq2seq(nn.Module):
    def __init__(self,configs):
        super(seq2seq,self).__init__()
        self.encoder = Encoder(configs)
        self.decoder = Decoder(configs)
        
    def forward(self,x):
        outputs = torch.zeros(x.size(0),1,1) # feature가1개고 target도 1개인 데이터
        _,final_hidden = self.encoder(x)
        output,_ = (self.decoder(x = x[:,-1,:].cpu().unsqueeze(-1)).to(device).unsqueeze,encoder_hidden = final_hidden)
        
        return output
173/63:
class seq2seq(nn.Module):
    def __init__(self,configs):
        super(seq2seq,self).__init__()
        self.encoder = Encoder(configs)
        self.decoder = Decoder(configs)
        
    def forward(self,x):
        outputs = torch.zeros(x.size(0),1,1) # feature가1개고 target도 1개인 데이터
        _,final_hidden = self.encoder(x)
        output,_ = self.decoder((x = x[:,-1,:].cpu().unsqueeze(-1)).to(device).unsqueeze,encoder_hidden = final_hidden)
        
        return output
173/64:
class seq2seq(nn.Module):
    def __init__(self,configs):
        super(seq2seq,self).__init__()
        self.encoder = Encoder(configs)
        self.decoder = Decoder(configs)
        
    def forward(self,x):
        outputs = torch.zeros(x.size(0),1,1) # feature가1개고 target도 1개인 데이터
        _,final_hidden = self.encoder(x)
        output,_ = self.decoder(x = (x[:,-1,:].cpu().unsqueeze(-1)).to(device).unsqueeze,encoder_hidden = final_hidden)
        
        return output
173/65:
from pytorch_forecasting.metrics import SMAPE
model = seq2seq(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = Adam(model.parameters(),lr=1e-5)
173/66:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        seq=seq.to(device)
        out = model(x=seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
173/67:
class seq2seq(nn.Module):
    def __init__(self,configs):
        super(seq2seq,self).__init__()
        self.encoder = Encoder(configs)
        self.decoder = Decoder(configs)
        
    def forward(self,x):
        outputs = torch.zeros(x.size(0),1,1) # feature가1개고 target도 1개인 데이터
        print(x)
        _,final_hidden = self.encoder(x)
        output,_ = self.decoder(x = (x[:,-1,:].cpu().unsqueeze(-1)).unsqueeze,encoder_hidden = final_hidden)
        
        return output
173/68:
from pytorch_forecasting.metrics import SMAPE
model = seq2seq(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = Adam(model.parameters(),lr=1e-5)
173/69:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        seq=seq.to(device)
        out = model(x=seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
173/70:
class seq2seq(nn.Module):
    def __init__(self,configs):
        super(seq2seq,self).__init__()
        self.encoder = Encoder(configs)
        self.decoder = Decoder(configs)
        
    def forward(self,x):
        outputs = torch.zeros(x.size(0),1,1) # feature가1개고 target도 1개인 데이터
        _,final_hidden = self.encoder(x)
        output,_ = self.decoder(x = (x[:,-1,:].cpu().unsqueeze(-1)).to(device),encoder_hidden = final_hidden)
        
        return output
173/71:
from pytorch_forecasting.metrics import SMAPE
model = seq2seq(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = Adam(model.parameters(),lr=1e-5)
173/72:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        seq=seq.to(device)
        out = model(x=seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
173/73:
class seq2seq(nn.Module):
    def __init__(self,configs):
        super(seq2seq,self).__init__()
        self.encoder = Encoder(configs)
        self.decoder = Decoder(configs)
        
    def forward(self,x):
        outputs = torch.zeros(x.size(0),1,1) # feature가1개고 target도 1개인 데이터
        _,final_hidden = self.encoder(x)
        print(type(x))
        print(type((x[:,-1,:].cpu().unsqueeze(-1)).to(device)))
        output,_ = self.decoder(x = (x[:,-1,:].cpu().unsqueeze(-1)).to(device),encoder_hidden = final_hidden)
        
        return output
173/74:
from pytorch_forecasting.metrics import SMAPE
model = seq2seq(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = Adam(model.parameters(),lr=1e-5)
173/75:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        seq=seq.to(device)
        out = model(x=seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
173/76:
class seq2seq(nn.Module):
    def __init__(self,configs):
        super(seq2seq,self).__init__()
        self.encoder = Encoder(configs)
        self.decoder = Decoder(configs)
        
    def forward(self,x):
        outputs = torch.zeros(x.size(0),1,1) # feature가1개고 target도 1개인 데이터
        _,final_hidden = self.encoder(x)
        print(type(x))
        print(type((x[:,-1,:].cpu().unsqueeze(-1)).to(device)))
        print(final_hidden)
        print(type(final_hidden))
        output,_ = self.decoder(x = (x[:,-1,:].cpu().unsqueeze(-1)).to(device),encoder_hidden = final_hidden)
        
        return output
173/77:
from pytorch_forecasting.metrics import SMAPE
model = seq2seq(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = Adam(model.parameters(),lr=1e-5)
173/78:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        seq=seq.to(device)
        out = model(x=seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
173/79:
class Encoder(nn.Module):
    def __init__(self,configs):
        super(Encoder,self).__init__()
        self.input_size = int(configs['seq2seq']['input_size'])
        # input의 feature dimension을 넣어주어야 한다
        self.hidden_size = int(configs['seq2seq']['hidden_size'])
        # 내부에서 feature dimension을 어떻게 바꿔주고 싶은지 넣어주면 된다
        # 가령 mxn matrix가 입력으로 들어왔을때 hidden size를 h라 한다면 mxh의 크기로 바꾼다
        self.num_layers = int(configs['seq2seq']['num_layers'])
        self.lstm = nn.LSTM(input_size=self.input_size,hidden_size = self.hidden_size,
                           num_layers = self.num_layers,batch_first = True)

    
    def forward(self,x):
        lstm_out,hidden = self.lstm(x) #lstm의 output으로 나오는 hidden_state는 마지막 hidden_state값이다
        return lstm_out,hidden

class Decoder(nn.Module):
    def __init__(self,configs):
        super(Decoder,self).__init__()
        self.input_size = int(configs['seq2seq']['input_size'])
        # input의 feature dimension을 넣어주어야 한다
        self.hidden_size = int(configs['seq2seq']['hidden_size'])
        # 내부에서 feature dimension을 어떻게 바꿔주고 싶은지 넣어주면 된다
        # 가령 mxn matrix가 입력으로 들어왔을때 hidden size를 h라 한다면 mxh의 크기로 바꾼다
        self.num_layers = int(configs['seq2seq']['num_layers'])
        self.lstm = nn.LSTM(input_size=self.input_size,hidden_size = self.hidden_size,
                           num_layers = self.num_layers,batch_first = True)
        self.linear = nn.Linear(self.hidden_size,self.input_size)
        
    def forward(self,x,encoder_hidden):
        lstm_out,hidden = self.lstm(x,encoder_hidden)
        output = self.linear(lstm_out)
        
        return output
    # 정리하자면 우리가 정답을 알고 있는 시점에서 1시점 전까지의 데이터를 encoder로 넣고
    # 알고 있는 마지막시점 정보와 encoder에 넣어서 나온 hidden state를 넣어줌으로써 다음시점을 예측
173/80:
class seq2seq(nn.Module):
    def __init__(self,configs):
        super(seq2seq,self).__init__()
        self.encoder = Encoder(configs)
        self.decoder = Decoder(configs)
        
    def forward(self,x):
        outputs = torch.zeros(x.size(0),1,1) # feature가1개고 target도 1개인 데이터
        _,final_hidden = self.encoder(x)
        print(type(x))
        print(type((x[:,-1,:].cpu().unsqueeze(-1)).to(device)))
        print(final_hidden)
        print(type(final_hidden))
        output,_ = self.decoder(x = (x[:,-1,:].cpu().unsqueeze(-1)).to(device),encoder_hidden = final_hidden)
        
        return output
173/81:
from pytorch_forecasting.metrics import SMAPE
model = seq2seq(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = Adam(model.parameters(),lr=1e-5)
173/82:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        seq=seq.to(device)
        out = model(x=seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
173/83:
class seq2seq(nn.Module):
    def __init__(self,configs):
        super(seq2seq,self).__init__()
        self.encoder = Encoder(configs)
        self.decoder = Decoder(configs)
        
    def forward(self,x):
        outputs = torch.zeros(x.size(0),1,1) # feature가1개고 target도 1개인 데이터
        _,final_hidden = self.encoder(x)
        output,_ = self.decoder(x = (x[:,-1,:].cpu().unsqueeze(-1)).to(device),encoder_hidden = final_hidden)
        
        return output
173/84:
from pytorch_forecasting.metrics import SMAPE
model = seq2seq(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = Adam(model.parameters(),lr=1e-5)
173/85:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        seq=seq.to(device)
        out = model(x=seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
173/86:
import configparser
config = configparser.ConfigParser()
config['seq2seq']= {
    'input_size' : int(1),
    'hidden_size' : int(8),
    'num_layers' : int(1) 
}
with open('seq2seq','w') as f:
    config.write(f)
173/87:
import os
config.read(os.getcwd()+os.sep+'seq2seq',encoding='utf-8')
173/88:
from pytorch_forecasting.metrics import SMAPE
model = seq2seq(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = Adam(model.parameters(),lr=1e-5)
173/89:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        seq=seq.to(device)
        out = model(x=seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
173/90:
class seq2seq(nn.Module):
    def __init__(self,configs):
        super(seq2seq,self).__init__()
        self.encoder = Encoder(configs)
        self.decoder = Decoder(configs)
        
    def forward(self,x):
        outputs = torch.zeros(x.size(0),1,1) # feature가1개고 target도 1개인 데이터
        _,final_hidden = self.encoder(x)
        print(final_hidden)
        output,_ = self.decoder(x = (x[:,-1,:].cpu().unsqueeze(-1)).to(device),encoder_hidden = final_hidden)
        
        return output
173/91:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        seq=seq.to(device)
        out = model(x=seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
173/92:
from pytorch_forecasting.metrics import SMAPE
model = seq2seq(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = Adam(model.parameters(),lr=1e-5)
173/93:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        seq=seq.to(device)
        out = model(x=seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
173/94:
class Encoder(nn.Module):
    def __init__(self,configs):
        super(Encoder,self).__init__()
        self.input_size = int(configs['seq2seq']['input_size'])
        # input의 feature dimension을 넣어주어야 한다
        self.hidden_size = int(configs['seq2seq']['hidden_size'])
        # 내부에서 feature dimension을 어떻게 바꿔주고 싶은지 넣어주면 된다
        # 가령 mxn matrix가 입력으로 들어왔을때 hidden size를 h라 한다면 mxh의 크기로 바꾼다
        self.num_layers = int(configs['seq2seq']['num_layers'])
        self.lstm = nn.LSTM(input_size=self.input_size,hidden_size = self.hidden_size,
                           num_layers = self.num_layers,batch_first = True)

    
    def forward(self,x):
        lstm_out,hidden = self.lstm(x) #lstm의 output으로 나오는 hidden_state는 마지막 hidden_state값이다
        return lstm_out,hidden

class Decoder(nn.Module):
    def __init__(self,configs):
        super(Decoder,self).__init__()
        self.input_size = int(configs['seq2seq']['input_size'])
        # input의 feature dimension을 넣어주어야 한다
        self.hidden_size = int(configs['seq2seq']['hidden_size'])
        # 내부에서 feature dimension을 어떻게 바꿔주고 싶은지 넣어주면 된다
        # 가령 mxn matrix가 입력으로 들어왔을때 hidden size를 h라 한다면 mxh의 크기로 바꾼다
        self.num_layers = int(configs['seq2seq']['num_layers'])
        self.lstm = nn.LSTM(input_size=self.input_size,hidden_size = self.hidden_size,
                           num_layers = self.num_layers,batch_first = True)
        self.linear = nn.Linear(self.hidden_size,self.input_size)
        
    def forward(self,x,encoder_hidden,encoder_cell):
        lstm_out,hidden = self.lstm(x,h_0=encoder_hidden,c_0=encoder_cell)
        output = self.linear(lstm_out)
        
        return output,hidden
    # 정리하자면 우리가 정답을 알고 있는 시점에서 1시점 전까지의 데이터를 encoder로 넣고
    # 알고 있는 마지막시점 정보와 encoder에 넣어서 나온 hidden state를 넣어줌으로써 다음시점을 예측
173/95: device = torch.device('cuda:0')
173/96:
class seq2seq(nn.Module):
    def __init__(self,configs):
        super(seq2seq,self).__init__()
        self.encoder = Encoder(configs)
        self.decoder = Decoder(configs)
        
    def forward(self,x):
        outputs = torch.zeros(x.size(0),1,1) # feature가1개고 target도 1개인 데이터
        _,final_hidden = self.encoder(x)
        print(final_hidden)
        output,_ = self.decoder(x = (x[:,-1,:].cpu().unsqueeze(-1)).to(device),encoder_hidden = final_hidden[0],
                                encoder_cell = final_hidden[1])
        
        return output
173/97:
from pytorch_forecasting.metrics import SMAPE
model = seq2seq(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = Adam(model.parameters(),lr=1e-5)
173/98:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        seq=seq.to(device)
        out = model(x=seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
173/99:
class Encoder(nn.Module):
    def __init__(self,configs):
        super(Encoder,self).__init__()
        self.input_size = int(configs['seq2seq']['input_size'])
        # input의 feature dimension을 넣어주어야 한다
        self.hidden_size = int(configs['seq2seq']['hidden_size'])
        # 내부에서 feature dimension을 어떻게 바꿔주고 싶은지 넣어주면 된다
        # 가령 mxn matrix가 입력으로 들어왔을때 hidden size를 h라 한다면 mxh의 크기로 바꾼다
        self.num_layers = int(configs['seq2seq']['num_layers'])
        self.lstm = nn.LSTM(input_size=self.input_size,hidden_size = self.hidden_size,
                           num_layers = self.num_layers,batch_first = True)

    
    def forward(self,x):
        lstm_out,hidden = self.lstm(x) #lstm의 output으로 나오는 hidden_state는 마지막 hidden_state값이다
        return lstm_out,hidden

class Decoder(nn.Module):
    def __init__(self,configs):
        super(Decoder,self).__init__()
        self.input_size = int(configs['seq2seq']['input_size'])
        # input의 feature dimension을 넣어주어야 한다
        self.hidden_size = int(configs['seq2seq']['hidden_size'])
        # 내부에서 feature dimension을 어떻게 바꿔주고 싶은지 넣어주면 된다
        # 가령 mxn matrix가 입력으로 들어왔을때 hidden size를 h라 한다면 mxh의 크기로 바꾼다
        self.num_layers = int(configs['seq2seq']['num_layers'])
        self.lstm = nn.LSTM(input_size=self.input_size,hidden_size = self.hidden_size,
                           num_layers = self.num_layers,batch_first = True)
        self.linear = nn.Linear(self.hidden_size,self.input_size)
        
    def forward(self,x,encoder_hidden,encoder_cell):
        lstm_out,hidden = self.lstm(x,(h_0=encoder_hidden,c_0=encoder_cell))
        output = self.linear(lstm_out)
        
        return output,hidden
    # 정리하자면 우리가 정답을 알고 있는 시점에서 1시점 전까지의 데이터를 encoder로 넣고
    # 알고 있는 마지막시점 정보와 encoder에 넣어서 나온 hidden state를 넣어줌으로써 다음시점을 예측
173/100:
class Encoder(nn.Module):
    def __init__(self,configs):
        super(Encoder,self).__init__()
        self.input_size = int(configs['seq2seq']['input_size'])
        # input의 feature dimension을 넣어주어야 한다
        self.hidden_size = int(configs['seq2seq']['hidden_size'])
        # 내부에서 feature dimension을 어떻게 바꿔주고 싶은지 넣어주면 된다
        # 가령 mxn matrix가 입력으로 들어왔을때 hidden size를 h라 한다면 mxh의 크기로 바꾼다
        self.num_layers = int(configs['seq2seq']['num_layers'])
        self.lstm = nn.LSTM(input_size=self.input_size,hidden_size = self.hidden_size,
                           num_layers = self.num_layers,batch_first = True)

    
    def forward(self,x):
        lstm_out,hidden = self.lstm(x) #lstm의 output으로 나오는 hidden_state는 마지막 hidden_state값이다
        return lstm_out,hidden

class Decoder(nn.Module):
    def __init__(self,configs):
        super(Decoder,self).__init__()
        self.input_size = int(configs['seq2seq']['input_size'])
        # input의 feature dimension을 넣어주어야 한다
        self.hidden_size = int(configs['seq2seq']['hidden_size'])
        # 내부에서 feature dimension을 어떻게 바꿔주고 싶은지 넣어주면 된다
        # 가령 mxn matrix가 입력으로 들어왔을때 hidden size를 h라 한다면 mxh의 크기로 바꾼다
        self.num_layers = int(configs['seq2seq']['num_layers'])
        self.lstm = nn.LSTM(input_size=self.input_size,hidden_size = self.hidden_size,
                           num_layers = self.num_layers,batch_first = True)
        self.linear = nn.Linear(self.hidden_size,self.input_size)
        
    def forward(self,x,encoder_hidden,encoder_cell):
        lstm_out,hidden = self.lstm(x,(encoder_hidden,encoder_cell))
        output = self.linear(lstm_out)
        
        return output,hidden
    # 정리하자면 우리가 정답을 알고 있는 시점에서 1시점 전까지의 데이터를 encoder로 넣고
    # 알고 있는 마지막시점 정보와 encoder에 넣어서 나온 hidden state를 넣어줌으로써 다음시점을 예측
173/101:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        seq=seq.to(device)
        out = model(x=seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
173/102:
from pytorch_forecasting.metrics import SMAPE
model = seq2seq(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = Adam(model.parameters(),lr=1e-5)
173/103:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        seq=seq.to(device)
        out = model(x=seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
173/104:
class Encoder(nn.Module):
    def __init__(self,configs):
        super(Encoder,self).__init__()
        self.input_size = int(configs['seq2seq']['input_size'])
        # input의 feature dimension을 넣어주어야 한다
        self.hidden_size = int(configs['seq2seq']['hidden_size'])
        # 내부에서 feature dimension을 어떻게 바꿔주고 싶은지 넣어주면 된다
        # 가령 mxn matrix가 입력으로 들어왔을때 hidden size를 h라 한다면 mxh의 크기로 바꾼다
        self.num_layers = int(configs['seq2seq']['num_layers'])
        self.lstm = nn.LSTM(input_size=self.input_size,hidden_size = self.hidden_size,
                           num_layers = self.num_layers,batch_first = True)

    
    def forward(self,x):
        lstm_out,hidden = self.lstm(x) #lstm의 output으로 나오는 hidden_state는 마지막 hidden_state값이다
        return lstm_out,hidden

class Decoder(nn.Module):
    def __init__(self,configs):
        super(Decoder,self).__init__()
        self.input_size = int(configs['seq2seq']['input_size'])
        # input의 feature dimension을 넣어주어야 한다
        self.hidden_size = int(configs['seq2seq']['hidden_size'])
        # 내부에서 feature dimension을 어떻게 바꿔주고 싶은지 넣어주면 된다
        # 가령 mxn matrix가 입력으로 들어왔을때 hidden size를 h라 한다면 mxh의 크기로 바꾼다
        self.num_layers = int(configs['seq2seq']['num_layers'])
        self.lstm = nn.LSTM(input_size=self.input_size,hidden_size = self.hidden_size,
                           num_layers = self.num_layers,batch_first = True)
        self.linear = nn.Linear(self.hidden_size,self.input_size)
        
    def forward(self,x,encoder_hidden,encoder_cell):
        lstm_out,hidden = self.lstm(x,(encoder_hidden,encoder_cell))
        output = self.linear(lstm_out)
        
        return output,hidden
    # 정리하자면 우리가 정답을 알고 있는 시점에서 1시점 전까지의 데이터를 encoder로 넣고
    # 알고 있는 마지막시점 정보와 encoder에 넣어서 나온 hidden state를 넣어줌으로써 다음시점을 예측
173/105: device = torch.device('cuda:0')
173/106:
class seq2seq(nn.Module):
    def __init__(self,configs):
        super(seq2seq,self).__init__()
        self.encoder = Encoder(configs)
        self.decoder = Decoder(configs)
        
    def forward(self,x):
        outputs = torch.zeros(x.size(0),1,1) # feature가1개고 target도 1개인 데이터
        _,final_hidden = self.encoder(x)
        #print(final_hidden)
        output,_ = self.decoder(x = (x[:,-1,:].cpu().unsqueeze(-1)).to(device),encoder_hidden = final_hidden[0],
                                encoder_cell = final_hidden[1])
        
        return output
173/107:
def seq_data(x,y,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length])
#             if i == 0:
#                  print(x.iloc[i:i+sequence_length].values)
#                  print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).view(-1,sequence_length,1),FloatTensor(target_list).view(-1,1,1)
    # DataLoader로 batch_size만큼 데이터를 쪼개고 싶다면 [Batch, Input_length, Channel] 형태를 따라줘야됨
173/108:
from pytorch_forecasting.metrics import SMAPE
model = seq2seq(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = Adam(model.parameters(),lr=1e-5)
173/109:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        seq=seq.to(device)
        out = model(x=seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
173/110:
class Encoder(nn.Module):
    def __init__(self,configs):
        super(Encoder,self).__init__()
        self.input_size = int(configs['seq2seq']['input_size'])
        # input의 feature dimension을 넣어주어야 한다
        self.hidden_size = int(configs['seq2seq']['hidden_size'])
        # 내부에서 feature dimension을 어떻게 바꿔주고 싶은지 넣어주면 된다
        # 가령 mxn matrix가 입력으로 들어왔을때 hidden size를 h라 한다면 mxh의 크기로 바꾼다
        self.num_layers = int(configs['seq2seq']['num_layers'])
        self.lstm = nn.LSTM(input_size=self.input_size,hidden_size = self.hidden_size,
                           num_layers = self.num_layers,batch_first = True)

    
    def forward(self,x):
        lstm_out,hidden = self.lstm(x) #lstm의 output으로 나오는 hidden_state는 마지막 hidden_state값이다
        return lstm_out,hidden

class Decoder(nn.Module):
    def __init__(self,configs):
        super(Decoder,self).__init__()
        self.input_size = int(configs['seq2seq']['input_size'])
        # input의 feature dimension을 넣어주어야 한다
        self.hidden_size = int(configs['seq2seq']['hidden_size'])
        # 내부에서 feature dimension을 어떻게 바꿔주고 싶은지 넣어주면 된다
        # 가령 mxn matrix가 입력으로 들어왔을때 hidden size를 h라 한다면 mxh의 크기로 바꾼다
        self.num_layers = int(configs['seq2seq']['num_layers'])
        self.lstm = nn.LSTM(input_size=self.input_size,hidden_size = self.hidden_size,
                           num_layers = self.num_layers,batch_first = True)
        self.linear = nn.Linear(self.hidden_size,self.input_size)
        
    def forward(self,x,encoder_hidden,encoder_cell):
        lstm_out,hidden = self.lstm(x,(encoder_hidden,encoder_cell))
        output = self.linear(lstm_out)
        
        return output,hidden
    # 정리하자면 우리가 정답을 알고 있는 시점에서 1시점 전까지의 데이터를 encoder로 넣고
    # 알고 있는 마지막시점 정보와 encoder에 넣어서 나온 hidden state를 넣어줌으로써 다음시점을 예측
173/111: device = torch.device('cuda:0')
173/112:
class seq2seq(nn.Module):
    def __init__(self,configs):
        super(seq2seq,self).__init__()
        self.encoder = Encoder(configs)
        self.decoder = Decoder(configs)
        
    def forward(self,x):
        outputs = torch.zeros(x.size(0),1,1) # feature가1개고 target도 1개인 데이터
        _,final_hidden = self.encoder(x)
        #print(final_hidden)
        output,_ = self.decoder(x = (x[:,-1,:].cpu().unsqueeze(-1)).to(device),encoder_hidden = final_hidden[0],
                                encoder_cell = final_hidden[1])
        
        return output
173/113:
def seq_data(x,y,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length])
#             if i == 0:
#                  print(x.iloc[i:i+sequence_length].values)
#                  print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).view(-1,sequence_length,1),FloatTensor(target_list).view(-1,1,1)
    # DataLoader로 batch_size만큼 데이터를 쪼개고 싶다면 [Batch, Input_length, Channel] 형태를 따라줘야됨
173/114:
torch_data = TensorDataset(x_seq,target)
batch_size = 64
train_loader = DataLoader(torch_data,batch_size = batch_size)
173/115:
from pytorch_forecasting.metrics import SMAPE
model = seq2seq(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = Adam(model.parameters(),lr=1e-3)
173/116:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        seq=seq.to(device)
        out = model(x=seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
173/117:
torch_data = TensorDataset(x_seq,target)
batch_size = 64
train_loader = DataLoader(torch_data,batch_size = batch_size)
173/118:
from pytorch_forecasting.metrics import SMAPE
model = seq2seq(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = Adam(model.parameters(),lr=1e-5)
173/119:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        seq=seq.to(device)
        
        out = model(x=seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
173/120:
from pytorch_forecasting.metrics import SMAPE
model = seq2seq(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = Adam(model.parameters(),lr=1e-2)
173/121:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        seq=seq.to(device)
        
        out = model(x=seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
173/122:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        seq=seq.to(device)
        
        out = model(x=seq)
        print(out)
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
173/123:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        seq=seq.to(device)
        
        out = model(x=seq)
        print(out.size())
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
173/124:
import configparser
config = configparser.ConfigParser()
config['seq2seq']= {
    'input_size' : int(2),
    'hidden_size' : int(8),
    'num_layers' : int(2) 
}
with open('seq2seq','w') as f:
    config.write(f)
173/125:
import os
config.read(os.getcwd()+os.sep+'seq2seq',encoding='utf-8')
173/126:
class Encoder(nn.Module):
    def __init__(self,configs):
        super(Encoder,self).__init__()
        self.input_size = int(configs['seq2seq']['input_size'])
        # input의 feature dimension을 넣어주어야 한다
        self.hidden_size = int(configs['seq2seq']['hidden_size'])
        # 내부에서 feature dimension을 어떻게 바꿔주고 싶은지 넣어주면 된다
        # 가령 mxn matrix가 입력으로 들어왔을때 hidden size를 h라 한다면 mxh의 크기로 바꾼다
        self.num_layers = int(configs['seq2seq']['num_layers'])
        self.lstm = nn.LSTM(input_size=self.input_size,hidden_size = self.hidden_size,
                           num_layers = self.num_layers,batch_first = True)

    
    def forward(self,x):
        lstm_out,hidden = self.lstm(x) #lstm의 output으로 나오는 hidden_state는 마지막 hidden_state값이다
        return lstm_out,hidden

class Decoder(nn.Module):
    def __init__(self,configs):
        super(Decoder,self).__init__()
        self.input_size = int(configs['seq2seq']['input_size'])
        # input의 feature dimension을 넣어주어야 한다
        self.hidden_size = int(configs['seq2seq']['hidden_size'])
        # 내부에서 feature dimension을 어떻게 바꿔주고 싶은지 넣어주면 된다
        # 가령 mxn matrix가 입력으로 들어왔을때 hidden size를 h라 한다면 mxh의 크기로 바꾼다
        self.num_layers = int(configs['seq2seq']['num_layers'])
        self.lstm = nn.LSTM(input_size=self.input_size,hidden_size = self.hidden_size,
                           num_layers = self.num_layers,batch_first = True)
        self.linear = nn.Linear(self.hidden_size,self.input_size)
        
    def forward(self,x,encoder_hidden,encoder_cell):
        lstm_out,hidden = self.lstm(x,(encoder_hidden,encoder_cell))
        output = self.linear(lstm_out)
        
        return output,hidden
    # 정리하자면 우리가 정답을 알고 있는 시점에서 1시점 전까지의 데이터를 encoder로 넣고
    # 알고 있는 마지막시점 정보와 encoder에 넣어서 나온 hidden state를 넣어줌으로써 다음시점을 예측
173/127: device = torch.device('cuda:0')
173/128:
class seq2seq(nn.Module):
    def __init__(self,configs):
        super(seq2seq,self).__init__()
        self.encoder = Encoder(configs)
        self.decoder = Decoder(configs)
        
    def forward(self,x):
        outputs = torch.zeros(x.size(0),1,1) # feature가1개고 target도 1개인 데이터
        _,final_hidden = self.encoder(x)
        #print(final_hidden)
        output,_ = self.decoder(x = (x[:,-1,:].cpu().unsqueeze(-1)).to(device),encoder_hidden = final_hidden[0],
                                encoder_cell = final_hidden[1])
        
        return output
173/129:
def seq_data(x,y,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length])
#             if i == 0:
#                  print(x.iloc[i:i+sequence_length].values)
#                  print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).view(-1,sequence_length,1),FloatTensor(target_list).view(-1,1,1)
    # DataLoader로 batch_size만큼 데이터를 쪼개고 싶다면 [Batch, Input_length, Channel] 형태를 따라줘야됨
173/130:
from pytorch_forecasting.metrics import SMAPE
model = seq2seq(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = Adam(model.parameters(),lr=1e-2)
173/131:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        seq=seq.to(device)
        
        out = model(x=seq)
        print(out.size())
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
173/132:
import configparser
config = configparser.ConfigParser()
config['seq2seq']= {
    'input_size' : int(1),
    'hidden_size' : int(1),
    'num_layers' : int(2) 
}
with open('seq2seq','w') as f:
    config.write(f)
173/133:
import os
config.read(os.getcwd()+os.sep+'seq2seq',encoding='utf-8')
173/134:
class Encoder(nn.Module):
    def __init__(self,configs):
        super(Encoder,self).__init__()
        self.input_size = int(configs['seq2seq']['input_size'])
        # input의 feature dimension을 넣어주어야 한다
        self.hidden_size = int(configs['seq2seq']['hidden_size'])
        # 내부에서 feature dimension을 어떻게 바꿔주고 싶은지 넣어주면 된다
        # 가령 mxn matrix가 입력으로 들어왔을때 hidden size를 h라 한다면 mxh의 크기로 바꾼다
        self.num_layers = int(configs['seq2seq']['num_layers'])
        self.lstm = nn.LSTM(input_size=self.input_size,hidden_size = self.hidden_size,
                           num_layers = self.num_layers,batch_first = True)

    
    def forward(self,x):
        lstm_out,hidden = self.lstm(x) #lstm의 output으로 나오는 hidden_state는 마지막 hidden_state값이다
        return lstm_out,hidden

class Decoder(nn.Module):
    def __init__(self,configs):
        super(Decoder,self).__init__()
        self.input_size = int(configs['seq2seq']['input_size'])
        # input의 feature dimension을 넣어주어야 한다
        self.hidden_size = int(configs['seq2seq']['hidden_size'])
        # 내부에서 feature dimension을 어떻게 바꿔주고 싶은지 넣어주면 된다
        # 가령 mxn matrix가 입력으로 들어왔을때 hidden size를 h라 한다면 mxh의 크기로 바꾼다
        self.num_layers = int(configs['seq2seq']['num_layers'])
        self.lstm = nn.LSTM(input_size=self.input_size,hidden_size = self.hidden_size,
                           num_layers = self.num_layers,batch_first = True)
        self.linear = nn.Linear(self.hidden_size,self.input_size)
        
    def forward(self,x,encoder_hidden,encoder_cell):
        lstm_out,hidden = self.lstm(x,(encoder_hidden,encoder_cell))
        output = self.linear(lstm_out)
        
        return output,hidden
    # 정리하자면 우리가 정답을 알고 있는 시점에서 1시점 전까지의 데이터를 encoder로 넣고
    # 알고 있는 마지막시점 정보와 encoder에 넣어서 나온 hidden state를 넣어줌으로써 다음시점을 예측
173/135: device = torch.device('cuda:0')
173/136:
class seq2seq(nn.Module):
    def __init__(self,configs):
        super(seq2seq,self).__init__()
        self.encoder = Encoder(configs)
        self.decoder = Decoder(configs)
        
    def forward(self,x):
        outputs = torch.zeros(x.size(0),1,1) # feature가1개고 target도 1개인 데이터
        _,final_hidden = self.encoder(x)
        #print(final_hidden)
        output,_ = self.decoder(x = (x[:,-1,:].cpu().unsqueeze(-1)).to(device),encoder_hidden = final_hidden[0],
                                encoder_cell = final_hidden[1])
        
        return output
173/137:
def seq_data(x,y,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length])
#             if i == 0:
#                  print(x.iloc[i:i+sequence_length].values)
#                  print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).view(-1,sequence_length,1),FloatTensor(target_list).view(-1,1,1)
    # DataLoader로 batch_size만큼 데이터를 쪼개고 싶다면 [Batch, Input_length, Channel] 형태를 따라줘야됨
173/138:
from pytorch_forecasting.metrics import SMAPE
model = seq2seq(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = Adam(model.parameters(),lr=1e-2)
173/139:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        seq=seq.to(device)
        
        out = model(x=seq)
        print(out.size())
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
173/140:
from pytorch_forecasting.metrics import SMAPE
model = seq2seq(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = Adam(model.parameters(),lr=1e-2)
173/141:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        seq=seq.to(device)
        
        out = model(x=seq)
       #print(out.size())
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
173/142:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        seq=seq.to(device)
        
        out = model(x=seq)
        print(out)
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
173/143:
import configparser
config = configparser.ConfigParser()
config['seq2seq']= {
    'input_size' : int(1),
    'hidden_size' : int(16),
    'num_layers' : int(2) 
}
with open('seq2seq','w') as f:
    config.write(f)
173/144:
import os
config.read(os.getcwd()+os.sep+'seq2seq',encoding='utf-8')
173/145:
class Encoder(nn.Module):
    def __init__(self,configs):
        super(Encoder,self).__init__()
        self.input_size = int(configs['seq2seq']['input_size'])
        # input의 feature dimension을 넣어주어야 한다
        self.hidden_size = int(configs['seq2seq']['hidden_size'])
        # 내부에서 feature dimension을 어떻게 바꿔주고 싶은지 넣어주면 된다
        # 가령 mxn matrix가 입력으로 들어왔을때 hidden size를 h라 한다면 mxh의 크기로 바꾼다
        self.num_layers = int(configs['seq2seq']['num_layers'])
        self.lstm = nn.LSTM(input_size=self.input_size,hidden_size = self.hidden_size,
                           num_layers = self.num_layers,batch_first = True)

    
    def forward(self,x):
        lstm_out,hidden = self.lstm(x) #lstm의 output으로 나오는 hidden_state는 마지막 hidden_state값이다
        return lstm_out,hidden

class Decoder(nn.Module):
    def __init__(self,configs):
        super(Decoder,self).__init__()
        self.input_size = int(configs['seq2seq']['input_size'])
        # input의 feature dimension을 넣어주어야 한다
        self.hidden_size = int(configs['seq2seq']['hidden_size'])
        # 내부에서 feature dimension을 어떻게 바꿔주고 싶은지 넣어주면 된다
        # 가령 mxn matrix가 입력으로 들어왔을때 hidden size를 h라 한다면 mxh의 크기로 바꾼다
        self.num_layers = int(configs['seq2seq']['num_layers'])
        self.lstm = nn.LSTM(input_size=self.input_size,hidden_size = self.hidden_size,
                           num_layers = self.num_layers,batch_first = True)
        self.linear = nn.Linear(self.hidden_size,self.input_size)
        
    def forward(self,x,encoder_hidden,encoder_cell):
        lstm_out,hidden = self.lstm(x,(encoder_hidden,encoder_cell))
        output = self.linear(lstm_out)
        
        return output,hidden
    # 정리하자면 우리가 정답을 알고 있는 시점에서 1시점 전까지의 데이터를 encoder로 넣고
    # 알고 있는 마지막시점 정보와 encoder에 넣어서 나온 hidden state를 넣어줌으로써 다음시점을 예측
173/146: device = torch.device('cuda:0')
173/147:
class seq2seq(nn.Module):
    def __init__(self,configs):
        super(seq2seq,self).__init__()
        self.encoder = Encoder(configs)
        self.decoder = Decoder(configs)
        
    def forward(self,x):
        outputs = torch.zeros(x.size(0),1,1) # feature가1개고 target도 1개인 데이터
        _,final_hidden = self.encoder(x)
        #print(final_hidden)
        output,_ = self.decoder(x = (x[:,-1,:].cpu().unsqueeze(-1)).to(device),encoder_hidden = final_hidden[0],
                                encoder_cell = final_hidden[1])
        
        return output
173/148:
def seq_data(x,y,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length])
#             if i == 0:
#                  print(x.iloc[i:i+sequence_length].values)
#                  print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).view(-1,sequence_length,1),FloatTensor(target_list).view(-1,1,1)
    # DataLoader로 batch_size만큼 데이터를 쪼개고 싶다면 [Batch, Input_length, Channel] 형태를 따라줘야됨
173/149:
data = pd.read_csv('../traffic/data/5.csv')

data.drop(['service_name','datetime','packets','unknown'],inplace=True,axis=1)

split = 1440
sequence_length = 1440
device = torch.device('cuda:0')
X = data['volumn'].iloc[:-split]
Y = data['volumn']
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
173/150:
torch_data = TensorDataset(x_seq,target)
batch_size = 64
train_loader = DataLoader(torch_data,batch_size = batch_size)
173/151:
from pytorch_forecasting.metrics import SMAPE
model = seq2seq(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = Adam(model.parameters(),lr=1e-2)
173/152:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        seq=seq.to(device)
        
        out = model(x=seq)
        print(out)
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
173/153:
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
173/154:
data = pd.read_csv('../traffic/data/5.csv')

data.drop(['service_name','datetime','packets','unknown'],inplace=True,axis=1)

split = 1440
sequence_length = 1440
device = torch.device('cuda:0')
X = scaler.fit_transform(data['volumn'].iloc[:-split])
Y = scaler.fit_transform(data['volumn'])
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
173/155: data['volumn'].iloc[:-split]
173/156:
data = pd.read_csv('../traffic/data/5.csv')

data.drop(['service_name','datetime','packets','unknown'],inplace=True,axis=1)

split = 1440
sequence_length = 1440
device = torch.device('cuda:0')
X = scaler.fit_transform(data['volumn'].iloc[:-split].values.reshape(-1,1))
Y = scaler.fit_transform(data['volumn'])
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
173/157:
data = pd.read_csv('../traffic/data/5.csv')

data.drop(['service_name','datetime','packets','unknown'],inplace=True,axis=1)

split = 1440
sequence_length = 1440
device = torch.device('cuda:0')
X = scaler.fit_transform(data['volumn'].iloc[:-split].values.reshape(-1,1))
Y = scaler.fit_transform(data['volumn'].values.reshape(-1,1))
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
173/158:
data = pd.read_csv('../traffic/data/5.csv')

data.drop(['service_name','datetime','packets','unknown'],inplace=True,axis=1)

split = 1440
sequence_length = 1440
device = torch.device('cuda:0')
X = scaler.fit_transform(data[['volumn']].iloc[:-split])
Y = scaler.fit_transform(data[['volumn']])
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
173/159:
data = pd.read_csv('../traffic/data/5.csv')

data.drop(['service_name','datetime','packets','unknown'],inplace=True,axis=1)

split = 1440
sequence_length = 1440
device = torch.device('cuda:0')
X = scaler.fit_transform(data[['volumn']].iloc[:-split])
print(X)
Y = scaler.fit_transform(data[['volumn']])
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
173/160:
def seq_data(x,y,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length])
    
    if (type(x) == pd.Series)|(type(x) == pd.DataFrame):
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length])
#             if i == 0:
#                  print(x.iloc[i:i+sequence_length].values)
#                  print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).view(-1,sequence_length,1),FloatTensor(target_list).view(-1,1,1)
    # DataLoader로 batch_size만큼 데이터를 쪼개고 싶다면 [Batch, Input_length, Channel] 형태를 따라줘야됨
173/161:
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
173/162:
data = pd.read_csv('../traffic/data/5.csv')

data.drop(['service_name','datetime','packets','unknown'],inplace=True,axis=1)

split = 1440
sequence_length = 1440
device = torch.device('cuda:0')
X = scaler.fit_transform(data[['volumn']].iloc[:-split])
print(X)
Y = scaler.fit_transform(data[['volumn']])
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
173/163:
def seq_data(x,y,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length])
    
    else:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length])
#             if i == 0:
#                  print(x.iloc[i:i+sequence_length].values)
#                  print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
#     else:
#         print('error')

    return FloatTensor(seq_list).view(-1,sequence_length,1),FloatTensor(target_list).view(-1,1,1)
    # DataLoader로 batch_size만큼 데이터를 쪼개고 싶다면 [Batch, Input_length, Channel] 형태를 따라줘야됨
173/164:
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
173/165:
data = pd.read_csv('../traffic/data/5.csv')

data.drop(['service_name','datetime','packets','unknown'],inplace=True,axis=1)

split = 1440
sequence_length = 1440
device = torch.device('cuda:0')
X = scaler.fit_transform(data[['volumn']].iloc[:-split])
print(X)
Y = scaler.fit_transform(data[['volumn']])
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
173/166: data[['volumn']].iloc[:-split]
173/167:
data = pd.read_csv('../traffic/data/5.csv')

data.drop(['service_name','datetime','packets','unknown'],inplace=True,axis=1)

split = 1440
sequence_length = 1440
device = torch.device('cuda:0')
X = scaler.fit_transform(data[['volumn']].iloc[:-split])
print(X.shape())
Y = scaler.fit_transform(data[['volumn']])
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
173/168:
data = pd.read_csv('../traffic/data/5.csv')

data.drop(['service_name','datetime','packets','unknown'],inplace=True,axis=1)

split = 1440
sequence_length = 1440
device = torch.device('cuda:0')
X = scaler.fit_transform(data[['volumn']].iloc[:-split])
print(X)
Y = scaler.fit_transform(data[['volumn']])
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
173/169:
data = pd.read_csv('../traffic/data/5.csv')

data.drop(['service_name','datetime','packets','unknown'],inplace=True,axis=1)

split = 1440
sequence_length = 1440
device = torch.device('cuda:0')
X = scaler.fit_transform(data[['volumn']].iloc[:-split])
print(type(X))
Y = scaler.fit_transform(data[['volumn']])
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
173/170:
data = pd.read_csv('../traffic/data/5.csv')

data.drop(['service_name','datetime','packets','unknown'],inplace=True,axis=1)

split = 1440
sequence_length = 1440
device = torch.device('cuda:0')
X = scaler.fit_transform(data[['volumn']].iloc[:-split])
print(X.shape())
Y = scaler.fit_transform(data[['volumn']])
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
173/171:
data = pd.read_csv('../traffic/data/5.csv')

data.drop(['service_name','datetime','packets','unknown'],inplace=True,axis=1)

split = 1440
sequence_length = 1440
device = torch.device('cuda:0')
X = scaler.fit_transform(data[['volumn']].iloc[:-split])
print(X.shape
Y = scaler.fit_transform(data[['volumn']])
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
173/172:
data = pd.read_csv('../traffic/data/5.csv')

data.drop(['service_name','datetime','packets','unknown'],inplace=True,axis=1)

split = 1440
sequence_length = 1440
device = torch.device('cuda:0')
X = scaler.fit_transform(data[['volumn']].iloc[:-split])
print(X.shape)
Y = scaler.fit_transform(data[['volumn']])
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
173/173:
def seq_data(x,y,sequence_length):
    seq_list = []
    target_list = []
    if type(x)==np.array:
        for i in range(x.shape()-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length])
#             if i == 0:
#                  print(x.iloc[i:i+sequence_length].values)
#                  print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).view(-1,sequence_length,1),FloatTensor(target_list).view(-1,1,1)
    # DataLoader로 batch_size만큼 데이터를 쪼개고 싶다면 [Batch, Input_length, Channel] 형태를 따라줘야됨
173/174:
def seq_data(x,y,sequence_length):
    seq_list = []
    target_list = []
    if type(x)==np.array:
        for i in range(x.shape()-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length])
#             if i == 0:
#                  print(x.iloc[i:i+sequence_length].values)
#                  print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).view(-1,sequence_length,1),FloatTensor(target_list).view(-1,1,1)
    # DataLoader로 batch_size만큼 데이터를 쪼개고 싶다면 [Batch, Input_length, Channel] 형태를 따라줘야됨
173/175:
data = pd.read_csv('../traffic/data/5.csv')

data.drop(['service_name','datetime','packets','unknown'],inplace=True,axis=1)

split = 1440
sequence_length = 1440
device = torch.device('cuda:0')
X = scaler.fit_transform(data[['volumn']].iloc[:-split])
print(X.shape)
Y = scaler.fit_transform(data[['volumn']])
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
173/176:
def seq_data(x,y,sequence_length):
    seq_list = []
    target_list = []
    if type(x)==np.array:
        print(1)
        for i in range(x.shape()-sequence_length):
            print(1)
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length])
#             if i == 0:
#                  print(x.iloc[i:i+sequence_length].values)
#                  print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).view(-1,sequence_length,1),FloatTensor(target_list).view(-1,1,1)
    # DataLoader로 batch_size만큼 데이터를 쪼개고 싶다면 [Batch, Input_length, Channel] 형태를 따라줘야됨
173/177:
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
173/178: data[['volumn']].iloc[:-split]
173/179:
data = pd.read_csv('../traffic/data/5.csv')

data.drop(['service_name','datetime','packets','unknown'],inplace=True,axis=1)

split = 1440
sequence_length = 1440
device = torch.device('cuda:0')
X = scaler.fit_transform(data[['volumn']].iloc[:-split])
#print(X.shape)
Y = scaler.fit_transform(data[['volumn']])
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
173/180:
data = pd.read_csv('../traffic/data/5.csv')

data.drop(['service_name','datetime','packets','unknown'],inplace=True,axis=1)

split = 1440
sequence_length = 1440
device = torch.device('cuda:0')
X = scaler.fit_transform(data[['volumn']].iloc[:-split])
print(type(x))
Y = scaler.fit_transform(data[['volumn']])
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
173/181:
data = pd.read_csv('../traffic/data/5.csv')

data.drop(['service_name','datetime','packets','unknown'],inplace=True,axis=1)

split = 1440
sequence_length = 1440
device = torch.device('cuda:0')
X = scaler.fit_transform(data[['volumn']].iloc[:-split])
print(type(X))
Y = scaler.fit_transform(data[['volumn']])
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
173/182:
def seq_data(x,y,sequence_length):
    seq_list = []
    target_list = []
    if type(x)==np.ndarray:
        print(1)
        for i in range(x.shape()-sequence_length):
            print(1)
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length])
#             if i == 0:
#                  print(x.iloc[i:i+sequence_length].values)
#                  print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).view(-1,sequence_length,1),FloatTensor(target_list).view(-1,1,1)
    # DataLoader로 batch_size만큼 데이터를 쪼개고 싶다면 [Batch, Input_length, Channel] 형태를 따라줘야됨
173/183:
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
173/184: data[['volumn']].iloc[:-split]
173/185:
data = pd.read_csv('../traffic/data/5.csv')

data.drop(['service_name','datetime','packets','unknown'],inplace=True,axis=1)

split = 1440
sequence_length = 1440
device = torch.device('cuda:0')
X = scaler.fit_transform(data[['volumn']].iloc[:-split])
print(type(X))
Y = scaler.fit_transform(data[['volumn']])
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
173/186:
def seq_data(x,y,sequence_length):
    seq_list = []
    target_list = []
    if type(x)==np.ndarray:
        print(1)
        for i in range(x.shape[0]-sequence_length):
            print(1)
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length])
#             if i == 0:
#                  print(x.iloc[i:i+sequence_length].values)
#                  print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).view(-1,sequence_length,1),FloatTensor(target_list).view(-1,1,1)
    # DataLoader로 batch_size만큼 데이터를 쪼개고 싶다면 [Batch, Input_length, Channel] 형태를 따라줘야됨
173/187:
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
173/188: data[['volumn']].iloc[:-split]
173/189:
data = pd.read_csv('../traffic/data/5.csv')

data.drop(['service_name','datetime','packets','unknown'],inplace=True,axis=1)

split = 1440
sequence_length = 1440
device = torch.device('cuda:0')
X = scaler.fit_transform(data[['volumn']].iloc[:-split])
print(type(X))
Y = scaler.fit_transform(data[['volumn']])
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
173/190:
def seq_data(x,y,sequence_length):
    seq_list = []
    target_list = []
    if type(x)==np.ndarray:
        print(1)
        for i in range(x.shape[0]-sequence_length):
            print(1)
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length])
#             if i == 0:
#                  print(x.iloc[i:i+sequence_length].values)
#                  print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).view(-1,sequence_length,1),FloatTensor(target_list).view(-1,1,1)
    # DataLoader로 batch_size만큼 데이터를 쪼개고 싶다면 [Batch, Input_length, Channel] 형태를 따라줘야됨
173/191: data[['volumn']].iloc[:-split]
173/192:
data = pd.read_csv('../traffic/data/5.csv')

data.drop(['service_name','datetime','packets','unknown'],inplace=True,axis=1)

split = 1440
sequence_length = 1440
device = torch.device('cuda:0')
X = scaler.fit_transform(data[['volumn']].iloc[:-split])
print(type(X))
Y = scaler.fit_transform(data[['volumn']])
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
173/193:
def seq_data(x,y,sequence_length):
    seq_list = []
    target_list = []
    if type(x)==np.ndarray:
        print(1)
        for i in range(x.shape[0]-sequence_length):
            print(1)
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        #print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length])
#             if i == 0:
#                  print(x.iloc[i:i+sequence_length].values)
#                  print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).view(-1,sequence_length,1),FloatTensor(target_list).view(-1,1,1)
    # DataLoader로 batch_size만큼 데이터를 쪼개고 싶다면 [Batch, Input_length, Channel] 형태를 따라줘야됨
173/194:
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
173/195: data[['volumn']].iloc[:-split]
173/196:
data = pd.read_csv('../traffic/data/5.csv')

data.drop(['service_name','datetime','packets','unknown'],inplace=True,axis=1)

split = 1440
sequence_length = 1440
device = torch.device('cuda:0')
X = scaler.fit_transform(data[['volumn']].iloc[:-split])
#print(type(X))
Y = scaler.fit_transform(data[['volumn']])
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
# print(x_seq,target)
# print(x_seq.size(),target.size())
173/197:
data = pd.read_csv('../traffic/data/5.csv')

data.drop(['service_name','datetime','packets','unknown'],inplace=True,axis=1)

split = 1440
sequence_length = 1440
device = torch.device('cuda:0')
X = scaler.fit_transform(data[['volumn']].iloc[:-split])
#print(type(X))
Y = scaler.fit_transform(data[['volumn']])
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
 print(x_seq,target)
# print(x_seq.size(),target.size())
173/198:
data = pd.read_csv('../traffic/data/5.csv')

data.drop(['service_name','datetime','packets','unknown'],inplace=True,axis=1)

split = 1440
sequence_length = 1440
device = torch.device('cuda:0')
X = scaler.fit_transform(data[['volumn']].iloc[:-split])
#print(type(X))
Y = scaler.fit_transform(data[['volumn']])
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
# print(x_seq.size(),target.size())
173/199:
def seq_data(x,y,sequence_length):
    seq_list = []
    target_list = []
    if type(x)==np.ndarray:
        print(1)
        for i in range(x.shape[0]-sequence_length):
            #print(1)
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        #print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length])
#             if i == 0:
#                  print(x.iloc[i:i+sequence_length].values)
#                  print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).view(-1,sequence_length,1),FloatTensor(target_list).view(-1,1,1)
    # DataLoader로 batch_size만큼 데이터를 쪼개고 싶다면 [Batch, Input_length, Channel] 형태를 따라줘야됨
173/200:
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
173/201: data[['volumn']].iloc[:-split]
173/202:
data = pd.read_csv('../traffic/data/5.csv')

data.drop(['service_name','datetime','packets','unknown'],inplace=True,axis=1)

split = 1440
sequence_length = 1440
device = torch.device('cuda:0')
X = scaler.fit_transform(data[['volumn']].iloc[:-split])
#print(type(X))
Y = scaler.fit_transform(data[['volumn']])
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
# print(x_seq.size(),target.size())
173/203:
def seq_data(x,y,sequence_length):
    seq_list = []
    target_list = []
    if type(x)==np.ndarray:
        print(1)
        for i in range(x.shape[0]-sequence_length):
            #print(1)
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    else if type(x) == pd.Series:
        #print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length])
#             if i == 0:
#                  print(x.iloc[i:i+sequence_length].values)
#                  print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).view(-1,sequence_length,1),FloatTensor(target_list).view(-1,1,1)
    # DataLoader로 batch_size만큼 데이터를 쪼개고 싶다면 [Batch, Input_length, Channel] 형태를 따라줘야됨
173/204:
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
173/205:
def seq_data(x,y,sequence_length):
    seq_list = []
    target_list = []
    if type(x)==np.ndarray:
        print(1)
        for i in range(x.shape[0]-sequence_length):
            #print(1)
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    elif type(x) == pd.Series:
        #print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length])
#             if i == 0:
#                  print(x.iloc[i:i+sequence_length].values)
#                  print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).view(-1,sequence_length,1),FloatTensor(target_list).view(-1,1,1)
    # DataLoader로 batch_size만큼 데이터를 쪼개고 싶다면 [Batch, Input_length, Channel] 형태를 따라줘야됨
173/206:
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
173/207: data[['volumn']].iloc[:-split]
173/208:
data = pd.read_csv('../traffic/data/5.csv')

data.drop(['service_name','datetime','packets','unknown'],inplace=True,axis=1)

split = 1440
sequence_length = 1440
device = torch.device('cuda:0')
X = scaler.fit_transform(data[['volumn']].iloc[:-split])
#print(type(X))
Y = scaler.fit_transform(data[['volumn']])
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
# print(x_seq.size(),target.size())
173/209:
data = pd.read_csv('../traffic/data/5.csv')

data.drop(['service_name','datetime','packets','unknown'],inplace=True,axis=1)

split = 1440
sequence_length = 1440
device = torch.device('cuda:0')
X = scaler.fit_transform(data[['volumn']].iloc[:-split])
#print(type(X))
Y = scaler.fit_transform(data[['volumn']])
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
#print(x_seq,target)
print(x_seq.size(),target.size())
173/210:
torch_data = TensorDataset(x_seq,target)
batch_size = 64
train_loader = DataLoader(torch_data,batch_size = batch_size)
173/211:
from pytorch_forecasting.metrics import SMAPE
model = seq2seq(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = Adam(model.parameters(),lr=1e-2)
173/212:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        seq=seq.to(device)
        
        out = model(x=seq)
    
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
173/213:
from pytorch_forecasting.metrics import SMAPE
model = seq2seq(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = Adam(model.parameters(),lr=1e-5)
173/214:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        seq=seq.to(device)
        
        out = model(x=seq)
    
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
173/215:
target = Y[-split:]
target
173/216:
pred_list = []
time_list = []
for idx in range(split):
    if idx == 0:
         new_train = torch.FloatTensor(
        [X[-sequence_length+idx:].values]
    ).view(1,sequence_length,1).to(device)
    else:
        new_train = torch.FloatTensor(
            [pd.concat([X[-sequence_length+idx:],target[0:idx]],axis=0,ignore_index=True).values]
        ).view(1,sequence_length,1).to(device)
    # 3차원 텐서로 만들어주는 과정 내가 원하는 모양 (1,1440,1)
    start = time.time()
    out = model(new_train)
    end = time.time()
    pred_list.append(out.cpu().view(1).item())
    time_list.append(end-start)
173/217:
pred_list = []
time_list = []
for idx in range(split):
    if idx == 0:
         new_train = torch.FloatTensor(
        X[-sequence_length+idx:]
    ).view(1,sequence_length,1).to(device)
    else:
        new_train = torch.FloatTensor(
            [pd.concat([X[-sequence_length+idx:],target[0:idx]],axis=0,ignore_index=True).values]
        ).view(1,sequence_length,1).to(device)
    # 3차원 텐서로 만들어주는 과정 내가 원하는 모양 (1,1440,1)
    start = time.time()
    out = model(new_train)
    end = time.time()
    pred_list.append(out.cpu().view(1).item())
    time_list.append(end-start)
173/218:
pred_list = []
time_list = []
for idx in range(split):
    if idx == 0:
         new_train = torch.FloatTensor(
        pd.DataFrame('volumn':X[-sequence_length+idx:])
    ).view(1,sequence_length,1).to(device)
    else:
        new_train = torch.FloatTensor(
            [pd.concat([X[-sequence_length+idx:],target[0:idx]],axis=0,ignore_index=True).values]
        ).view(1,sequence_length,1).to(device)
    # 3차원 텐서로 만들어주는 과정 내가 원하는 모양 (1,1440,1)
    start = time.time()
    out = model(new_train)
    end = time.time()
    pred_list.append(out.cpu().view(1).item())
    time_list.append(end-start)
173/219:
pred_list = []
time_list = []
for idx in range(split):
    if idx == 0:
         new_train = torch.FloatTensor(
        pd.DataFrame({'volumn':X[-sequence_length+idx:]})
    ).view(1,sequence_length,1).to(device)
    else:
        new_train = torch.FloatTensor(
            [pd.concat([X[-sequence_length+idx:],target[0:idx]],axis=0,ignore_index=True).values]
        ).view(1,sequence_length,1).to(device)
    # 3차원 텐서로 만들어주는 과정 내가 원하는 모양 (1,1440,1)
    start = time.time()
    out = model(new_train)
    end = time.time()
    pred_list.append(out.cpu().view(1).item())
    time_list.append(end-start)
173/220:
pred_list = []
time_list = []
for idx in range(split):
    if idx == 0:
         new_train = torch.FloatTensor(
        pd.DataFrame({'volumn':X[-sequence_length+idx:].reshape(-1,1)})
    ).view(1,sequence_length,1).to(device)
    else:
        new_train = torch.FloatTensor(
            [pd.concat([X[-sequence_length+idx:],target[0:idx]],axis=0,ignore_index=True).values]
        ).view(1,sequence_length,1).to(device)
    # 3차원 텐서로 만들어주는 과정 내가 원하는 모양 (1,1440,1)
    start = time.time()
    out = model(new_train)
    end = time.time()
    pred_list.append(out.cpu().view(1).item())
    time_list.append(end-start)
173/221: X[-sequence_length:].reshape(-1,1)
173/222: X[-sequence_length:].reshape(-1,)
173/223:
pred_list = []
time_list = []
for idx in range(split):
    if idx == 0:
         new_train = torch.FloatTensor(
        pd.DataFrame({'volumn':X[-sequence_length+idx:].reshape(-1,)})
    ).view(1,sequence_length,1).to(device)
    else:
        new_train = torch.FloatTensor(
            [pd.concat([X[-sequence_length+idx:],target[0:idx]],axis=0,ignore_index=True).values]
        ).view(1,sequence_length,1).to(device)
    # 3차원 텐서로 만들어주는 과정 내가 원하는 모양 (1,1440,1)
    start = time.time()
    out = model(new_train)
    end = time.time()
    pred_list.append(out.cpu().view(1).item())
    time_list.append(end-start)
173/224: pd.DataFrame({'a':1,2,3})
173/225: pd.DataFrame({'a':1})
173/226: pd.DataFrame(index=1,{'a':1})
173/227: pd.DataFrame(index=[1],{'a':1})
173/228: pd.DataFrame(data={'a':1})
173/229: pd.DataFrame(data={'a':[1]})
173/230:
pred_list = []
time_list = []
for idx in range(split):
    X= X[-sequence_length+idx:].reshape(-1,)
    if idx == 0:
         new_train = torch.FloatTensor(
        X
    ).view(1,sequence_length,1).to(device)
    else:
        new_train = torch.FloatTensor(
            [pd.concat(pd.DataFrame({'volumn':X}),target[0:idx]],axis=0,ignore_index=True).values]
        ).view(1,sequence_length,1).to(device)
    # 3차원 텐서로 만들어주는 과정 내가 원하는 모양 (1,1440,1)
    start = time.time()
    out = model(new_train)
    end = time.time()
    pred_list.append(out.cpu().view(1).item())
    time_list.append(end-start)
173/231:
pred_list = []
time_list = []
for idx in range(split):
    X= X[-sequence_length+idx:].reshape(-1,)
    if idx == 0:
         new_train = torch.FloatTensor(
        X
    ).view(1,sequence_length,1).to(device)
    else:
        new_train = torch.FloatTensor(
            [pd.concat([pd.DataFrame({'volumn':X}),target[0:idx]],axis=0,ignore_index=True).values]
        ).view(1,sequence_length,1).to(device)
    # 3차원 텐서로 만들어주는 과정 내가 원하는 모양 (1,1440,1)
    start = time.time()
    out = model(new_train)
    end = time.time()
    pred_list.append(out.cpu().view(1).item())
    time_list.append(end-start)
173/232: target[0:]
173/233:
pred_list = []
time_list = []
for idx in range(split):
    X= X[-sequence_length+idx:].reshape(-1,)
    target = target.reshape(-1,)
    if idx == 0:
         new_train = torch.FloatTensor(
        X
    ).view(1,sequence_length,1).to(device)
    else:
        new_train = torch.FloatTensor(
            [pd.concat([pd.DataFrame({'volumn':X}),pd.DataFrame({'volumn':target[0:idx]})],axis=0,ignore_index=True).values]
        ).view(1,sequence_length,1).to(device)
    # 3차원 텐서로 만들어주는 과정 내가 원하는 모양 (1,1440,1)
    start = time.time()
    out = model(new_train)
    end = time.time()
    pred_list.append(out.cpu().view(1).item())
    time_list.append(end-start)
173/234:
pred_list = []
time_list = []
X= X[-sequence_length:].reshape(-1,)
target = target.reshape(-1,)
for idx in range(split):
    if idx == 0:
         new_train = torch.FloatTensor(
        X[-sequence_length+idx:]
    ).view(1,sequence_length,1).to(device)
    else:
        new_train = torch.FloatTensor(
            [pd.concat([pd.DataFrame({'volumn': X[-sequence_length+idx:]}),pd.DataFrame({'volumn':target[0:idx]})],axis=0,ignore_index=True).values]
        ).view(1,sequence_length,1).to(device)
    # 3차원 텐서로 만들어주는 과정 내가 원하는 모양 (1,1440,1)
    start = time.time()
    out = model(new_train)
    end = time.time()
    pred_list.append(out.cpu().view(1).item())
    time_list.append(end-start)
173/235:
pred_list = []
time_list = []
X= X.reshape(-1,)
target = target.reshape(-1,)
for idx in range(split):
    if idx == 0:
         new_train = torch.FloatTensor(
        X[-sequence_length+idx:]
    ).view(1,sequence_length,1).to(device)
    else:
        new_train = torch.FloatTensor(
            [pd.concat([pd.DataFrame({'volumn': X[-sequence_length+idx:]}),pd.DataFrame({'volumn':target[0:idx]})],axis=0,ignore_index=True).values]
        ).view(1,sequence_length,1).to(device)
    # 3차원 텐서로 만들어주는 과정 내가 원하는 모양 (1,1440,1)
    start = time.time()
    out = model(new_train)
    end = time.time()
    pred_list.append(out.cpu().view(1).item())
    time_list.append(end-start)
173/236: X= X.reshape(-1,)
173/237:
X= X.reshape(-1,)
X
173/238:
X= X.reshape(-1,)
X.shape
173/239:
pred_list = []
time_list = []
X = scaler.fit_transform(data[['volumn']].iloc[-2*split:-split]).reshape(-1,)
Y = scaler.fit_transform(data[['volumn']])
target = Y[-split:].reshape(-1,)
for idx in range(split):
    if idx == 0:
         new_train = torch.FloatTensor(
        X[-sequence_length+idx:]
    ).view(1,sequence_length,1).to(device)
    else:
        new_train = torch.FloatTensor(
            [pd.concat([pd.DataFrame({'volumn': X[-sequence_length+idx:]}),pd.DataFrame({'volumn':target[0:idx]})],axis=0,ignore_index=True).values]
        ).view(1,sequence_length,1).to(device)
    # 3차원 텐서로 만들어주는 과정 내가 원하는 모양 (1,1440,1)
    start = time.time()
    out = model(new_train)
    end = time.time()
    pred_list.append(out.cpu().view(1).item())
    time_list.append(end-start)
173/240: pred_list
173/241: len(pred_list)
173/242:
fig,axes = plt.subplots(1,1,figsize=(12,8))
axes.plot(np.arange(1,1441),pred_list,label='prediction')
axes.plot(np.arange(1,1441),target,label='target')
axes.legend()
173/243:
import torch
import torch.nn as nn
import pandas as pd
import matplotlib.pyplot as plt
from torch.optim import Adam
from torch import FloatTensor
from torch.utils.data import TensorDataset,DataLoader
173/244:
fig,axes = plt.subplots(1,1,figsize=(12,8))
axes.plot(np.arange(1,1441),pred_list,label='prediction')
axes.plot(np.arange(1,1441),target,label='target')
axes.legend()
173/245:
mean_pred_time = np.mean(time_list)
mean_pred_time
173/246:
score = SMAPE()(torch.FloatTensor(target.values).view(-1,1),torch.FloatTensor(pred_list).view(-1,1)) * 100
score.item()
173/247:
score = SMAPE()(torch.FloatTensor(target).view(-1,1),torch.FloatTensor(pred_list).view(-1,1)) * 100
score.item()
173/248:
from pytorch_forecasting.metrics import RMSE
score = RMSE()(torch.FloatTensor(pred_list).view(-1,1),torch.FloatTensor(target.values).view(-1,1))
score.item()
173/249:
from pytorch_forecasting.metrics import RMSE
score = RMSE()(torch.FloatTensor(pred_list).view(-1,1),torch.FloatTensor(target).view(-1,1))
score.item()
176/1:
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
import torch.optim as optim
import matplotlib.pyplot as plt
from torch.utils.data import TensorDataset,DataLoader
from torch import FloatTensor
from sklearn.preprocessing import MinMaxScaler
176/2:
import configparser
config = configparser.ConfigParser()
config['NLinear_config'] = {
    'seq_len':1440,
    'pred_len':1,
    'enc_in':1,
    'individual':bool(False)
}
with open('NLinear_config','w') as f:
    config.write(f)
176/3:
import configparser
config = configparser.ConfigParser()
config['NLinear_config_vol'] = {
    'seq_len':7,
    'pred_len':1,
    'enc_in':1,
    'individual':bool(False)
}
with open('NLinear_config_vol','w') as f:
    config.write(f)
176/4: config.read(os.getcwd()+os.sep+'NLinear_config',encoding = 'utf-8')
176/5:
import os
config.read(os.getcwd()+os.sep+'NLinear_config',encoding = 'utf-8')
176/6:
import os
config.read(os.getcwd()+os.sep+'NLinear_config_vol',encoding = 'utf-8')
176/7:
class Model(nn.Module):
    def __init__(self,configs):
        super(Model,self).__init__()
        self.seq_len = int(configs['NLinear_config_vol']['seq_len'])
        self.pred_len = int(configs['NLinear_config_vol']['pred_len'])
        self.channels = int(configs['NLinear_config_vol']['enc_in'])
        self.individual = configs['NLinear_config_vol']['individual']
        if self.individual == True:
            self.Linear = nn.ModuleList()
            for i in range(self.channels):
                self.Linear.append(nn.Linear(self.seq_len,self.pred_len))
        else:
            self.Linear = nn.Linear(self.seq_len,self.pred_len)
    
    def forward(self,x):
        # x는 RNN Layer처럼 하나의 값이 하나의 텐서안에 들어가 있도록 즉 (batch_size,sequence_length,channe)
        device = torch.device("cuda:0")
        x= x.to(device)
        seq_last = x[:,-1,:].unsqueeze(2).to(device)
        x = x-seq_last
        if self.individual == True:
            output = torch.zeros([x.size(0),self.pred_len,x.size(2)],dtype = x.dtype).to(x.device)
            for i in range(self.channels):
                output[:,:,i] = self.Linear[i](x[:,:,i])# 하나의 특성마다 각기 다른 Linear layer에 집어넣어줌
                # 이렇게 인덱싱하면 위에 처럼 linear layer에 넣기 좋게 나온다
            x = output
        else:
            x = self.Linear(x.permute(0,2,1)).permute(0,2,1)
        x = x+seq_last
        return x #[Batch, Output length, Channel]
176/8:
def seq_data(x,y,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length])
#             if i == 0:
#                  print(x.iloc[i:i+sequence_length].values)
#                  print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).view(-1,sequence_length,1),FloatTensor(target_list).view(-1,1,1)
    # DataLoader로 batch_size만큼 데이터를 쪼개고 싶다면 [Batch, Input_length, Channel] 형태를 따라줘야됨
176/9:
data = pd.read_csv('../mart/data/vege_8809215001015.csv')
data
176/10:
data = pd.read_csv('../mart/data/vege_8809215001015.csv',names=['datetime','volumn'])
data
176/11:
data = pd.read_csv('../mart/data/vege_8809215001015.csv',names=['datetime','sales'])
data
176/12:
data = pd.read_csv('../mart/data/vege_8809215001015.csv',names=['datetime','sales'])
data.drop(labels=['datetime'],inplace=True)
data
176/13:
data = pd.read_csv('../mart/data/vege_8809215001015.csv',names=['datetime','sales'])
data.drop(labels=['datetime'],inplace=True,axis=1)
data
176/14: data.plot()
176/15:
split = 60
sequence_length = 60
device = torch.device('cuda:0')
X = data['volumn'].iloc[:-split]
Y = data['volumn']
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
176/16:
split = 60
sequence_length = 60
device = torch.device('cuda:0')
X = data['sales'].iloc[:-split]
Y = data['sales']
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
176/17:
torch_data = TensorDataset(x_seq,target)

batch_size = 32
train_loader = DataLoader(torch_data,batch_size = batch_size)
176/18:
from pytorch_forecasting.metrics import SMAPE
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
176/19:
from pytorch_forecasting.metrics import SMAPE
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
176/20:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
176/21:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
176/22:
torch_data = TensorDataset(x_seq,target)

batch_size = 1
train_loader = DataLoader(torch_data,batch_size = batch_size)
176/23:
from pytorch_forecasting.metrics import SMAPE
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
176/24:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
176/25:
class Model(nn.Module):
    def __init__(self,configs):
        super(Model,self).__init__()
        self.seq_len = int(configs['NLinear_config_vol']['seq_len'])
        self.pred_len = int(configs['NLinear_config_vol']['pred_len'])
        self.channels = int(configs['NLinear_config_vol']['enc_in'])
        self.individual = configs['NLinear_config_vol']['individual']
        if self.individual == True:
            self.Linear = nn.ModuleList()
            for i in range(self.channels):
                self.Linear.append(nn.Linear(self.seq_len,self.pred_len))
        else:
            self.Linear = nn.Linear(self.seq_len,self.pred_len)
    
    def forward(self,x):
        # x는 RNN Layer처럼 하나의 값이 하나의 텐서안에 들어가 있도록 즉 (batch_size,sequence_length,channe)
        device = torch.device("cuda:0")
        x= x.to(device)
        seq_last = x[:,-1,:].unsqueeze(2).to(device)
        x = x-seq_last
        if self.individual == True:
            output = torch.zeros([x.size(0),self.pred_len,x.size(2)],dtype = x.dtype).to(x.device)
            for i in range(self.channels):
                output[:,:,i] = self.Linear[i](x[:,:,i])# 하나의 특성마다 각기 다른 Linear layer에 집어넣어줌
                # 이렇게 인덱싱하면 위에 처럼 linear layer에 넣기 좋게 나온다
            x = output
        else:
            print(x.size())
            x = self.Linear(x.permute(0,2,1)).permute(0,2,1)
        x = x+seq_last
        return x #[Batch, Output length, Channel]
176/26:
from pytorch_forecasting.metrics import SMAPE
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
176/27:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
176/28:
split = 60
sequence_length = 7
device = torch.device('cuda:0')
X = data['sales'].iloc[:-split]
Y = data['sales']
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
176/29:
torch_data = TensorDataset(x_seq,target)

batch_size = 1
train_loader = DataLoader(torch_data,batch_size = batch_size)
176/30:
from pytorch_forecasting.metrics import SMAPE
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
176/31:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
176/32:
class Model(nn.Module):
    def __init__(self,configs):
        super(Model,self).__init__()
        self.seq_len = int(configs['NLinear_config_vol']['seq_len'])
        self.pred_len = int(configs['NLinear_config_vol']['pred_len'])
        self.channels = int(configs['NLinear_config_vol']['enc_in'])
        self.individual = configs['NLinear_config_vol']['individual']
        if self.individual == True:
            self.Linear = nn.ModuleList()
            for i in range(self.channels):
                self.Linear.append(nn.Linear(self.seq_len,self.pred_len))
        else:
            self.Linear = nn.Linear(self.seq_len,self.pred_len)
    
    def forward(self,x):
        # x는 RNN Layer처럼 하나의 값이 하나의 텐서안에 들어가 있도록 즉 (batch_size,sequence_length,channe)
        device = torch.device("cuda:0")
        x= x.to(device)
        seq_last = x[:,-1,:].unsqueeze(2).to(device)
        x = x-seq_last
        if self.individual == True:
            output = torch.zeros([x.size(0),self.pred_len,x.size(2)],dtype = x.dtype).to(x.device)
            for i in range(self.channels):
                output[:,:,i] = self.Linear[i](x[:,:,i])# 하나의 특성마다 각기 다른 Linear layer에 집어넣어줌
                # 이렇게 인덱싱하면 위에 처럼 linear layer에 넣기 좋게 나온다
            x = output
        else:
            x = self.Linear(x.permute(0,2,1)).permute(0,2,1)
        x = x+seq_last
        return x #[Batch, Output length, Channel]
176/33:
def seq_data(x,y,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length])
#             if i == 0:
#                  print(x.iloc[i:i+sequence_length].values)
#                  print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).view(-1,sequence_length,1),FloatTensor(target_list).view(-1,1,1)
    # DataLoader로 batch_size만큼 데이터를 쪼개고 싶다면 [Batch, Input_length, Channel] 형태를 따라줘야됨
176/34:
data = pd.read_csv('../mart/data/vege_8809215001015.csv',names=['datetime','sales'])
data.drop(labels=['datetime'],inplace=True,axis=1)
data
176/35: data.plot()
176/36:
split = 60
sequence_length = 7
device = torch.device('cuda:0')
X = data['sales'].iloc[:-split]
Y = data['sales']
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
176/37:
torch_data = TensorDataset(x_seq,target)

batch_size = 1
train_loader = DataLoader(torch_data,batch_size = batch_size)
176/38:
from pytorch_forecasting.metrics import SMAPE
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
176/39:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
176/40:
from pytorch_forecasting.metrics import SMAPE
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
176/41:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
176/42:
from pytorch_forecasting.metrics import SMAPE
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-2)
176/43:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
176/44:
from pytorch_forecasting.metrics import SMAPE
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
176/45:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
176/46:
from pytorch_forecasting.metrics import SMAPE
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
176/47:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
176/48:
pred_list = []
time_list = []
target = Y[-split:]
for idx in range(split):
    if idx == 0:
         new_train = torch.FloatTensor(
        [X[-sequence_length+idx:].values]
    ).view(1,sequence_length,1).to(device)
    else:
        new_train = torch.FloatTensor(
            [pd.concat([X[-sequence_length+idx:],target[0:idx]],axis=0,ignore_index=True).values]
        ).view(1,sequence_length,1).to(device)
    # 3차원 텐서로 만들어주는 과정 내가 원하는 모양 (1,1440,1)
    start = time.time()
    out = model(new_train)
    end = time.time()
    pred_list.append(out.cpu().view(1).item())
    time_list.append(end-start)
176/49: X[-sequence_length:]
176/50:
pred_list = []
time_list = []
target = Y[-split:]
for idx in range(split):
    if idx == 0:
         new_train = torch.FloatTensor(
        [X[-sequence_length+idx:].values]
    ).view(1,sequence_length,1).to(device)
        print(1)
    else:
        new_train = torch.FloatTensor(
            [pd.concat([X[-sequence_length+idx:],target[0:idx]],axis=0,ignore_index=True).values]
        ).view(1,sequence_length,1).to(device)
    # 3차원 텐서로 만들어주는 과정 내가 원하는 모양 (1,7,1)
    start = time.time()
    out = model(new_train)
    end = time.time()
    pred_list.append(out.cpu().view(1).item())
    time_list.append(end-start)
176/52:
pred_list = []
time_list = []
target = Y[-split:]
for idx in range(split):
    if idx == 0:
        new_train = torch.FloatTensor([X[-sequence_length+idx:].values]).view(1,sequence_length,1).to(device)
        print(1)
    else:
        new_train = torch.FloatTensor(
            [pd.concat([X[-sequence_length+idx:],target[0:idx]],axis=0,ignore_index=True).values]
        ).view(1,sequence_length,1).to(device)
    # 3차원 텐서로 만들어주는 과정 내가 원하는 모양 (1,7,1)
    start = time.time()
    out = model(new_train)
    end = time.time()
    pred_list.append(out.cpu().view(1).item())
    time_list.append(end-start)
176/53:
pred_list = []
time_list = []
target = Y[-split:]
for idx in range(split):
    if idx == 0:
        new_train = torch.FloatTensor([X[-sequence_length+idx:].values]).view(1,sequence_length,1).to(device)
        print(1)
    else:
        print(1)
        new_train = torch.FloatTensor(
            [pd.concat([X[-sequence_length+idx:],target[0:idx]],axis=0,ignore_index=True).values]
        ).view(1,sequence_length,1).to(device)
    # 3차원 텐서로 만들어주는 과정 내가 원하는 모양 (1,7,1)
    start = time.time()
    out = model(new_train)
    end = time.time()
    pred_list.append(out.cpu().view(1).item())
    time_list.append(end-start)
176/54:
pred_list = []
time_list = []
target = Y[-split:]
for idx in range(split):
    if idx == 0:
        new_train = torch.FloatTensor([X[-sequence_length+idx:].values]).view(1,sequence_length,1).to(device)
        print(1)
    else:
        print(pd.concat([X[-sequence_length+idx:],target[0:idx]],axis=0,ignore_index=True).values)
        new_train = torch.FloatTensor(
            [pd.concat([X[-sequence_length+idx:],target[0:idx]],axis=0,ignore_index=True).values]
        ).view(1,sequence_length,1).to(device)
    # 3차원 텐서로 만들어주는 과정 내가 원하는 모양 (1,7,1)
    start = time.time()
    out = model(new_train)
    end = time.time()
    pred_list.append(out.cpu().view(1).item())
    time_list.append(end-start)
176/55:
pred_list = []
time_list = []
target = Y[-split:]
X = data['sales'].iloc[:-split]
for idx in range(split):
    if idx == 0:
        new_train = torch.FloatTensor([X[-sequence_length+idx:].values]).view(1,sequence_length,1).to(device)
        print(1)
    else:
        print(pd.concat([X[-sequence_length+idx:],target[0:idx]],axis=0,ignore_index=True).values)
        new_train = torch.FloatTensor(
            [pd.concat([X[-sequence_length+idx:],target[0:idx]],axis=0,ignore_index=True).values]
        ).view(1,sequence_length,1).to(device)
    # 3차원 텐서로 만들어주는 과정 내가 원하는 모양 (1,7,1)
    start = time.time()
    out = model(new_train)
    end = time.time()
    pred_list.append(out.cpu().view(1).item())
    time_list.append(end-start)
176/56:
pred_list = []
time_list = []
target = Y[-split:]
X = data['sales'].iloc[:-split]
for idx in range(split):
    if idx == 0:
        new_train = torch.FloatTensor([X[-sequence_length+idx:].values]).view(1,sequence_length,1).to(device)
        print(1)
    elif idx<=7:
        new_train = torch.FloatTensor(
                [pd.concat([X[-sequence_length+idx:],target[0:idx]],axis=0,ignore_index=True).values]
            ).view(1,sequence_length,1).to(device)
    else:
        new_train = torch.FloatTensor(target[0:idx].values).view(1,sequence_length,1).to(device)
    # 3차원 텐서로 만들어주는 과정 내가 원하는 모양 (1,7,1)
    start = time.time()
    out = model(new_train)
    end = time.time()
    pred_list.append(out.cpu().view(1).item())
    time_list.append(end-start)
176/57:
pred_list = []
time_list = []
target = Y[-split:]
X = data['sales'].iloc[:-split]
for idx in range(split):
    if idx == 0:
        new_train = torch.FloatTensor([X[-sequence_length+idx:].values]).view(1,sequence_length,1).to(device)
        
    elif idx<=7:
        print(1)
        new_train = torch.FloatTensor(
                [pd.concat([X[-sequence_length+idx:],target[0:idx]],axis=0,ignore_index=True).values]
            ).view(1,sequence_length,1).to(device)
    else:
        new_train = torch.FloatTensor(target[0:idx].values).view(1,sequence_length,1).to(device)
    # 3차원 텐서로 만들어주는 과정 내가 원하는 모양 (1,7,1)
    start = time.time()
    out = model(new_train)
    end = time.time()
    pred_list.append(out.cpu().view(1).item())
    time_list.append(end-start)
176/58:
pred_list = []
time_list = []
target = Y[-split:]
X = data['sales'].iloc[:-split]
for idx in range(split):
    if idx == 0:
        new_train = torch.FloatTensor([X[-sequence_length+idx:].values]).view(1,sequence_length,1).to(device)
        
    elif idx<=6:
        print(1)
        new_train = torch.FloatTensor(
                [pd.concat([X[-sequence_length+idx:],target[0:idx]],axis=0,ignore_index=True).values]
            ).view(1,sequence_length,1).to(device)
    else:
        new_train = torch.FloatTensor(target[0:idx].values).view(1,sequence_length,1).to(device)
    # 3차원 텐서로 만들어주는 과정 내가 원하는 모양 (1,7,1)
    start = time.time()
    out = model(new_train)
    end = time.time()
    pred_list.append(out.cpu().view(1).item())
    time_list.append(end-start)
176/59:
pred_list = []
time_list = []
target = Y[-split:]
X = data['sales'].iloc[:-split]
for idx in range(split):
    if idx == 0:
        new_train = torch.FloatTensor([X[-sequence_length+idx:].values]).view(1,sequence_length,1).to(device)
        
    elif idx<=7:
        print(1)
        new_train = torch.FloatTensor(
                [pd.concat([X[-sequence_length+idx:],target[0:idx]],axis=0,ignore_index=True).values]
            ).view(1,sequence_length,1).to(device)
    else:
        new_train = torch.FloatTensor(target[idx-7:idx].values).view(1,sequence_length,1).to(device)
    # 3차원 텐서로 만들어주는 과정 내가 원하는 모양 (1,7,1)
    start = time.time()
    out = model(new_train)
    end = time.time()
    pred_list.append(out.cpu().view(1).item())
    time_list.append(end-start)
176/60:
pred_list = []
time_list = []
target = Y[-split:]
X = data['sales'].iloc[:-split]
for idx in range(split):
    if idx == 0:
        new_train = torch.FloatTensor([X[-sequence_length+idx:].values]).view(1,sequence_length,1).to(device)
        
    elif idx<=6:
        print(1)
        new_train = torch.FloatTensor(
                [pd.concat([X[-sequence_length+idx:],target[0:idx]],axis=0,ignore_index=True).values]
            ).view(1,sequence_length,1).to(device)
    else:
        new_train = torch.FloatTensor(target[idx-7:idx].values).view(1,sequence_length,1).to(device)
    # 3차원 텐서로 만들어주는 과정 내가 원하는 모양 (1,7,1)
    start = time.time()
    out = model(new_train)
    end = time.time()
    pred_list.append(out.cpu().view(1).item())
    time_list.append(end-start)
176/61: X[-7+6:]
176/62: X[-7+7:]
176/63:
fig,axes = plt.subplots(1,1,figsize=(12,8))
axes.plot(np.arange(1,1441),pred_list,label='prediction')
axes.plot(np.arange(1,1441),target,label='target')
axes.legend()
176/64: len(pred_list)
176/65:
fig,axes = plt.subplots(1,1,figsize=(12,8))
axes.plot(np.arange(1,61),pred_list,label='prediction')
axes.plot(np.arange(1,61),target,label='target')
axes.legend()
179/1:
fig,axes = plt.subplots(1,1,figsize=(12,8))
axes.plot(np.arange(1,61),pred_list[-60:],label='prediction')
axes.plot(np.arange(1,61),target[-60:],label='target')
axes.legend()
179/2:
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
import torch.optim as optim
import matplotlib.pyplot as plt
from torch.utils.data import TensorDataset,DataLoader
from torch import FloatTensor
from sklearn.preprocessing import MinMaxScaler
179/3:
import configparser
config = configparser.ConfigParser()
config['NLinear_config'] = {
    'seq_len':1440,
    'pred_len':1,
    'enc_in':1,
    'individual':bool(False)
}
with open('NLinear_config','w') as f:
    config.write(f)
179/4: config.read(os.getcwd()+os.sep+'NLinear_config',encoding = 'utf-8')
179/5:
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
import torch.optim as optim
import matplotlib.pyplot as plt
from torch.utils.data import TensorDataset,DataLoader
from torch import FloatTensor
from sklearn.preprocessing import MinMaxScaler
179/6:
import configparser
config = configparser.ConfigParser()
config['NLinear_config'] = {
    'seq_len':1440,
    'pred_len':1,
    'enc_in':1,
    'individual':bool(False)
}
with open('NLinear_config','w') as f:
    config.write(f)
179/7:
import os
config.read(os.getcwd()+os.sep+'NLinear_config',encoding = 'utf-8')
179/8: torch.FloatTensor([[[1,2,3],[4,5,6],[7,8,9]],[[2,3,4],[5,6,7],[8,9,10]]])+torch.FloatTensor([1,2,3])
179/9:
x = torch.FloatTensor([[[1,2,3],[4,5,6],[7,8,9]],[[2,3,4],[5,6,7],[8,9,10]]])
x[:,:,2]
179/10:
class Model(nn.Module):
    def __init__(self,configs):
        super(Model,self).__init__()
        self.seq_len = int(configs['NLinear_config']['seq_len'])
        self.pred_len = int(configs['NLinear_config']['pred_len'])
        self.channels = int(configs['NLinear_config']['enc_in'])
        self.individual = configs['NLinear_config']['individual']
        if self.individual == True:
            self.Linear = nn.ModuleList()
            for i in range(self.channels):
                self.Linear.append(nn.Linear(self.seq_len,self.pred_len))
        else:
            self.Linear = nn.Linear(self.seq_len,self.pred_len)
    
    def forward(self,x):
        # x는 RNN Layer처럼 하나의 값이 하나의 텐서안에 들어가 있도록 즉 (batch_size,sequence_length,channe)
        device = torch.device("cuda:0")
        x= x.to(device)
        seq_last = x[:,-1,:].unsqueeze(2).to(device)
        x = x-seq_last
        if self.individual == True:
            output = torch.zeros([x.size(0),self.pred_len,x.size(2)],dtype = x.dtype).to(x.device)
            for i in range(self.channels):
                output[:,:,i] = self.Linear[i](x[:,:,i])# 하나의 특성마다 각기 다른 Linear layer에 집어넣어줌
                # 이렇게 인덱싱하면 위에 처럼 linear layer에 넣기 좋게 나온다
            x = output
        else:
            x = self.Linear(x.permute(0,2,1)).permute(0,2,1)
        x = x+seq_last
        return x #[Batch, Output length, Channel]
179/11:
# def seq_data(x,y,sequence_length):
#     if (type(x)==pd.Series)|(type(x)==pd.DataFrame):
#         for i in range(len(x)-sequence_length):
#             x_temp = torch.FloatTensor(x.iloc[i:i+sequence_length].values).view(1,-1,1)
#             y_temp= torch.FloatTensor(y.iloc[i+sequence_length]).view(1,-1,1)
            
#             if i==0: x_seq,y_seq = x_temp,y_temp
                
#             else:
#                 x_seq,y_seq=torch.cat((x_seq,x_temp),dim = 1),torch.cat((y_seq,y_temp),dim = 1)
                
#     else: print("make to Series or DataFrame")
#     return x_seq,y_seq
179/12:
def seq_data(x,y,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length])
#             if i == 0:
#                  print(x.iloc[i:i+sequence_length].values)
#                  print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).view(-1,sequence_length,1),FloatTensor(target_list).view(-1,1,1)
    # DataLoader로 batch_size만큼 데이터를 쪼개고 싶다면 [Batch, Input_length, Channel] 형태를 따라줘야됨
179/13:
data = pd.read_csv('../traffic/data/5.csv')
data
179/14: data.drop(['service_name','datetime','packets','unknown'],inplace=True,axis=1)
179/15: data
179/16:
split = 1440
sequence_length = 1440
device = torch.device('cuda:0')
X = data['volumn'].iloc[:-split]
Y = data['volumn']
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
179/17: torch_data = TensorDataset(x_seq,target)
179/18:
batch_size = 32
train_loader = DataLoader(torch_data,batch_size = batch_size)
179/19:
for i,j in train_loader:
    print(i.size())
    print(j.size())
179/20:
from pytorch_forecasting.metrics import SMAPE
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-5)
179/21:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
179/22:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
176/66:
mean_pred_time = np.mean(time_list)
mean_pred_time
176/67:
score = SMAPE()(torch.FloatTensor(target.values).view(-1,1),torch.FloatTensor(pred_list).view(-1,1)) * 100
score.item()
176/68:
from pytorch_forecasting.metrics import RMSE

score = RMSE()(torch.FloatTensor(pred_list).view(-1,1),torch.FloatTensor(target.values).view(-1,1))
score.item()
179/23:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
179/24:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
179/25:
target = Y[-split:]
target
179/26:
pred_list = []
time_list = []
for idx in range(split):
    if idx == 0:
         new_train = torch.FloatTensor(
        [X[-sequence_length+idx:].values]
    ).view(1,sequence_length,1).to(device)
    else:
        new_train = torch.FloatTensor(
            [pd.concat([X[-sequence_length+idx:],target[0:idx]],axis=0,ignore_index=True).values]
        ).view(1,sequence_length,1).to(device)
    # 3차원 텐서로 만들어주는 과정 내가 원하는 모양 (1,1440,1)
    start = time.time()
    out = model(new_train)
    end = time.time()
    pred_list.append(out.cpu().view(1).item())
    time_list.append(end-start)
179/27: len(pred_list)
179/28:
fig,axes = plt.subplots(1,1,figsize=(12,8))
axes.plot(np.arange(1,1441),pred_list,label='prediction')
axes.plot(np.arange(1,1441),target,label='target')
axes.legend()
179/29:
mean_pred_time = np.mean(time_list)
mean_pred_time
179/30:
score = SMAPE()(torch.FloatTensor(target.values).view(-1,1),torch.FloatTensor(pred_list).view(-1,1)) * 100
score.item()
179/31:
score = SMAPE()(torch.FloatTensor(target.values).view(-1,1),torch.FloatTensor(pred_list).view(-1,1)) * 100
score.item()
179/32: from pytorch_forecasting.metrics import RMSE
179/33:
score = RMSE()(torch.FloatTensor(pred_list).view(-1,1),torch.FloatTensor(target.values).view(-1,1))
score.item()
179/34:
fig,axes = plt.subplots(1,1,figsize=(12,8))
axes.plot(np.arange(1,61),pred_list[-60:],label='prediction')
axes.plot(np.arange(1,61),target[-60:],label='target')
axes.legend()
179/35:
score = SMAPE()(torch.FloatTensor(target.values).view(-1,1),torch.FloatTensor(pred_list).view(-1,1)) * 100
score.item()
179/36:
pred_list = []
time_list = []
for idx in range(split):
    if idx == 0:
         new_train = torch.FloatTensor(
        [X[-sequence_length+idx:].values]
    ).view(1,sequence_length,1).to(device)
    else:
        new_train = torch.FloatTensor(
            [pd.concat([X[-sequence_length+idx:],target[0:idx]],axis=0,ignore_index=True).values]
        ).view(1,sequence_length,1).to(device)
    # 3차원 텐서로 만들어주는 과정 내가 원하는 모양 (1,1440,1)
    start = time.time()
    out = model(new_train)
    end = time.time()
    pred_list.append(out.cpu().view(1).item())
    time_list.append(end-start)
179/37: len(pred_list)
179/38:
fig,axes = plt.subplots(1,1,figsize=(12,8))
axes.plot(np.arange(1,1441),pred_list,label='prediction')
axes.plot(np.arange(1,1441),target,label='target')
axes.legend()
179/39:
mean_pred_time = np.mean(time_list)
mean_pred_time
179/40:
score = SMAPE()(torch.FloatTensor(target.values).view(-1,1),torch.FloatTensor(pred_list).view(-1,1)) * 100
score.item()
180/1: print(target)
180/2:
split = 60
sequence_length = 7
device = torch.device('cuda:0')
X = data['sales'].iloc[:-split]
Y = data['sales']
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
180/3:
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
import torch.optim as optim
import matplotlib.pyplot as plt
from torch.utils.data import TensorDataset,DataLoader
from torch import FloatTensor
from sklearn.preprocessing import MinMaxScaler
180/4:
import configparser
config = configparser.ConfigParser()
config['NLinear_config_vol'] = {
    'seq_len':7,
    'pred_len':1,
    'enc_in':1,
    'individual':bool(False)
}
with open('NLinear_config_vol','w') as f:
    config.write(f)
180/5:
import os
config.read(os.getcwd()+os.sep+'NLinear_config_vol',encoding = 'utf-8')
180/6:
class Model(nn.Module):
    def __init__(self,configs):
        super(Model,self).__init__()
        self.seq_len = int(configs['NLinear_config_vol']['seq_len'])
        self.pred_len = int(configs['NLinear_config_vol']['pred_len'])
        self.channels = int(configs['NLinear_config_vol']['enc_in'])
        self.individual = configs['NLinear_config_vol']['individual']
        if self.individual == True:
            self.Linear = nn.ModuleList()
            for i in range(self.channels):
                self.Linear.append(nn.Linear(self.seq_len,self.pred_len))
        else:
            self.Linear = nn.Linear(self.seq_len,self.pred_len)
    
    def forward(self,x):
        # x는 RNN Layer처럼 하나의 값이 하나의 텐서안에 들어가 있도록 즉 (batch_size,sequence_length,channe)
        device = torch.device("cuda:0")
        x= x.to(device)
        seq_last = x[:,-1,:].unsqueeze(2).to(device)
        x = x-seq_last
        if self.individual == True:
            output = torch.zeros([x.size(0),self.pred_len,x.size(2)],dtype = x.dtype).to(x.device)
            for i in range(self.channels):
                output[:,:,i] = self.Linear[i](x[:,:,i])# 하나의 특성마다 각기 다른 Linear layer에 집어넣어줌
                # 이렇게 인덱싱하면 위에 처럼 linear layer에 넣기 좋게 나온다
            x = output
        else:
            x = self.Linear(x.permute(0,2,1)).permute(0,2,1)
        x = x+seq_last
        return x #[Batch, Output length, Channel]
180/7:
def seq_data(x,y,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length])
#             if i == 0:
#                  print(x.iloc[i:i+sequence_length].values)
#                  print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).view(-1,sequence_length,1),FloatTensor(target_list).view(-1,1,1)
    # DataLoader로 batch_size만큼 데이터를 쪼개고 싶다면 [Batch, Input_length, Channel] 형태를 따라줘야됨
180/8:
data = pd.read_csv('../mart/data/vege_8809215001015.csv',names=['datetime','sales'])
data.drop(labels=['datetime'],inplace=True,axis=1)
data
180/9: data.plot()
180/10:
split = 60
sequence_length = 7
device = torch.device('cuda:0')
X = data['sales'].iloc[:-split]
Y = data['sales']
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
180/11: print(target)
180/12:
torch_data = TensorDataset(x_seq,target)

batch_size = 1
train_loader = DataLoader(torch_data,batch_size = batch_size)
180/13:
from pytorch_forecasting.metrics import SMAPE
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
180/14:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
180/15: print(data['sales'].iloc[-6])
180/16: print(data['sales'].iloc[-61])
180/17: print(data['sales'].iloc[-62])
180/18:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
180/19: X[-7+7:] #이렇게 되면서 0 부터 reset 되서 오류 발생
180/20:
pred_list = []
time_list = []
target = Y[-split:]
X = data['sales'].iloc[:-split]
for idx in range(split):
    if idx == 0:
        new_train = torch.FloatTensor([X[-sequence_length+idx:].values]).view(1,sequence_length,1).to(device)
        
    elif idx<=6:
        print(1)
        new_train = torch.FloatTensor(
                [pd.concat([X[-sequence_length+idx:],target[0:idx]],axis=0,ignore_index=True).values]
            ).view(1,sequence_length,1).to(device)
    else:
        new_train = torch.FloatTensor(target[idx-7:idx].values).view(1,sequence_length,1).to(device)
    # 3차원 텐서로 만들어주는 과정 내가 원하는 모양 (1,7,1)
    start = time.time()
    out = model(new_train)
    end = time.time()
    pred_list.append(out.cpu().view(1).item())
    time_list.append(end-start)
180/21: len(pred_list)
180/22:
fig,axes = plt.subplots(1,1,figsize=(12,8))
axes.plot(np.arange(1,61),pred_list,label='prediction')
axes.plot(np.arange(1,61),target,label='target')
axes.legend()
180/23:
mean_pred_time = np.mean(time_list)
mean_pred_time
180/24:
score = SMAPE()(torch.FloatTensor(target.values).view(-1,1),torch.FloatTensor(pred_list).view(-1,1)) * 100
score.item()
180/25:
from pytorch_forecasting.metrics import RMSE

score = RMSE()(torch.FloatTensor(pred_list).view(-1,1),torch.FloatTensor(target.values).view(-1,1))
score.item()
180/26:
pred_list = []
time_list = []
target = Y[-split:]
X = data['sales'].iloc[:-split]
for idx in range(split):
    if idx == 0:
        new_train = torch.FloatTensor([X[-sequence_length+idx:].values]).view(1,sequence_length,1).to(device)
        
    elif idx<=6:
        print(1)
        new_train = torch.FloatTensor(
                [pd.concat([X[-sequence_length+idx:],target[0:idx]],axis=0,ignore_index=True).values]
            ).view(1,sequence_length,1).to(device)
    else:
        new_train = torch.FloatTensor(pred_list[idx-7:idx]).view(1,sequence_length,1).to(device)
    # 3차원 텐서로 만들어주는 과정 내가 원하는 모양 (1,7,1)
    start = time.time()
    out = model(new_train)
    end = time.time()
    pred_list.append(out.cpu().view(1).item())
    time_list.append(end-start)
180/27:
fig,axes = plt.subplots(1,1,figsize=(12,8))
axes.plot(np.arange(1,61),pred_list,label='prediction')
axes.plot(np.arange(1,61),target,label='target')
axes.legend()
182/1:
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
import torch.optim as optim
import matplotlib.pyplot as plt
from torch.utils.data import TensorDataset,DataLoader
from torch import FloatTensor
from sklearn.preprocessing import MinMaxScaler
182/2:
import configparser
config = configparser.ConfigParser()
config['DLinear_config_vol'] = {
    'seq_len':7,
    'pred_len':1,
    'enc_in':1,
    'individual':bool(False)
}
with open('DLinear_config_vol','w') as f:
    config.write(f)

import os
config.read(os.getcwd()+os.sep+'DLinear_config_vol',encoding = 'utf-8')
182/3:
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['1step_DLinear_config']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['1step_DLinear_config']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs['1step_DLinear_config']['individual'] # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['1step_DLinear_config']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual==True: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasonal_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            #print(self.individual)
            if self.individual==True:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
182/4:
def seq_data(x,y,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length])
#             if i == 0:
#                  print(x.iloc[i:i+sequence_length].values)
#                  print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,pred_len).to(device)
182/5:
data = pd.read_csv('../mart/data/vege_8809215001015.csv',names=['datetime','sales'])
data.drop(labels=['datetime'],inplace=True,axis=1)
data
182/6:
split = 60
sequence_length = 7
device = torch.device('cuda:0')
X = data['sales'].iloc[:-split]
Y = data['sales']
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
182/7:
split = 60
sequence_length = 7
pred_len = 1
device = torch.device('cuda:0')
X = data['sales'].iloc[:-split]
Y = data['sales']
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
182/8:
torch_data = TensorDataset(x_seq,target)

batch_size = 1
train_loader = DataLoader(torch_data,batch_size = batch_size)
182/9:
from pytorch_forecasting.metrics import SMAPE
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
182/10:
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['DLinear_config_vol']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['DLinear_config_vol']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs['DLinear_config_vol']['individual'] # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['DLinear_config_vol']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual==True: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasonal_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            #print(self.individual)
            if self.individual==True:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
182/11:
from pytorch_forecasting.metrics import SMAPE
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
182/12:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
182/13:
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['DLinear_config_vol']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['DLinear_config_vol']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs['DLinear_config_vol']['individual'] # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['DLinear_config_vol']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual==True: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasonal_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            #print(self.individual)
            print(seasonal_init,trend_init)
            if self.individual==True:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
182/14:
from pytorch_forecasting.metrics import SMAPE
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
182/15:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
182/16:
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['DLinear_config_vol']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['DLinear_config_vol']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs['DLinear_config_vol']['individual'] # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['DLinear_config_vol']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual==True: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            #seasonal_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            #print(self.individual)
            print(seasonal_init,trend_init)
            if self.individual==True:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
182/17:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
182/18:
from pytorch_forecasting.metrics import SMAPE
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
182/19:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
182/20:
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['DLinear_config_vol']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['DLinear_config_vol']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs['DLinear_config_vol']['individual'] # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['DLinear_config_vol']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual==True: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            #seasonal_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            #print(self.individual)
            print(seasonal_init.size(),trend_init.size())
            if self.individual==True:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
182/21:
from pytorch_forecasting.metrics import SMAPE
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
182/22:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
        
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
182/23:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
182/24:
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['DLinear_config_vol']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['DLinear_config_vol']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs['DLinear_config_vol']['individual'] # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['DLinear_config_vol']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual==True: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            seasonal_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            #print(self.individual)
            print(seasonal_init.size(),trend_init.size())
            if self.individual==True:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
182/25:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
182/26:
from pytorch_forecasting.metrics import SMAPE
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
182/27:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
182/28:
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['DLinear_config_vol']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['DLinear_config_vol']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs['DLinear_config_vol']['individual'] # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['DLinear_config_vol']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual==True: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            #seasonal_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            #print(self.individual)
            print(seasonal_init.size(),trend_init.size())
            if self.individual==True:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
182/29:
def seq_data(x,y,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length])
#             if i == 0:
#                  print(x.iloc[i:i+sequence_length].values)
#                  print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,pred_len).to(device)
182/30:
data = pd.read_csv('../mart/data/vege_8809215001015.csv',names=['datetime','sales'])
data.drop(labels=['datetime'],inplace=True,axis=1)
data
182/31:
split = 60
sequence_length = 7
pred_len = 1
device = torch.device('cuda:0')
X = data['sales'].iloc[:-split]
Y = data['sales']
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
182/32:
torch_data = TensorDataset(x_seq,target)

batch_size = 1
train_loader = DataLoader(torch_data,batch_size = batch_size)
182/33:
from pytorch_forecasting.metrics import SMAPE
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
182/34:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
184/1:
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
import torch.optim as optim
import matplotlib.pyplot as plt
from torch.utils.data import TensorDataset,DataLoader
from torch import FloatTensor
from sklearn.preprocessing import MinMaxScaler
184/2:
import configparser
config = configparser.ConfigParser()
config['DLinear_config_vol'] = {
    'seq_len':7,
    'pred_len':1,
    'enc_in':1,
    'individual':bool(False)
}
with open('DLinear_config_vol','w') as f:
    config.write(f)

import os
config.read(os.getcwd()+os.sep+'DLinear_config_vol',encoding = 'utf-8')
184/3:
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['DLinear_config_vol']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['DLinear_config_vol']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs['DLinear_config_vol']['individual'] # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['DLinear_config_vol']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual==True: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            #seasonal_init,trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) # 새로로 펼친걸 가로로 늘림
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            #print(self.individual)
            print(seasonal_init.size(),trend_init.size())
            if self.individual==True:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
184/4:
def seq_data(x,y,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length])
#             if i == 0:
#                  print(x.iloc[i:i+sequence_length].values)
#                  print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,pred_len).to(device)
184/5:
data = pd.read_csv('../mart/data/vege_8809215001015.csv',names=['datetime','sales'])
data.drop(labels=['datetime'],inplace=True,axis=1)
data
184/6:
split = 60
sequence_length = 7
pred_len = 1
device = torch.device('cuda:0')
X = data['sales'].iloc[:-split]
Y = data['sales']
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
184/7:
from pytorch_forecasting.metrics import SMAPE
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
184/8:
torch_data = TensorDataset(x_seq,target)

batch_size = 1
train_loader = DataLoader(torch_data,batch_size = batch_size)
184/9:
from pytorch_forecasting.metrics import SMAPE
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
184/10:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
184/11:
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['DLinear_config_vol']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['DLinear_config_vol']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs['DLinear_config_vol']['individual'] # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['DLinear_config_vol']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual==True: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
            #print(self.individual)
            print(seasonal_init.size(),trend_init.size())
            if self.individual==True:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
184/12:
def seq_data(x,y,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length])
#             if i == 0:
#                  print(x.iloc[i:i+sequence_length].values)
#                  print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,pred_len).to(device)
184/13:
data = pd.read_csv('../mart/data/vege_8809215001015.csv',names=['datetime','sales'])
data.drop(labels=['datetime'],inplace=True,axis=1)
data
184/14:
split = 60
sequence_length = 7
pred_len = 1
device = torch.device('cuda:0')
X = data['sales'].iloc[:-split]
Y = data['sales']
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
184/15:
torch_data = TensorDataset(x_seq,target)

batch_size = 1
train_loader = DataLoader(torch_data,batch_size = batch_size)
184/16:
from pytorch_forecasting.metrics import SMAPE
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
184/17:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
184/18:
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    "Decomposition - Linear"
    def __init__(self,configs): # 내가 만든 configure module file
        super(Model,self).__init__()
        self.seq_len = int(configs['DLinear_config_vol']['seq_len']) # configure의 sequence length
        self.pred_len = int(configs['DLinear_config_vol']['pred_len']) # configure의 prediction length1
        
        # Decomposition Kernel Size
        kernel_size = 25
        self.decomposition = series_decomp(kernel_size)
        # residual과 moving_average 값을 받기 위한 decomposition model
        self.individual = configs['DLinear_config_vol']['individual'] # 1) 특성각각에 대해 별도의 layer를 둘것이냐의여부 이고
        self.channels = int(configs['DLinear_config_vol']['enc_in']) # 2) 이때 특성의개수
        
        if self.individual==True: # 특성 각각에 별도의 layer를 두자고 하면 -> Layer를 담아놓을 list가 필요함
            self.Linear_Seasonal = nn.ModuleList() # 모듈을 담아놓는 리스트 파이썬의 리스트처럼 인덱스로 접근 가능하다
            self.Linear_Trend = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
        else: # 특성 각각에 별도의 layer를 두지 말자고 하면 하나씩의 Layer만 있으면 됨
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            
    def forward(self,x):
            # x의 size는 [Batch, Input_length, Channel]
            seasonal_init,trend_init = self.decomposition(x) # res = seasonality 를 moving_mean은 trend를 의미함
            
        # batch,input_size, sequence_length ex> tensor([[[1.6667, 2.0000, 2.6667, 3.6667],[1.6667, 2.0000, 2.6667, 3.6667]]])
    
            if self.individual==True:
                seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],
                                              dtype = seasonal_init.dtype).to(seasonal_init.device)
                # seasonal_output을 저장할 Tensor를 만듦 Tip> ones도 비슷함
                trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),
                                            self.pred_len],dtype = trend_init.dtype).to(trend_init.device)
                for i in range(self.channels):# feature의 갯수만큼 for문을 돌림 feature 각각에 대해서 linear layer에 투입
                    seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                    # 위에서 seasonal init을 batch, input_size, sequence length로 바꿈 여기서 input size가 의미하는 바가 사실 
                    # feature의 갯수임 -> 따라서 각 피처마다 Linear Layer가 따로 있고 하나의 피처의 sequence length만큼의 데이터를 집어넣은후 학습하는 구조
                    trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
            else:
                seasonal_output = self.Linear_Seasonal(seasonal_init)    
                trend_output = self.Linear_Trend(trend_init)
            
            x = seasonal_output + trend_output
            return x.permute(0,2,1) # batch size, output_length(not sequence_length Layer를 돌면서 크기가 바뀜), channel(feature_size)로 변경해서 return
184/19:
def seq_data(x,y,sequence_length):
    seq_list = []
    target_list = []
    if (type(x)==list)|(type(x)==np.array):
        for i in range(len(x)-sequence_length):
            seq_list.append(x[i:i+sequence_length].values)
            target_list.append(x[i+sequence_length])
    
    if type(x) == pd.Series:
        print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length])
#             if i == 0:
#                  print(x.iloc[i:i+sequence_length].values)
#                  print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).unsqueeze(1).to(device),FloatTensor(target_list).unsqueeze(1).view(-1,1,pred_len).to(device)
184/20:
data = pd.read_csv('../mart/data/vege_8809215001015.csv',names=['datetime','sales'])
data.drop(labels=['datetime'],inplace=True,axis=1)
data
184/21:
split = 60
sequence_length = 7
pred_len = 1
device = torch.device('cuda:0')
X = data['sales'].iloc[:-split]
Y = data['sales']
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
184/22:
torch_data = TensorDataset(x_seq,target)

batch_size = 1
train_loader = DataLoader(torch_data,batch_size = batch_size)
184/23:
from pytorch_forecasting.metrics import SMAPE
model = Model(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = optim.Adam(model.parameters(),lr=1e-3)
184/24:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        
        out = model(seq)
  
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
184/25:
pred_list = []
time_list = []
target = Y[-split:]
X = data['sales'].iloc[:-split]
for idx in range(split):
    if idx == 0:
        new_train = torch.FloatTensor([X[-sequence_length+idx:].values]).view(1,sequence_length,1).to(device)
        
    elif idx<=6:
        print(1)
        new_train = torch.FloatTensor(
                [pd.concat([X[-sequence_length+idx:],target[0:idx]],axis=0,ignore_index=True).values]
            ).view(1,sequence_length,1).to(device)
    else:
        new_train = torch.FloatTensor(target[idx-7:idx].values).view(1,sequence_length,1).to(device)
    # 3차원 텐서로 만들어주는 과정 내가 원하는 모양 (1,7,1)
    start = time.time()
    out = model(new_train)
    end = time.time()
    pred_list.append(out.cpu().view(1).item())
    time_list.append(end-start)
184/26:
pred_list = []
time_list = []
target = Y[-split:]
X = data['sales'].iloc[:-split]
for idx in range(split):
    if idx == 0:
        new_train = torch.FloatTensor([X[-sequence_length+idx:].values]).view(1,1,sequence_length).to(device)
        
    elif idx<=6:
        print(1)
        new_train = torch.FloatTensor(
                [pd.concat([X[-sequence_length+idx:],target[0:idx]],axis=0,ignore_index=True).values]
            ).view(1,1,sequence_length).to(device)
    else:
        new_train = torch.FloatTensor(target[idx-7:idx].values).view(1,1,sequence_length).to(device)
    # 3차원 텐서로 만들어주는 과정 내가 원하는 모양 (1,7,1)
    start = time.time()
    out = model(new_train)
    end = time.time()
    pred_list.append(out.cpu().view(1).item())
    time_list.append(end-start)
184/27:
fig,axes = plt.subplots(1,1,figsize=(12,8))
axes.plot(np.arange(1,61),pred_list,label='prediction')
axes.plot(np.arange(1,61),target,label='target')
axes.legend()
184/28: seasonal_init
191/1:
import torch
import torch.nn as nn
import pandas as pd
import matplotlib.pyplot as plt
from torch.optim import Adam
from torch import FloatTensor
from torch.utils.data import TensorDataset,DataLoader
191/2:
import configparser
config = configparser.ConfigParser()
config['seq2seq_vol']= {
    'input_size' : int(1),
    'hidden_size' : int(16),
    'num_layers' : int(2) 
}
with open('seq2seq_vol','w') as f:
    config.write(f)
191/3:
import os
config.read(os.getcwd()+os.sep+'seq2seq',encoding='utf-8')
191/4:
import os
config.read(os.getcwd()+os.sep+'seq2seq_vol',encoding='utf-8')
191/5:
class Encoder(nn.Module):
    def __init__(self,configs):
        super(Encoder,self).__init__()
        self.input_size = int(configs['seq2seq']['input_size'])
        # input의 feature dimension을 넣어주어야 한다
        self.hidden_size = int(configs['seq2seq']['hidden_size'])
        # 내부에서 feature dimension을 어떻게 바꿔주고 싶은지 넣어주면 된다
        # 가령 mxn matrix가 입력으로 들어왔을때 hidden size를 h라 한다면 mxh의 크기로 바꾼다
        self.num_layers = int(configs['seq2seq']['num_layers'])
        self.lstm = nn.LSTM(input_size=self.input_size,hidden_size = self.hidden_size,
                           num_layers = self.num_layers,batch_first = True)

    
    def forward(self,x):
        lstm_out,hidden = self.lstm(x) #lstm의 output으로 나오는 hidden_state는 마지막 hidden_state값이다
        return lstm_out,hidden

class Decoder(nn.Module):
    def __init__(self,configs):
        super(Decoder,self).__init__()
        self.input_size = int(configs['seq2seq']['input_size'])
        # input의 feature dimension을 넣어주어야 한다
        self.hidden_size = int(configs['seq2seq']['hidden_size'])
        # 내부에서 feature dimension을 어떻게 바꿔주고 싶은지 넣어주면 된다
        # 가령 mxn matrix가 입력으로 들어왔을때 hidden size를 h라 한다면 mxh의 크기로 바꾼다
        self.num_layers = int(configs['seq2seq']['num_layers'])
        self.lstm = nn.LSTM(input_size=self.input_size,hidden_size = self.hidden_size,
                           num_layers = self.num_layers,batch_first = True)
        self.linear = nn.Linear(self.hidden_size,self.input_size)
        
    def forward(self,x,encoder_hidden,encoder_cell):
        lstm_out,hidden = self.lstm(x,(encoder_hidden,encoder_cell))
        output = self.linear(lstm_out)
        
        return output,hidden
191/6:
class seq2seq(nn.Module):
    def __init__(self,configs):
        super(seq2seq,self).__init__()
        self.encoder = Encoder(configs)
        self.decoder = Decoder(configs)
        
    def forward(self,x):
        outputs = torch.zeros(x.size(0),1,1) # feature가1개고 target도 1개인 데이터
        _,final_hidden = self.encoder(x)
        #print(final_hidden)
        output,_ = self.decoder(x = (x[:,-1,:].cpu().unsqueeze(-1)).to(device),encoder_hidden = final_hidden[0],
                                encoder_cell = final_hidden[1])
        
        return output
191/7:
def seq_data(x,y,sequence_length):
    seq_list = []
    target_list = []
    if type(x)==np.ndarray:
        print(1)
        for i in range(x.shape[0]-sequence_length):
            #print(1)
            seq_list.append(x[i:i+sequence_length])
            target_list.append(x[i+sequence_length])
    
    elif type(x) == pd.Series:
        #print(1)
        for i in range(len(x)-sequence_length):
            seq_list.append(x.iloc[i:i+sequence_length].values)
            target_list.append(y[i+sequence_length])
#             if i == 0:
#                  print(x.iloc[i:i+sequence_length].values)
#                  print(y[i+sequence_length:i+sequence_length+pred_len].values)
            #print(x[i+sequence_length:i+sequence_length+pred_len].values.shape)
    else:
        print('error')

    return FloatTensor(seq_list).view(-1,sequence_length,1),FloatTensor(target_list).view(-1,1,1)
    # DataLoader로 batch_size만큼 데이터를 쪼개고 싶다면 [Batch, Input_length, Channel] 형태를 따라줘야됨
191/8:
data = pd.read_csv('../mart/data/vege_8809215001015.csv',names=['datetime','sales'])

data.drop(['datetime'],inplace=True,axis=1)

split = 60
sequence_length = 7
pred_len = 1
device = torch.device('cuda:0')
X = data['sales'].iloc[:-split]
Y = data['sales']
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
191/9:
torch_data = TensorDataset(x_seq,target)
batch_size = 64
train_loader = DataLoader(torch_data,batch_size = batch_size)
191/10:
torch_data = TensorDataset(x_seq,target)
batch_size = 1
train_loader = DataLoader(torch_data,batch_size = batch_size)
191/11:
from pytorch_forecasting.metrics import SMAPE
model = seq2seq(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = Adam(model.parameters(),lr=1e-5)
191/12:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        seq=seq.to(device)
        
        out = model(x=seq)
    
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
191/13:
data = pd.read_csv('../mart/data/vege_8809215001015.csv',names=['datetime','sales'])

data.drop(['datetime'],inplace=True,axis=1)

split = 60
sequence_length = 7
pred_len = 1
device = torch.device('cuda:0')
X = scaler.fit_transform(data[['sales']].iloc[:-split])
#print(type(X))
Y = scaler.fit_transform(data[['sales']])
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
191/14:
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
191/15:
data = pd.read_csv('../mart/data/vege_8809215001015.csv',names=['datetime','sales'])

data.drop(['datetime'],inplace=True,axis=1)

split = 60
sequence_length = 7
pred_len = 1
device = torch.device('cuda:0')
X = scaler.fit_transform(data[['sales']].iloc[:-split])
#print(type(X))
Y = scaler.fit_transform(data[['sales']])
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
191/16:
torch_data = TensorDataset(x_seq,target)
batch_size = 1
train_loader = DataLoader(torch_data,batch_size = batch_size)
191/17:
from pytorch_forecasting.metrics import SMAPE
model = seq2seq(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = Adam(model.parameters(),lr=1e-5)
191/18:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        seq=seq.to(device)
        
        out = model(x=seq)
    
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
191/19:
from pytorch_forecasting.metrics import SMAPE
model = seq2seq(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = Adam(model.parameters(),lr=1e-3)
191/20:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        seq=seq.to(device)
        
        out = model(x=seq)
    
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
191/21:
data = pd.read_csv('../mart/data/vege_8809215001015.csv',names=['datetime','sales'])

data.drop(['datetime'],inplace=True,axis=1)

split = 60
sequence_length = 7
pred_len = 1
device = torch.device('cuda:0')
X = scaler.fit_transform(data[['sales']].iloc[:-split])
#print(type(X))
Y = scaler.fit_transform(data[['sales']])
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
191/22:
torch_data = TensorDataset(x_seq,target)
batch_size = 1
train_loader = DataLoader(torch_data,batch_size = batch_size)
191/23:
from pytorch_forecasting.metrics import SMAPE
model = seq2seq(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = Adam(model.parameters(),lr=1e-3)
191/24:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        seq=seq.to(device)
        
        out = model(x=seq)
    
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
191/25:
torch_data = TensorDataset(x_seq,target)
batch_size = 1
train_loader = DataLoader(torch_data,batch_size = batch_size)
191/26:
data = pd.read_csv('../mart/data/vege_8809215001015.csv',names=['datetime','sales'])

data.drop(['datetime'],inplace=True,axis=1)

split = 60
sequence_length = 7
pred_len = 1
device = torch.device('cuda:0')
X = scaler.fit_transform(data[['sales']].iloc[:-split])
#print(type(X))
Y = scaler.fit_transform(data[['sales']])
x_seq,target = seq_data(x=X,y=Y,sequence_length=sequence_length)
print(x_seq,target)
print(x_seq.size(),target.size())
191/27:
torch_data = TensorDataset(x_seq,target)
batch_size = 1
train_loader = DataLoader(torch_data,batch_size = batch_size)
191/28:
from pytorch_forecasting.metrics import SMAPE
model = seq2seq(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = Adam(model.parameters(),lr=1e-5)
191/29:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        seq=seq.to(device)
        
        out = model(x=seq)
    
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
191/30:
pred_list = []
time_list = []
X = scaler.fit_transform(data[['volumn']].iloc[-2*split:-split]).reshape(-1,)
Y = scaler.fit_transform(data[['volumn']])
target = Y[-split:].reshape(-1,)
for idx in range(split):
    if idx == 0:
         new_train = torch.FloatTensor(
        X[-sequence_length+idx:]
    ).view(1,sequence_length,1).to(device)
    else:
        new_train = torch.FloatTensor(
            [pd.concat([pd.DataFrame({'volumn': X[-sequence_length+idx:]}),pd.DataFrame({'volumn':target[0:idx]})],axis=0,ignore_index=True).values]
        ).view(1,sequence_length,1).to(device)
    # 3차원 텐서로 만들어주는 과정 내가 원하는 모양 (1,1440,1)
    start = time.time()
    out = model(new_train)
    end = time.time()
    pred_list.append(out.cpu().view(1).item())
    time_list.append(end-start)
191/31:
pred_list = []
time_list = []
X = scaler.fit_transform(data[['sales']].iloc[-2*split:-split]).reshape(-1,)
Y = scaler.fit_transform(data[['sales']])
target = Y[-split:].reshape(-1,)
for idx in range(split):
    if idx == 0:
         new_train = torch.FloatTensor(
        X[-sequence_length+idx:]
    ).view(1,sequence_length,1).to(device)
    else:
        new_train = torch.FloatTensor(
            [pd.concat([pd.DataFrame({'volumn': X[-sequence_length+idx:]}),pd.DataFrame({'volumn':target[0:idx]})],axis=0,ignore_index=True).values]
        ).view(1,sequence_length,1).to(device)
    # 3차원 텐서로 만들어주는 과정 내가 원하는 모양 (1,1440,1)
    start = time.time()
    out = model(new_train)
    end = time.time()
    pred_list.append(out.cpu().view(1).item())
    time_list.append(end-start)
191/32:
pred_list = []
time_list = []
X = scaler.fit_transform(data[['sales']].iloc[-2*split:-split]).reshape(-1,)
Y = scaler.fit_transform(data[['sales']])
target = Y[-split:].reshape(-1,)
for idx in range(split):
    if idx == 0:
         new_train = torch.FloatTensor(
        X[-sequence_length+idx:]
    ).view(1,sequence_length,1).to(device)
    else:
        new_train = torch.FloatTensor(
            [pd.concat([pd.DataFrame({'sales': X[-sequence_length+idx:]}),pd.DataFrame({'sales':target[0:idx]})],axis=0,ignore_index=True).values]
        ).view(1,sequence_length,1).to(device)
    # 3차원 텐서로 만들어주는 과정 내가 원하는 모양 (1,1440,1)
    start = time.time()
    out = model(new_train)
    end = time.time()
    pred_list.append(out.cpu().view(1).item())
    time_list.append(end-start)
191/33:
pred_list = []
time_list = []
X = scaler.fit_transform(data[['sales']].iloc[-2*split:-split]).reshape(-1,)
Y = scaler.fit_transform(data[['sales']])
target = Y[-split:].reshape(-1,)
for idx in range(split):
    if idx == 0:
         new_train = torch.FloatTensor(
        X[-sequence_length+idx:]
    ).view(1,sequence_length,1).to(device)
    elif idx<=6:
        print(1)
        new_train = torch.FloatTensor(
                [pd.concat([X[-sequence_length+idx:],target[0:idx]],axis=0,ignore_index=True).values]
            ).view(1,1,sequence_length).to(device)
    else:
        new_train = torch.FloatTensor(target[idx-7:idx].values).view(1,1,sequence_length).to(device)
    start = time.time()
    out = model(new_train)
    end = time.time()
    pred_list.append(out.cpu().view(1).item())
    time_list.append(end-start)
191/34:
pred_list = []
time_list = []
X = scaler.fit_transform(data[['sales']].iloc[-2*split:-split]).reshape(-1,)
Y = scaler.fit_transform(data[['sales']])
target = Y[-split:].reshape(-1,)
for idx in range(split):
    if idx == 0:
         new_train = torch.FloatTensor(
        X[-sequence_length+idx:]
    ).view(1,sequence_length,1).to(device)
    elif idx<=6:
        print(1)
        new_train = torch.FloatTensor(
                pd.concat(pd.DataFrame({'sales':X[-sequence_length+idx:]}),pd.DataFrame({'sales':target[0:idx]}),axis=0,ignore_index=True).values
            ).view(1,1,sequence_length).to(device)
    else:
        new_train = torch.FloatTensor(target[idx-7:idx].values).view(1,1,sequence_length).to(device)
    start = time.time()
    out = model(new_train)
    end = time.time()
    pred_list.append(out.cpu().view(1).item())
    time_list.append(end-start)
191/35:
fig,axes = plt.subplots(1,1,figsize=(12,8))
axes.plot(np.arange(1,1441),pred_list,label='prediction')
axes.plot(np.arange(1,1441),target,label='target')
axes.legend()
191/36:
pred_list = []
time_list = []
X = scaler.fit_transform(data[['sales']].iloc[-2*split:-split]).reshape(-1,)
Y = scaler.fit_transform(data[['sales']])
target = Y[-split:].reshape(-1,)
for idx in range(split):
    if idx == 0:
         new_train = torch.FloatTensor(
        X[-sequence_length+idx:]
    ).view(1,sequence_length,1).to(device)
    elif idx<=6:
        print(1)
        new_train = torch.FloatTensor(
                pd.concat([pd.DataFrame({'sales':X[-sequence_length+idx:]}),pd.DataFrame({'sales':target[0:idx]})],
                          axis=0,ignore_index=True).values
            ).view(1,1,sequence_length).to(device)
    else:
        new_train = torch.FloatTensor(target[idx-7:idx].values).view(1,1,sequence_length).to(device)
    start = time.time()
    out = model(new_train)
    end = time.time()
    pred_list.append(out.cpu().view(1).item())
    time_list.append(end-start)
191/37:
pred_list = []
time_list = []
X = scaler.fit_transform(data[['sales']].iloc[-2*split:-split]).reshape(-1,)
Y = scaler.fit_transform(data[['sales']])
target = Y[-split:].reshape(-1,)
for idx in range(split):
    if idx == 0:
         new_train = torch.FloatTensor(
        X[-sequence_length+idx:]
    ).view(1,sequence_length,1).to(device)
    elif idx<=6:
        print(1)
        new_train = torch.FloatTensor(
                pd.concat([pd.DataFrame({'sales':X[-sequence_length+idx:]}),pd.DataFrame({'sales':target[0:idx]})],
                          axis=0,ignore_index=True).values
            ).view(1,sequence_length,1).to(device)
    else:
        new_train = torch.FloatTensor(target[idx-7:idx].values).view(1,sequence_length,1).to(device)
    start = time.time()
    out = model(new_train)
    end = time.time()
    pred_list.append(out.cpu().view(1).item())
    time_list.append(end-start)
191/38:
pred_list = []
time_list = []
X = scaler.fit_transform(data[['sales']].iloc[-2*split:-split]).reshape(-1,)
Y = scaler.fit_transform(data[['sales']])
target = Y[-split:].reshape(-1,)
for idx in range(split):
    if idx == 0:
         new_train = torch.FloatTensor(
        X[-sequence_length+idx:]
    ).view(1,sequence_length,1).to(device)
    elif idx<=6:
        print(1)
        new_train = torch.FloatTensor(
                pd.concat([pd.DataFrame({'sales':X[-sequence_length+idx:]}),pd.DataFrame({'sales':target[0:idx]})],
                          axis=0,ignore_index=True).values
            ).view(1,sequence_length,1).to(device)
    else:
        new_train = torch.FloatTensor(target[idx-7:idx]).view(1,sequence_length,1).to(device)
    start = time.time()
    out = model(new_train)
    end = time.time()
    pred_list.append(out.cpu().view(1).item())
    time_list.append(end-start)
191/39:
fig,axes = plt.subplots(1,1,figsize=(12,8))
axes.plot(np.arange(1,1441),pred_list,label='prediction')
axes.plot(np.arange(1,1441),target,label='target')
axes.legend()
191/40:
fig,axes = plt.subplots(1,1,figsize=(12,8))
axes.plot(np.arange(1,61),pred_list,label='prediction')
axes.plot(np.arange(1,61),target,label='target')
axes.legend()
191/41:
torch_data = TensorDataset(x_seq,target)
batch_size = 1
train_loader = DataLoader(torch_data,batch_size = batch_size)
191/42:
from pytorch_forecasting.metrics import SMAPE
model = seq2seq(config).to(device)
criterion = SMAPE()
num_epochs = 100
optimizer = Adam(model.parameters(),lr=1e-4)
191/43:
import time
count = 0
out_list = []
loss_graph = []
n = len(train_loader)
begin = time.time()
for epoch in range(num_epochs):

    running_loss = 0.0
    for dt in train_loader:
        seq,tg = dt
        #print(seq.size())
        seq=seq.to(device)
        
        out = model(x=seq)
    
        tg = tg.to(device)
        
        loss = criterion(out,tg)# 손실함수 계산
        
        optimizer.zero_grad() # optimizer 초기화
        
        loss.backward()
        
        optimizer.step() # optimizer 최적화
        
        running_loss = running_loss + loss.item()
        
        count+=1
        
    loss_graph.append(running_loss/n)
    
    print('smape',running_loss/n*100,'%') 

end = time.time()
#시간차
result = end - begin
print(count) # 64크기를 갖는 배치가 585개 있는데(마지막은 < 64) 이걸 50번 돌았
191/44:
pred_list = []
time_list = []
X = scaler.fit_transform(data[['sales']].iloc[-2*split:-split]).reshape(-1,)
Y = scaler.fit_transform(data[['sales']])
target = Y[-split:].reshape(-1,)
for idx in range(split):
    if idx == 0:
         new_train = torch.FloatTensor(
        X[-sequence_length+idx:]
    ).view(1,sequence_length,1).to(device)
    elif idx<=6:
        print(1)
        new_train = torch.FloatTensor(
                pd.concat([pd.DataFrame({'sales':X[-sequence_length+idx:]}),pd.DataFrame({'sales':target[0:idx]})],
                          axis=0,ignore_index=True).values
            ).view(1,sequence_length,1).to(device)
    else:
        new_train = torch.FloatTensor(target[idx-7:idx]).view(1,sequence_length,1).to(device)
    start = time.time()
    out = model(new_train)
    end = time.time()
    pred_list.append(out.cpu().view(1).item())
    time_list.append(end-start)
191/45:
fig,axes = plt.subplots(1,1,figsize=(12,8))
axes.plot(np.arange(1,61),pred_list,label='prediction')
axes.plot(np.arange(1,61),target,label='target')
axes.legend()
191/46:
mean_pred_time = np.mean(time_list)
mean_pred_time
190/1:
score = SMAPE()(torch.FloatTensor(target).view(-1,1),torch.FloatTensor(pred_list).view(-1,1)) * 100
score.item()
191/47:
score = SMAPE()(torch.FloatTensor(target).view(-1,1),torch.FloatTensor(pred_list).view(-1,1)) * 100
score.item()
   1: %history -g
   2: %history -g -f 'NLinear-1step'
